{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"CvW2nSZvgMrM","executionInfo":{"status":"ok","timestamp":1678668939084,"user_tz":420,"elapsed":2,"user":{"displayName":"Anish Salvi","userId":"11521197594931735168"}}},"outputs":[],"source":["#installations\n","#pip install SimpleITK"]},{"cell_type":"code","source":["#imports\n","import os\n","import torch\n","import wandb\n","import pandas as pd\n","import torch.nn as nn\n","import sys\n","#this path can be specified (if importing)\n","#sys.path.append()\n","from PIL import Image\n","import torchvision.transforms as transforms\n","#import matplotlib.pyplot as plt\n","from torch import nn\n","import sklearn\n","import numpy as np\n","import SimpleITK as sitk\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import random\n","import torchvision\n","#import livelossplot\n","#from livelossplot import PlotLosses\n","#import scipy\n","#import sklearn\n","#from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","#from sklearn.utils import shuffle\n","#from sklearn.metrics import confusion_matrix\n","#import seaborn as sns\n","#from ctviewer import CTViewer\n","#from sklearn.metrics import RocCurveDisplay\n","import datetime\n","from datetime import datetime\n","import json\n","#check if the gpu machine is available\n","if torch.cuda.is_available():\n","  device = 'cuda'\n","  gpu = torch.cuda.get_device_name(0)\n","  print('Device: ', gpu)\n","else:\n","  device = 'cpu'\n","  gpu = None\n","  print('Device', device)\n","\n","#wanddb\n","key = \"\" #specify wandb key\n","#Weights and Bias\n","if key:\n","  wandb.login(key=key) #API Key is in your wandb account, under settings (wandb.ai/settings)"],"metadata":{"id":"RnQ_JokVgUcI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#functions\n","\n","#Dataset\n","class CustomImageDataset(Dataset):\n","    def __init__(self, df, col_image, col_label, aug = False, shuffle = False):\n","      #params\n","      self.df = df\n","      if shuffle:\n","        self.df = self.df.sample(frac = 1, random_state = 42,).reset_index()\n","      self.aug = aug\n","      self.col_image = col_image\n","      self.col_label = col_label\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","      #row\n","      row = self.df.iloc[idx]\n","\n","      #read image\n","      image = torch.Tensor(np.expand_dims(sitk.GetArrayFromImage(sitk.ReadImage(row[self.col_image])), axis = 0))\n","      #read label\n","      label = row[self.col_label]\n","\n","      #if aug\n","      if self.aug:\n","        #augmentation (increase this given size of the data)\n","        if random.random() > 0.5: #0.7 0.8\n","          #horizontal\n","          image = torchvision.transforms.functional.hflip(image)\n","        if random.random() > 0.5:\n","          #vertical\n","          image = torchvision.transforms.functional.vflip(image)\n","        #if random.random() > 0.5:\n","          #rotate\n","          image = torchvision.transforms.functional.rotate(image, \n","                                                           random.choice([30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330]))\n","        #noise\n","        if random.random() > 0.5:\n","          noise = torch.normal(random.uniform(-0.9, 0.9), random.uniform(0.01, 0.1), image.shape)\n","          image = image + noise\n","\n","        #alternate roations doesn't work due to shape\n","        #if random.random() > 0.5:\n","        #  ls = random.sample([1,2,3], 2)\n","        #  k = random.randint(1, 3)\n","        #  image = torch.rot90(image, k, ls)\n","\n","        #affine or perspective may be more aggressive? tranlate and scale --> have to make sure are valid inputs!\n","        #hopefully increase scale invariance!\n","        #if random.random() > 0.5:\n","          #careful not to cut off\n","          #x = random.choice([5, 10, 15, -5, -10, -15])\n","          #y = random.choice([5, 10, 15, -5, -10, -15])\n","          #higher indicates more zoom in (AAA usually in center; may decide to remove if model not fitting correctly)\n","          #may also remove the perspective aspect of the problem (gauging if bigger or smaller!)\n","          #scaling = random.choice([0.9, 1.1, 1.5, 1.9])\n","          #image = torchvision.transforms.functional.affine(image, angle = 0, translate = (x, y), scale = scaling, shear = 0, fill = -1, \n","          #interpolation = torchvision.transforms.InterpolationMode.BILINEAR)\n","      #return\n","      return image, label\n","\n","#save json file\n","def save_params(hyper_params, save_path):\n","  json_string = json.dumps(hyper_params)\n","  with open(save_path, 'w') as outfile:\n","    outfile.write(json_string)\n","\n","#load the train params back in\n","def load_params(fpath):\n","  # Opening JSON file\n","  with open(fpath) as json_file:\n","    data = json.load(json_file)\n","  return data\n","\n","#get the optimizer\n","def get_optimizer(model_config, model):\n","  #AdamW\n","  if model_config['optimizer'] == 'AdamW':\n","    optimizer = torch.optim.AdamW(model.parameters(), lr = model_config['init_lr'], weight_decay = model_config['weight_decay'])\n","  #Adam\n","  if model_config['optimizer'] == 'Adam':\n","    optimizer = torch.optim.Adam(model.parameters(), lr = model_config['init_lr'], weight_decay = model_config['weight_decay'])\n","  #NAdam\n","  if model_config['optimizer'] == 'NAdam':\n","    optimizer = torch.optim.NAdam(model.parameters(), lr = model_config['init_lr'], weight_decay = model_config['weight_decay'])\n","  #RAdam\n","  if model_config['optimizer'] == 'RAdam':\n","    optimizer = torch.optim.RAdam(model.parameters(), lr = model_config['init_lr'], weight_decay = model_config['weight_decay'])\n","  #return\n","  return optimizer\n","\n","#get the model\n","def get_model(model_config):\n","  #specify the model init here\n","  model = Model(model_config['img_size'])\n","  #return\n","  return model\n","\n","#model saving policy\n","def save_model(model_config, model):\n","  #save model use weights instead\n","  model.eval()\n","  #depending on choice\n","  if model_config['save_weights_only']:\n","    torch.save(model.state_dict(), model_config['save_folder'] + 'model_weights.pth')\n","  else:\n","    torch.save(model, model_config['save_folder'] + 'model.pth')\n","  #save info\n","  save_params(model_config, model_config['save_folder'] + 'model_config.json')\n","\n","#update previously saved config only \n","def update_config_stopearly(save_path):\n","  #load\n","  model_config = load_params(save_path + 'model_config.json')\n","  #update\n","  model_config['early_stopping']['stopped_early'] = True\n","  #save\n","  save_params(model_config, save_path + 'model_config.json')\n","\n","#new saving policy\n","def new_saving_policy(early_stop, best_model, model_config, model, epoch):\n","  #if there is an early stop\n","  if early_stop:\n","    #exit training\n","    exit_training = True\n","    #has the model been already saved?\n","    if model_config['early_stopping']['model_criteria']:\n","      #save just the config with update\n","      update_config_stopearly(model_config['save_folder'])\n","    #if not already saved\n","    else:\n","      #update\n","      model_config['early_stopping']['stopped_early'] = True\n","      #save the model and config\n","      save_model(model_config, model)\n","  #if there is not early stop\n","  else:\n","    #exit\n","    exit_training = False\n","    #need to log that we did not exit training early\n","    model_config['early_stopping']['stopped_early'] = False\n","    #you want to save the model every n_epochs often\n","    if model_config['save_best_model'] == False:\n","      #check if epocch is divisible and nonzero\n","      if (epoch % model_config['save_after_n_epochs'] == 0) and (epoch != 0):\n","        #indicate the model was saved\n","        model_config['early_stopping']['model_criteria'] = True\n","        #save\n","        save_model(model_config, model)\n","    else:\n","      #you want to save the best model\n","      if model_config['epochs_trained'] >= model_config['save_after_n_epochs']:\n","        #check if current model is the best model\n","        if best_model:\n","          #log that it is the best model\n","          model_config['early_stopping']['best_model'] = True\n","          #indicate the model was saved\n","          model_config['early_stopping']['model_criteria'] = True\n","          #then save\n","          save_model(model_config, model)\n","        #if current model is not the best model but want to save for the initital run\n","        if (best_model == False) and (model_config['epochs_trained'] == model_config['save_after_n_epochs']):\n","          #inidicate the model was saved\n","          model_config['early_stopping']['model_criteria'] = True\n","          #save the model and config\n","          save_model(model_config, model)\n","\n","  #return\n","  return model_config, exit_training\n","\n","#class earlystopping\n","class EarlyStopping:\n","  #early stop if validation does not improve for given patience\n","  def __init__(self, model_config, verbose = True, trace_func = print):\n","    #set up\n","    self.patience = model_config['early_stopping']['patience']\n","    self.delta = model_config['early_stopping']['delta']\n","    self.verbose = verbose\n","    self.trace_func = trace_func\n","    self.counter = 0\n","    self.best_score = None\n","    self.best_model = False\n","    self.early_stop = False\n","\n","  #call\n","  def __call__(self, val_loss):\n","    #neg val loss\n","    score = -val_loss\n","    #init condition\n","    if self.best_score is None:\n","      self.best_score = score\n","    #count number of times model failed to meet the condition\n","    elif score < self.best_score + self.delta:\n","      self.counter += 1\n","      self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","      self.best_model = False\n","      #identify when early stopping is required\n","      if self.counter >= self.patience:\n","        self.early_stop = True\n","    #if the model shows best score\n","    else:\n","      #get the score and counter\n","      self.best_score = score\n","      self.counter = 0\n","      self.best_model = True\n","    #return the interl\n","    return self.early_stop, self.best_model\n","\n","#get the loss fn\n","def get_loss(model_config, device, df = None):\n","  #criterion if manually weighted\n","  if (model_config['loss'] == 'CE') and (model_config['weighted'] == True):\n","    #get\n","    criterion = nn.CrossEntropyLoss(weight = torch.Tensor(model_config['weights']).to(device))\n","  #criterion if not weighted \n","  if (model_config['loss'] == 'CE') and (model_config['weighted'] == False):\n","    #get\n","    criterion = nn.CrossEntropyLoss()\n","  #criterion if using automatic weighting?\n","  if (model_config['loss'] == 'CE') and (model_config['weighted'] == 'auto'):\n","    #get\n","    criterion = nn.CrossEntropyLoss(weight = torch.Tensor(auto_weights(model_config, df)).to(device))\n","  #return\n","  return criterion\n","\n","#get scheduler\n","def get_scheduler(model_config, optimizer):\n","  #plateau\n","  if model_config['scheduler']['description'] == 'plateau':\n","    #get the scheduler\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = model_config['scheduler']['mode'], \n","                                                           factor = model_config['scheduler']['factor'], \n","                                                           patience = model_config['scheduler']['patience'], \n","                                                           threshold = model_config['scheduler']['threshold'], \n","                                                           threshold_mode = model_config['scheduler']['threshold_mode'], \n","                                                           cooldown = model_config['scheduler']['cooldown'], \n","                                                           min_lr = model_config['scheduler']['min_lr'], \n","                                                           eps = model_config['scheduler']['eps'], \n","                                                           verbose = True)\n","  #return\n","  return scheduler\n","\n","#get sampler\n","def get_sampler(df, model_config):\n","  labels_unique, counts = np.unique(df[model_config['col_label']], return_counts = True)\n","  class_weights = [sum(counts) / c for c in counts]\n","  example_weights = [class_weights[e] for e in df[model_config['col_label']]]\n","  sampler = torch.utils.data.WeightedRandomSampler(example_weights, len(df['Annotation_Label']))\n","  return sampler\n","\n","#get loss weights automatically\n","def auto_weights(model_config, df):\n","  #get class weights\n","  y = df[model_config['col_label']].to_numpy().astype(np.int8)\n","  #get the class weights\n","  class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n","  #convert to tensor\n","  class_weights = torch.tensor(class_weights, dtype = torch.float)\n","  #return (may have to send to gpu)\n","  return class_weights"],"metadata":{"id":"ERtnwnR0gXwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#main script\n","def main(config = None):\n","  #clear workspace when finished with a single model run\n","  model, x, y_true, y_pred, loss = (None, None, None, None, None)\n","  dset_train, train_loader, dset_val, val_loader = (None, None, None, None)\n","  criterion, optimizer, scheduler, early_stopper = (None, None, None, None)\n","  #reset\n","  if device == 'cuda':\n","    torch.cuda.empty_cache()\n","\n","  #name the model\n","  model_name = datetime.now().strftime('3D-Model-classification-%Y-%m-%d-%H-%M-%S')\n","\n","  #init a new wandb run (config = sweep_config)\n","  with wandb.init(config = config, name = model_name):\n","    #set up the config (WandB, locked)\n","    config = wandb.config\n","    #dict (not locked)\n","    model_config = dict(config)\n","    #name the model\n","    model_config['model'] = model_name\n","    #save location\n","    model_config['save_folder'] = model_config['save_folder'] +  model_config['model'] + '/'\n","    #create the model folder\n","    if os.path.isdir(model_config['save_folder']) == False:\n","      os.mkdir(model_config['save_folder'])\n","\n","    #training data(not in valkfolds!)\n","    df_train = df[~df['KFold'].isin(model_config['val_kfolds'])]\n","\n","    #get the training data (remove the folds corresponding to validation)\n","    dset_train = CustomImageDataset(df_train, \n","                                    col_image = model_config['col_image'], col_label = model_config['col_label'], \n","                                    aug = model_config['aug'], shuffle = False)\n","    \n","    #if train sampler\n","    if model_config['sampler']:\n","      train_sampler = get_sampler(df_train, model_config)\n","    else:\n","      train_sampler = None\n","\n","    #train loader\n","    train_loader = DataLoader(dset_train, sampler = train_sampler, batch_size = model_config['batch_size'])\n","\n","    #val data (in valkfolds)\n","    df_val = df[df['KFold'].isin(model_config['val_kfolds'])]\n","\n","    #get the validation data\n","    dset_val = CustomImageDataset(df_val, \n","                                  col_image = model_config['col_image'], col_label = model_config['col_label'], \n","                                  aug = False, shuffle = False)\n","    \n","    #if val sampler\n","    if model_config['sampler']:\n","      val_sampler = get_sampler(df_val, model_config)\n","    else:\n","      val_sampler = None\n","\n","    #val loader\n","    val_loader = DataLoader(dset_val, sampler = val_sampler, batch_size = model_config['batch_size'])\n","\n","    #init the model\n","    model = get_model(model_config)\n","    #send\n","    model.to(device)\n","    #criterion (can get loss based on training data)\n","    criterion = get_loss(model_config, device, df_train)\n","    #optimizer\n","    optimizer = get_optimizer(model_config, model)\n","    #scheduler\n","    scheduler = get_scheduler(model_config, optimizer)\n","    #early stopping (save time during the sweep)\n","    early_stopper = EarlyStopping(model_config)\n","\n","    #track in Jupter Notebook\n","    #liveloss = PlotLosses()\n","    #logs\n","    #logs = {}\n","\n","    #track for later\n","    log_train_loss = []\n","    log_train_acc = []\n","    log_val_loss = []\n","    log_val_acc = []\n","\n","\n","    #iterate through the entire dataset \n","    #+1 for shifting (python starts at 0)\n","    for epoch in range(model_config['epochs_trained'] + 1, model_config['epochs'] + 1):\n","      #determine train losses\n","      train_epoch_loss = 0\n","      #set for training\n","      model.train()\n","      #iterate through the training data\n","      for i, (x, y_true) in enumerate(train_loader):\n","        #zero optimizer\n","        optimizer.zero_grad()\n","        #send to device\n","        #x = x.to(device)\n","        #y_true = y_true.to(device)\n","        #predict\n","        y_pred = model(x.to(device))\n","        #determine loss (should already be averaged)\n","        loss = criterion(y_pred, y_true.to(device))\n","        #backward\n","        loss.backward()\n","        #step\n","        optimizer.step()\n","        #track the loss\n","        train_epoch_loss = train_epoch_loss + loss.item()\n","        #reset\n","        x, y_true, y_pred, loss = (None, None, None, None)\n","        if device == 'cuda':\n","          torch.cuda.empty_cache()\n","      #calculate train loss\n","      train_loss = train_epoch_loss / len(train_loader)\n","      #calculate train acc\n","      train_acc = 1 - train_loss\n","\n","      #determine validation losses\n","      val_epoch_loss = 0\n","      #specify eval\n","      model.eval()\n","      #set\n","      with torch.no_grad():\n","        #iterate\n","        for i, (x, y_true) in enumerate(val_loader):\n","          #send to device\n","          #x = x.to(device)\n","          #y_true = y_true.to(device)\n","          #predict\n","          y_pred = model(x.to(device))\n","          #determine loss\n","          loss = criterion(y_pred, y_true.to(device))\n","          #track the loss (shoudld already be averaged)\n","          val_epoch_loss = val_epoch_loss + loss.item()\n","          #reset\n","          x, y_true, y_pred, loss = (None, None, None, None)\n","          if device == 'cuda':\n","            torch.cuda.empty_cache()\n","      #calulate val loss\n","      val_loss = val_epoch_loss / len(val_loader)\n","      #calulate val acc\n","      val_acc = 1 - val_loss\n","\n","      #scheduler\n","      scheduler.step(train_loss)\n","\n","      #record for training\n","      log_train_loss.append(train_loss)\n","      log_train_acc.append(train_acc)\n","      #record for validation\n","      log_val_loss.append(val_loss)\n","      log_val_acc.append(val_acc)\n","\n","      #wont log lossess or acc after early stopping or save best model\n","\n","      #log the most recent info\n","      model_config['train_loss'] = train_loss\n","      model_config['train_acc'] = train_acc\n","      model_config['val_loss'] = val_loss\n","      model_config['val_acc'] = val_acc\n","\n","      #log all the info\n","      model_config['log_train_loss'] = log_train_loss\n","      model_config['log_train_acc'] = log_train_acc\n","      model_config['log_val_loss'] = log_val_loss\n","      model_config['log_val_acc'] = log_val_acc\n","\n","      #keep track of each epoch\n","      model_config['epochs_trained'] = epoch\n","\n","      #print\n","      print('Epoch {0} of {1}: Train Loss {2:.2g} & Acc {3:.2g} v Val Loss {4:.2g} and Acc {5:.2g}'.format(epoch, model_config['epochs'], \n","                                                                                                           train_loss, train_acc, val_loss, val_acc))\n","\n","      #wandb\n","      wandb.log(model_config)\n","\n","\n","      #determine if early stopping is required by validation loss\n","      early_stop, best_model = early_stopper(val_loss)\n","      #saving policy and determine if training should be exited based on early stop and best model\n","      model_config, exit_training = new_saving_policy(early_stop, best_model, model_config, model, epoch)\n","\n","      #specify the logs\n","      #prefix = ''\n","      #logs['Loss'] = train_loss\n","      #logs['Acc'] = train_acc\n","      #logs\n","      #prefix = 'val_'\n","      #logs[prefix + 'Loss'] = val_loss \n","      #logs[prefix + 'Acc'] = val_acc\n","\n","      #living loss\n","      #liveloss.update(logs)\n","      #send\n","      #liveloss.send()\n","\n","      #exit training early\n","      if exit_training:\n","        print('Early Stop: Exit Training')\n","        break\n","\n","    #clear workspace when finished with a single model run\n","    model, x, y_true, y_pred, loss = (None, None, None, None, None)\n","    dset_train, train_loader, dset_val, val_loader = (None, None, None, None)\n","    criterion, optimizer, scheduler, early_stopper = (None, None, None, None)\n","    #reset\n","    if device == 'cuda':\n","      torch.cuda.empty_cache()"],"metadata":{"id":"hw2ap7Xti5Wv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#parameters in wandb format\n","sweep_config = {\n","    #name decided later (sweep name)\n","    'name': None,\n","    #sweep method\n","    'method': 'grid',\n","    #metric\n","    'metric': {\n","        'name': 'val_acc',\n","        'goal': 'maximize',\n","    },\n","    #values which may be altered wandb wants all components\n","    'parameters': {\n","        #description\n","        'description': {'value': 'Model which classifies a Medical Image'}, \n","        #project in wandb\n","        'project':{'value': 'Classification'},\n","        'model': {'value': None}, #placeholder for actual name\n","        #documentation\n","        'data_path': {'value': ''}, #the input csv with filepaths\n","        #path to save the results of the sweep\n","        'save_folder': {'value': ''},\n","        'col_image': {'value': 'Norm-256-256-256'}, #input column of the csv\n","        'col_label': {'value': 'Annotation_Label'}, #label column of the csv\n","        'device': {'value': device},\n","        'val_kfolds': {'value': [4, 5]}, #kfolds for validation\n","        #model specific params\n","        'img_size': {'values': [(256, 256, 256)]},\n","        #training params\n","        'aug': {'values': [True, False]},\n","        'batch_size': {'values': [4]},\n","        'init_lr': {'values': [1e-4, 1e-6]},\n","        'epochs': {'values': [100]}, #max epochs to train\n","        'epochs_trained': {'value': 0}, #this is updated in the script!\n","        'save_after_n_epochs': {'value': 5}, #depends on if you want to save the best model\n","        'weight_decay': {'values': [1e-2, 1e-4]},\n","        'optimizer': {'values': ['AdamW']},\n","        'scheduler': {'values': [\n","            {'description': 'plateau',\n","             'mode': 'min',\n","             'factor': 5e-1,\n","             'patience': 10,\n","             'threshold': 1e-3,\n","             'threshold_mode': 'rel',\n","             'cooldown': 0,\n","             'min_lr': 0,\n","             'eps': 1}]},\n","        'loss': {'values': ['CE']},\n","        'weighted': {'values': ['auto']},\n","        'weights': {'values': [(0.3, 0.5, 0.2)]}, #by manual specification // depends on weighted\n","        'sampler': {'values':[False]},\n","        #saving\n","        'save_weights_only': {'value': True},\n","        'save_best_model': {'value': True},\n","        #early stopping\n","        'early_stopping': {'value':\n","            {'patience': 10,\n","            'delta': 1e-4,\n","            'stopped_early': None, #indicate if stopped early\n","            'best_model': None, #indicate if best model (if save best model)\n","            'model_criteria': False}\n","        },\n","        #log the model loss and acc\n","        'log_train_loss': {'value': None},\n","        'log_train_acc': {'value': None},\n","        'log_val_loss': {'value': None},\n","        'log_val_acc': {'value': None},\n","        #updating performance in WandB\n","        'train_loss': {'value': None},\n","        'train_acc': {'value': None},\n","        'val_loss': {'value': None},\n","        'val_acc': {'value': None}\n","    } \n","}\n"],"metadata":{"id":"YnQRR0Dx_LC3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#main script\n","\n","if __name__ == '__main__':\n","\n","  #read the pickle file\n","  df = pd.read_pickle(sweep_config['parameters']['data_path']['value'])\n","\n","  #specify the sweep save location\n","  sweep_config['name'] = datetime.now().strftime('3D-Model-sweep-%Y-%m-%d-%H-%M-%S')\n","  #set\n","  sweep_config['parameters']['save_folder']['value'] = sweep_config['parameters']['save_folder']['value'] + sweep_config['name'] + '/'\n","  #create the sweep folder\n","  if os.path.isdir(sweep_config['parameters']['save_folder']['value']) == False:\n","    os.mkdir(sweep_config['parameters']['save_folder']['value'])\n","  #save the sweep config in the sweep folder\n","  save_params(sweep_config, sweep_config['parameters']['save_folder']['value'] + 'sweep_config.json')\n","  #now run the main script\n","\n","  #select the project folder\n","  sweep_id = wandb.sweep(sweep_config, project = sweep_config['parameters']['project']['value'])\n","  #execute the search\n","  wandb.agent(sweep_id, main)\n","  #finish\n","  wandb.finish()\n"],"metadata":{"id":"S-L84KNIc0nA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#delete extra wandb files (can be run after)\n","#import shutil\n","#shutil.rmtree('wandb')"],"metadata":{"id":"dL-Ujcps--Gg"},"execution_count":null,"outputs":[]}]}