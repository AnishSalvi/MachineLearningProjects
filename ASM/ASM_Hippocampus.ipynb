{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7NJBGeJgMOT"
      },
      "source": [
        "# Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhdcDirUQLYJ",
        "outputId": "e4c70571-545e-4196-9166-3a6e6a3bc48f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount='True')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1hRwrzrXDzT"
      },
      "outputs": [],
      "source": [
        "#install\n",
        "!pip install --quiet SimpleITK transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2gR4XBtSBbD"
      },
      "outputs": [],
      "source": [
        "#import\n",
        "import tarfile\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import SimpleITK as sitk\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import transformers\n",
        "from transformers import CLIPTextConfig, CLIPTokenizer, CLIPTextModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iODpbv1RSWdq"
      },
      "outputs": [],
      "source": [
        "#untar\n",
        "def untar(inpath, outpath):\n",
        "  my_tar = tarfile.open(inpath)\n",
        "  my_tar.extractall(outpath)\n",
        "  my_tar.close()\n",
        "\n",
        "#get data\n",
        "def get_data(data_path):\n",
        "  image_path = data_path + 'imagesTr/'\n",
        "  label_path = data_path + 'labelsTr/'\n",
        "  patients = []\n",
        "  for patient in sorted(os.listdir(image_path)):\n",
        "    if '._' not in patient:\n",
        "      image = image_path + patient\n",
        "      label = label_path + patient\n",
        "      patient = patient.replace('.nii.gz','')\n",
        "      patients.append([patient, image, label])\n",
        "  return pd.DataFrame(patients, columns = ['Patient', 'Image', 'Label'])\n",
        "\n",
        "#resample image standardize\n",
        "def resample_image(itk_image, out_size = (64, 64, 64), is_label = False):\n",
        "  original_spacing = itk_image.GetSpacing()\n",
        "  original_size = itk_image.GetSize()\n",
        "  out_spacing = [original_size[0] * (original_spacing[0] / out_size[0]),\n",
        "                 original_size[1] * (original_spacing[1] / out_size[1]),\n",
        "                 original_size[2] * (original_spacing[2] / out_size[2])]\n",
        "  resample = sitk.ResampleImageFilter()\n",
        "  resample.SetOutputSpacing(out_spacing)\n",
        "  resample.SetOutputOrigin(itk_image.GetOrigin())\n",
        "  resample.SetSize(out_size)\n",
        "  resample.SetOutputDirection(itk_image.GetDirection())\n",
        "  resample.SetTransform(sitk.Transform())\n",
        "  if is_label:\n",
        "    resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
        "  else:\n",
        "    resample.SetInterpolator(sitk.sitkBSpline)\n",
        "  return resample.Execute(itk_image)\n",
        "\n",
        "#prepare\n",
        "def prepare(fpath, mode, norm = sitk.NormalizeImageFilter()):\n",
        "  image = sitk.ReadImage(fpath)\n",
        "  if mode == 'image':\n",
        "    image = resample_image(image, is_label = False)\n",
        "    image = norm.Execute(image)\n",
        "  if mode == 'label':\n",
        "    image = resample_image(image, is_label = True)\n",
        "    #cast and copy\n",
        "    cast_image = sitk.Cast(image, sitk.sitkUInt8)\n",
        "    cast_image.CopyInformation(image)\n",
        "    image = cast_image\n",
        "  return image\n",
        "\n",
        "#split data\n",
        "def split_data(df, test_size):\n",
        "  #split\n",
        "  df_train, df_val = train_test_split(df, test_size = test_size, random_state = 42)\n",
        "  #update\n",
        "  df_train['DATA'] = 'TRAIN'\n",
        "  df_val['DATA'] = 'VAL'\n",
        "  #combine\n",
        "  df_new = pd.concat([df_train, df_val]).reset_index(drop = True)\n",
        "  return df_new\n",
        "\n",
        "#save image\n",
        "def save_image(save_path, save_folder, patient, image):\n",
        "  #save file\n",
        "  if os.path.isdir(save_path + save_folder) == False:\n",
        "    os.mkdir(save_path + save_folder)\n",
        "  #save file\n",
        "  save_loc = save_path + save_folder + '/' + patient + '.nii.gz'\n",
        "  #write\n",
        "  sitk.WriteImage(image, save_loc)\n",
        "  #return\n",
        "  return save_loc\n",
        "\n",
        "#prepare data\n",
        "def prepare_data(row, save_path, save_image_folder, save_label_folder):\n",
        "  image = prepare(row['Image'], mode = 'image')\n",
        "  label = prepare(row['Label'], mode = 'label')\n",
        "  image_path = save_image(save_path, save_image_folder, row['Patient'], image)\n",
        "  label_path = save_image(save_path, save_label_folder, row['Patient'], label)\n",
        "  return image_path, label_path\n",
        "\n",
        "#get bounding box no background and don't understand the coordinates\n",
        "def get_bbox(label, label_shape_filter = sitk.LabelShapeStatisticsImageFilter()):\n",
        "  label_shape_filter.Execute(label)\n",
        "  num_labels = label_shape_filter.GetLabels()\n",
        "  bboxes = torch.Tensor(len(num_labels), 6)\n",
        "  for label_num in num_labels:\n",
        "    bbox = label_shape_filter.GetBoundingBox(label_num)\n",
        "    bboxes[label_num - 1, :] = torch.Tensor(bbox)[None, :]\n",
        "  return bboxes.tolist()\n",
        "\n",
        "#get text\n",
        "def get_text(label, enc, label_shape_filter = sitk.LabelShapeStatisticsImageFilter()):\n",
        "  label_shape_filter.Execute(label)\n",
        "  num_labels = label_shape_filter.GetLabels()\n",
        "  texts = []\n",
        "  for num_label in num_labels:\n",
        "    texts.append(enc[num_label])\n",
        "  return texts\n",
        "\n",
        "#rand rows\n",
        "def rand_row(a, n = 1000):\n",
        "  return a[torch.randperm(a.size()[0])][0:n,:]\n",
        "\n",
        "#get bounding boxes\n",
        "def bounding_boxes(pos_labels):\n",
        "  max_val = torch.amax(pos_labels[:,0:3], dim = 0)\n",
        "  min_val = torch.amin(pos_labels[:,0:3], dim = 0)\n",
        "  return torch.concatenate([min_val, max_val], dim = 0).unsqueeze(0)\n",
        "\n",
        "#coords pointsLabelShapeStatisticsImageFilter()):\n",
        "def get_coords_points(label, label_shape_filter = sitk.LabelShapeStatisticsImageFilter()):\n",
        "  #init\n",
        "  x, y, z = label.GetSize()\n",
        "  arr = torch.zeros((x * y * z, 4))\n",
        "  a = 0\n",
        "  for i in range(x):\n",
        "    for j in range(y):\n",
        "      for k in range(z):\n",
        "        arr[a, 0] = i\n",
        "        arr[a, 1] = j\n",
        "        arr[a, 2] = k\n",
        "        arr[a, 3] = label[i, j, k]\n",
        "        a = a + 1\n",
        "  #execute\n",
        "  label_shape_filter.Execute(label)\n",
        "  num_labels = label_shape_filter.GetLabels()\n",
        "  point_labels = arr[:, 3]\n",
        "  #iterate\n",
        "  ls = []\n",
        "  for num_label in num_labels:\n",
        "    #get pos labels\n",
        "    pos_labels = arr[point_labels == num_label]\n",
        "    #get neg labels\n",
        "    neg_labels = arr[(point_labels != num_label) & (point_labels != 0)]\n",
        "    #get background labels\n",
        "    back_labels = arr[point_labels == 0]\n",
        "    #encode\n",
        "    pos_labels[:, 3] = 1\n",
        "    neg_labels[:, 3] = 0\n",
        "    back_labels[:, 3] = -1\n",
        "    #random rows (can't store all values)\n",
        "    pos_labels = rand_row(pos_labels)\n",
        "    neg_labels = rand_row(neg_labels)\n",
        "    back_labels = rand_row(back_labels)\n",
        "    #concat\n",
        "    coords_labels = torch.concatenate([pos_labels, neg_labels, back_labels], dim = 0)\n",
        "    #ls\n",
        "    ls.append(torch.unsqueeze(coords_labels, dim = 0))\n",
        "  #concat\n",
        "  return torch.concatenate(ls, dim = 0).tolist()\n",
        "\n",
        "#characterize\n",
        "def characterize(row, save_label_folder, enc):\n",
        "  label = sitk.ReadImage(row[save_label_folder])\n",
        "  bboxes = get_bbox(label)\n",
        "  coords_points = get_coords_points(label)\n",
        "  texts = get_text(label, enc)\n",
        "  return bboxes, coords_points, texts\n",
        "\n",
        "#characterize\n",
        "def characterize2(row, save_label_folder, enc):\n",
        "  label = sitk.ReadImage(row[save_label_folder])\n",
        "  bboxes, coords_points = get_coords_points_boxes(label)\n",
        "  texts = get_text(label, enc)\n",
        "  return bboxes, coords_points, texts\n",
        "\n",
        "#create tokenizer\n",
        "def create_tokenizer(text, save_folder, path = \"openai/clip-vit-base-patch32\", num_tokens = 10):\n",
        "  old_tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "  tokenizer = old_tokenizer.train_new_from_iterator(text, num_tokens)\n",
        "  tokenizer.save_pretrained(save_folder)\n",
        "\n",
        "#load tokenizer\n",
        "def load_tokenizer(save_folder):\n",
        "  return AutoTokenizer.from_pretrained(save_folder)\n",
        "\n",
        "#get the coordinates, labels, boxes\n",
        "def get_coords_points_boxes(label, label_shape_filter = sitk.LabelShapeStatisticsImageFilter()):\n",
        "  #execute\n",
        "  label_shape_filter.Execute(label)\n",
        "  #get num labels (nonzero)\n",
        "  num_labels = label_shape_filter.GetLabels()\n",
        "  #get sparse labels\n",
        "  label = torch.Tensor(sitk.GetArrayFromImage(label)) + 1\n",
        "  sparse_label = label.to_sparse()\n",
        "  #get indices and vals\n",
        "  point_labels = sparse_label.values() - 1\n",
        "  coord_points = torch.concatenate([sparse_label.indices().T, point_labels.unsqueeze(1)], dim = 1)\n",
        "  #init\n",
        "  ls_coords = []\n",
        "  ls_boxes = []\n",
        "  #iterate\n",
        "  for num_label in num_labels:\n",
        "    #get pos labels\n",
        "    pos_labels = coord_points[point_labels == num_label]\n",
        "    #get neg labels\n",
        "    neg_labels = coord_points[(point_labels != num_label) & (point_labels != 0)]\n",
        "    #get background labels\n",
        "    back_labels = coord_points[point_labels == 0]\n",
        "    #get bounding boxes from pos labels\n",
        "    boxes = bounding_boxes(pos_labels)\n",
        "    #encode\n",
        "    pos_labels[:, 3] = 1\n",
        "    neg_labels[:, 3] = 0\n",
        "    back_labels[:, 3] = -1\n",
        "    #random rows (can't store all values)\n",
        "    pos_labels = rand_row(pos_labels)\n",
        "    neg_labels = rand_row(neg_labels)\n",
        "    back_labels = rand_row(back_labels)\n",
        "    #concat\n",
        "    coords_labels = torch.concatenate([pos_labels, neg_labels, back_labels], dim = 0)\n",
        "    #ls\n",
        "    ls_coords.append(torch.unsqueeze(coords_labels, dim = 0))\n",
        "    #ls\n",
        "    ls_boxes.append(boxes)\n",
        "  #concat\n",
        "  output_coords = torch.concatenate(ls_coords, dim = 0).tolist()\n",
        "  #concat\n",
        "  output_boxes = torch.concatenate(ls_boxes, dim = 0).tolist()\n",
        "  #return\n",
        "  return output_boxes, output_coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSU27lnPSVyE"
      },
      "outputs": [],
      "source": [
        "#main\n",
        "untar('/content/gdrive/MyDrive/SAMMI/data/MSD/Task04_Hippocampus.tar', '/content/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBmTWnrfSleU"
      },
      "outputs": [],
      "source": [
        "#init\n",
        "data_path = '/content/Task04_Hippocampus/'\n",
        "val_size = 0.8\n",
        "save_path = '/content/gdrive/MyDrive/SAMMI/preprocessing/'\n",
        "save_image_folder = 'Norm_Image_64'\n",
        "save_label_folder = 'Label_64'\n",
        "enc = {1: 'anterior hippocampus', 2: 'posterior hippocampus'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu8qQIvoVww9",
        "outputId": "0ed22dad-e8a3-40f2-aa3a-06d10f68a56a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 6s, sys: 3.46 s, total: 2min 9s\n",
            "Wall time: 2min 31s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#main\n",
        "df = get_data(data_path)\n",
        "#split files\n",
        "df = split_data(df, val_size)\n",
        "#row\n",
        "df[save_image_folder], df[save_label_folder] = zip(*df.apply(prepare_data, axis = 1,\n",
        "                                                             args = (save_path, save_image_folder, save_label_folder)))\n",
        "#collect some more intel, bounding box, text, points\n",
        "df['Input_BBoxes'], df['Input_Coords_Points'], df['Input_Text'] = zip(*df.apply(characterize2, axis = 1,\n",
        "                                                                                args = (save_label_folder, enc)))\n",
        "#save\n",
        "df.to_pickle(save_path + 'df_inputs_64.pkl')\n",
        "#create the custom tokenizer\n",
        "create_tokenizer(list(enc.values()), save_path + 'tokenizer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_IKZlICAx92"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQM_lmhkAw7I",
        "outputId": "3cf70cbd-9b7e-4cfb-c06b-217b56c3ed78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount='True')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gUnwNU4Eqci"
      },
      "outputs": [],
      "source": [
        "#install\n",
        "!pip install --quiet SimpleITK monai wandb transformers einops livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15A0G2GXExCR",
        "outputId": "6ef6ccff-d904-4c84-bb26-4d49b004cc50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manishsalvi-osail\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json\n",
        "import SimpleITK as sitk\n",
        "import torch\n",
        "import torchvision\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "import monai\n",
        "import livelossplot\n",
        "from livelossplot import PlotLosses\n",
        "import transformers\n",
        "from transformers import CLIPTextConfig, CLIPTokenizer, CLIPTextModel, AutoTokenizer\n",
        "import einops\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Any, Optional, Tuple, Type, List, Dict\n",
        "from torch import Tensor\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  gpu = torch.cuda.get_device_name(0)\n",
        "  print('Device: ', gpu)\n",
        "else:\n",
        "  device = 'cpu'\n",
        "  gpu = None\n",
        "  print('Device', device)\n",
        "\n",
        "#wanddb\n",
        "key = \"0ededc30a9b327450989bd6593cec2dbb642fc7a\" #specify wandb key\n",
        "#Weights and Bias\n",
        "if key:\n",
        "  wandb.login(key=key) #API Key is in your wandb account, under settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP9O3cLLVMpa"
      },
      "outputs": [],
      "source": [
        "#Image Encoder\n",
        "\n",
        "# Copyright (c) ImageRx and Anish Salvi.\n",
        "# All rights reserved.\n",
        "\n",
        "class LayerNorm3d(nn.Module):\n",
        "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
        "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        u = x.mean(1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.eps)\n",
        "        x = self.weight[:, None, None, None] * x + self.bias[:, None, None, None]\n",
        "        return x\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        mlp_dim: int,\n",
        "        act: Type[nn.Module] = nn.GELU,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n",
        "        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n",
        "        self.act = act()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.lin2(self.act(self.lin1(x)))\n",
        "\n",
        "\n",
        "# This class and its supporting functions below lightly adapted from the ViTDet backbone available at: https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py # noqa\n",
        "class ImageEncoderViT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: Tuple[int, int, int] = (64, 64, 64),\n",
        "        patch_size: Tuple[int, int, int] = (16, 16, 16),\n",
        "        in_chans: int = 1,\n",
        "        embed_dim: int = 768, #embed_dim / num_heads = img_size? at least 64, not depth\n",
        "        depth: int = 12,\n",
        "        num_heads: int = 12,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        out_chans: int = 256,\n",
        "        qkv_bias: bool = True,\n",
        "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
        "        act_layer: Type[nn.Module] = nn.GELU,\n",
        "        use_abs_pos: bool = True,\n",
        "        use_rel_pos: bool = False,\n",
        "        rel_pos_zero_init: bool = True,\n",
        "        window_size: int = 0,\n",
        "        global_attn_indexes: Tuple[int, ...] = (),\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_size (int): Input image size.\n",
        "            patch_size (int): Patch size.\n",
        "            in_chans (int): Number of input image channels.\n",
        "            embed_dim (int): Patch embedding dimension.\n",
        "            depth (int): Depth of ViT.\n",
        "            num_heads (int): Number of attention heads in each ViT block.\n",
        "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "            norm_layer (nn.Module): Normalization layer.\n",
        "            act_layer (nn.Module): Activation layer.\n",
        "            use_abs_pos (bool): If True, use absolute positional embeddings.\n",
        "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
        "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
        "            window_size (int): Window size for window attention blocks.\n",
        "            global_attn_indexes (list): Indexes for blocks using global attention.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        #store\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.out_chans = out_chans\n",
        "\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            kernel_size = patch_size,\n",
        "            stride = patch_size,\n",
        "            in_chans = in_chans,\n",
        "            embed_dim = embed_dim,\n",
        "        )\n",
        "\n",
        "        self.pos_embed: Optional[nn.Parameter] = None\n",
        "        if use_abs_pos:\n",
        "            # Initialize absolute positional embedding with pretrain image size.\n",
        "            self.pos_embed = nn.Parameter(\n",
        "                torch.zeros(1, img_size[0] // patch_size[0], img_size[1] // patch_size[1], img_size[2] // patch_size[2], embed_dim)\n",
        "            )\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            block = Block(\n",
        "                dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                norm_layer=norm_layer,\n",
        "                act_layer=act_layer,\n",
        "                use_rel_pos=use_rel_pos,\n",
        "                rel_pos_zero_init=rel_pos_zero_init,\n",
        "                window_size=window_size if i not in global_attn_indexes else 0,\n",
        "                input_size=(img_size[0] // patch_size[0], img_size[1] // patch_size[1], img_size[2] // patch_size[2]),\n",
        "            )\n",
        "            self.blocks.append(block)\n",
        "\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                embed_dim,\n",
        "                out_chans,\n",
        "                kernel_size=1,\n",
        "                bias=False,\n",
        "            ),\n",
        "            LayerNorm3d(out_chans),\n",
        "            nn.Conv3d(\n",
        "                out_chans,\n",
        "                out_chans,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "                bias=False,\n",
        "            ),\n",
        "            LayerNorm3d(out_chans),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.patch_embed(x)\n",
        "        if self.pos_embed is not None:\n",
        "            x = x + self.pos_embed\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.neck(x.permute(0, 4, 1, 2, 3))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer blocks with support of window attention and residual propagation blocks\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        qkv_bias: bool = True,\n",
        "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
        "        act_layer: Type[nn.Module] = nn.GELU,\n",
        "        use_rel_pos: bool = False,\n",
        "        rel_pos_zero_init: bool = True,\n",
        "        window_size: int = 0,\n",
        "        input_size: Optional[Tuple[int, int, int]] = None,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            num_heads (int): Number of attention heads in each ViT block.\n",
        "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "            norm_layer (nn.Module): Normalization layer.\n",
        "            act_layer (nn.Module): Activation layer.\n",
        "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
        "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
        "            window_size (int): Window size for window attention blocks. If it equals 0, then\n",
        "                use global attention.\n",
        "            input_size (tuple(int, int, int) or None): Input resolution for calculating the relative\n",
        "                positional parameter size.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = AttentionViT(\n",
        "            dim,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            use_rel_pos=use_rel_pos,\n",
        "            rel_pos_zero_init=rel_pos_zero_init,\n",
        "            input_size=input_size if window_size == 0 else (window_size, window_size, window_size),\n",
        "        )\n",
        "\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer)\n",
        "\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        # Window partition\n",
        "        if self.window_size > 0:\n",
        "            H, W, D = x.shape[1], x.shape[2], x.shape[3]\n",
        "            x, pad_hwd = window_partition(x, self.window_size)\n",
        "\n",
        "        x = self.attn(x)\n",
        "        # Reverse window partition\n",
        "        if self.window_size > 0:\n",
        "            x = window_unpartition(x, self.window_size, pad_hwd, (H, W, D))\n",
        "\n",
        "        x = shortcut + x\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionViT(nn.Module):\n",
        "    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int = 8,\n",
        "        qkv_bias: bool = True,\n",
        "        use_rel_pos: bool = False,\n",
        "        rel_pos_zero_init: bool = True,\n",
        "        input_size: Optional[Tuple[int, int, int]] = None,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n",
        "            rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
        "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
        "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
        "                positional parameter size.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim**-0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "        self.use_rel_pos = use_rel_pos\n",
        "        if self.use_rel_pos:\n",
        "            assert (\n",
        "                input_size is not None\n",
        "            ), \"Input size must be provided if using relative positional encoding.\"\n",
        "            # initialize relative positional embeddings\n",
        "            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n",
        "            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n",
        "            self.rel_pos_d = nn.Parameter(torch.zeros(2 * input_size[2] - 1, head_dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, H, W, D, _ = x.shape\n",
        "        # qkv with shape (3, B, nHead, H * W * D, C)\n",
        "        qkv = self.qkv(x).reshape(B, H * W * D, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
        "        # q, k, v with shape (B * nHead, H * W * D, C)\n",
        "        q, k, v = qkv.reshape(3, B * self.num_heads, H * W * D, -1).unbind(0)\n",
        "\n",
        "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
        "\n",
        "        if self.use_rel_pos:\n",
        "            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, self.rel_pos_d, (H, W, D), (H, W, D))\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        x = (attn @ v).view(B, self.num_heads, H, W, D, -1).permute(0, 2, 3, 4, 1, 5).reshape(B, H, W, D, -1)\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def window_partition(x: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int, int]]:\n",
        "    \"\"\"\n",
        "    Partition into non-overlapping windows with padding if needed.\n",
        "    Args:\n",
        "        x (tensor): input tokens with [B, C, H, W, D].\n",
        "        window_size (int): window size.\n",
        "\n",
        "    Returns:\n",
        "        windows: windows after partition with [B * num_windows, window_size, window_size, window_size, C].\n",
        "        (Hp, Wp, Dp): padded height and width before partition\n",
        "    \"\"\"\n",
        "    B, C, H, W, D = x.shape\n",
        "\n",
        "    pad_h = (window_size - H % window_size) % window_size\n",
        "    pad_w = (window_size - W % window_size) % window_size\n",
        "    pad_d = (window_size - D % window_size) % window_size\n",
        "    if pad_h > 0 or pad_w > 0 or pad_d > 0:\n",
        "        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h, 0, pad_d))\n",
        "    Hp, Wp, Dp = H + pad_h, W + pad_w, D + pad_d\n",
        "\n",
        "    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, Dp // window_size, window_size, C)\n",
        "    windows = x.contiguous().view(-1, window_size, window_size, window_size, C) #permute removed\n",
        "    return windows, (Hp, Wp, Dp)\n",
        "\n",
        "\n",
        "def window_unpartition(\n",
        "    windows: torch.Tensor, window_size: int, pad_hwd: Tuple[int, int, int], hwd: Tuple[int, int, int]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Window unpartition into original sequences and removing padding.\n",
        "    Args:\n",
        "        windows (tensor): input tokens with [B * num_windows, window_size, window_size, window_size, C].\n",
        "        window_size (int): window size.\n",
        "        pad_hwd (Tuple): padded height and width (Hp, Wp, Dp).\n",
        "        hwd (Tuple): original height and width (H, W, D) before padding.\n",
        "\n",
        "    Returns:\n",
        "        x: unpartitioned sequences with [B, C, H, W, D].\n",
        "    \"\"\"\n",
        "    Hp, Wp, Dp = pad_hwd\n",
        "    H, W, D = hwd\n",
        "    B = windows.shape[0] // (Hp * Wp * Dp // window_size // window_size // window_size)\n",
        "    x = windows.view(B, Hp // window_size, Wp // window_size, Dp // window_size, window_size, window_size, window_size, -1)\n",
        "    x = x.contiguous().view(B, Hp, Wp, Dp, -1) #permute removed\n",
        "\n",
        "    if Hp > H or Wp > W or Dp > D:\n",
        "        x = x[:, :H, :W, :D, :].contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Get relative positional embeddings according to the relative positions of\n",
        "        query and key sizes.\n",
        "    Args:\n",
        "        q_size (int): size of query q.\n",
        "        k_size (int): size of key k.\n",
        "        rel_pos (Tensor): relative position embeddings (L, C).\n",
        "\n",
        "    Returns:\n",
        "        Extracted positional embeddings according to relative positions.\n",
        "    \"\"\"\n",
        "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
        "    # Interpolate rel pos if needed.\n",
        "    if rel_pos.shape[0] != max_rel_dist:\n",
        "        # Interpolate rel pos.\n",
        "        rel_pos_resized = F.interpolate(\n",
        "            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n",
        "            size=max_rel_dist,\n",
        "            mode=\"linear\",\n",
        "        )\n",
        "        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n",
        "    else:\n",
        "        rel_pos_resized = rel_pos\n",
        "\n",
        "    # Scale the coords with short length if shapes for q and k are different.\n",
        "    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
        "    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
        "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
        "\n",
        "    return rel_pos_resized[relative_coords.long()]\n",
        "\n",
        "\n",
        "def add_decomposed_rel_pos(\n",
        "    attn: torch.Tensor,\n",
        "    q: torch.Tensor,\n",
        "    rel_pos_h: torch.Tensor,\n",
        "    rel_pos_w: torch.Tensor,\n",
        "    rel_pos_d: torch.Tensor,\n",
        "    q_size: Tuple[int, int, int],\n",
        "    k_size: Tuple[int, int, int],\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n",
        "    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950\n",
        "    Args:\n",
        "        attn (Tensor): attention map.\n",
        "        q (Tensor): query q in the attention layer with shape (B, q_h * q_w * q_d, C).\n",
        "        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
        "        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
        "        rel_pos_d (Tensor): relative position embeddings (Ld, C) for depth axis.\n",
        "        q_size (Tuple): spatial sequence size of query q with (q_h, q_w, q_d).\n",
        "        k_size (Tuple): spatial sequence size of key k with (k_h, k_w, k_d).\n",
        "\n",
        "    Returns:\n",
        "        attn (Tensor): attention map with added relative positional embeddings.\n",
        "    \"\"\"\n",
        "    q_h, q_w, q_d = q_size\n",
        "    k_h, k_w, k_d = k_size\n",
        "\n",
        "    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n",
        "    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n",
        "    Rd = get_rel_pos(q_d, k_d, rel_pos_d)\n",
        "\n",
        "    B, _, dim = q.shape\n",
        "    r_q = q.reshape(B, q_h, q_w, q_d, dim)\n",
        "    rel_h = torch.einsum(\"bhwdc,hkc->bhwdk\", r_q, Rh)\n",
        "    rel_w = torch.einsum(\"bhwdc,wkc->bhwdk\", r_q, Rw)\n",
        "    rel_d = torch.einsum(\"bhwdc,dkc->bhwdk\", r_q, Rd)\n",
        "\n",
        "    attn = (\n",
        "        attn.view(B, q_h, q_w, q_d, k_h, k_w, k_d)\n",
        "        + rel_h[:, :, :, :, :, None] + rel_w[:, :, :, :, None, :] + rel_d[:, :, :, None, :, :]\n",
        "        ).view(B, q_h * q_w * q_d, k_h * k_w * k_d)\n",
        "\n",
        "    return attn\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    Image to Patch Embedding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size: Tuple[int, int, int] = (16, 16, 16),\n",
        "        stride: Tuple[int, int, int] = (16, 16, 16),\n",
        "        padding: Tuple[int, int, int] = (0, 0, 0),\n",
        "        in_chans: int = 1,\n",
        "        embed_dim: int = 768,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            kernel_size (Tuple): kernel size of the projection layer.\n",
        "            stride (Tuple): stride of the projection layer.\n",
        "            padding (Tuple): padding size of the projection layer.\n",
        "            in_chans (int): Number of input image channels.\n",
        "            embed_dim (int): Patch embedding dimension.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.proj = nn.Conv3d(\n",
        "            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.proj(x)\n",
        "        # B C H W D -> B H W D C\n",
        "        x = x.permute(0, 2, 3, 4, 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ub1s-OZVMqM"
      },
      "outputs": [],
      "source": [
        "#Prompt Encoder\n",
        "\n",
        "# Copyright (c) ImageRx and Anish Salvi.\n",
        "# All rights reserved.\n",
        "\n",
        "def divide(test_tup1, test_tup2):\n",
        "  return tuple(ele1 // ele2 for ele1, ele2 in zip(test_tup1, test_tup2))\n",
        "\n",
        "def get_down_scaling(kernel_stride_size, mask_in_chans, embed_dim):\n",
        "      max_downscaling = nn.Sequential()\n",
        "      #iterate\n",
        "      for i in range(len(kernel_stride_size)):\n",
        "        max_downscaling.add_module(\"conv_\" + str(i), nn.Conv3d(mask_in_chans[i], mask_in_chans[i + 1],\n",
        "                                                           kernel_size = kernel_stride_size[i], stride = kernel_stride_size[i]))\n",
        "        max_downscaling.add_module(\"norm_\" + str(i), LayerNorm3d(mask_in_chans[i + 1]))\n",
        "        max_downscaling.add_module(\"act_\" + str(i), torch.nn.GELU())\n",
        "      max_downscaling.add_module(\"embed_\" + str(0), torch.nn.Conv3d(mask_in_chans[i + 1], embed_dim, kernel_size=1))\n",
        "      #return\n",
        "      return max_downscaling\n",
        "\n",
        "\n",
        "class PromptEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        input_image_size: Tuple[int, int, int],\n",
        "        patch_size: Tuple[int, int, int],\n",
        "        mask_in_chans: List,\n",
        "        kernel_stride_size: List,\n",
        "        num_attention_heads: int,\n",
        "        num_hidden_layers: int,\n",
        "        projection_dim: int,\n",
        "        intermediate_size: int,\n",
        "        max_position_embeddings: int,\n",
        "        activation: Type[nn.Module] = nn.GELU,\n",
        "        tokenizer: Optional[AutoTokenizer] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Encodes prompts for input to SAM's mask decoder.\n",
        "\n",
        "        Arguments:\n",
        "          embed_dim (int): The prompts' embedding dimension\n",
        "          input_image_size (int): The padded size of the image as input\n",
        "            to the image encoder, as (H, W, D).\n",
        "          patch_size (tuple(int, int, int)): The spatial size of the\n",
        "            image embedding, as (H, W, D).\n",
        "          mask_in_chans (int): The number of hidden channels used for\n",
        "            encoding input masks.\n",
        "          activation (nn.Module): The activation to use when encoding\n",
        "            input masks.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.input_image_size = input_image_size\n",
        "        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)\n",
        "\n",
        "        self.num_point_embeddings: int = 4  # pos/neg point + 2 box corners\n",
        "        point_embeddings = [nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)]\n",
        "        self.point_embeddings = nn.ModuleList(point_embeddings)\n",
        "        self.not_a_point_embed = nn.Embedding(1, embed_dim)\n",
        "        self.image_embedding_size = divide(self.input_image_size, patch_size)\n",
        "\n",
        "        #might need deeper network?\n",
        "        self.mask_downscaling = get_down_scaling(kernel_stride_size, mask_in_chans, embed_dim)\n",
        "        self.no_mask_embed = nn.Embedding(1, embed_dim)\n",
        "\n",
        "        #init\n",
        "        path = \"openai/clip-vit-base-patch32\"\n",
        "        #get config\n",
        "        config = CLIPTextConfig.from_pretrained(path)\n",
        "        #if\n",
        "        if tokenizer is None:\n",
        "          #get tokenizer\n",
        "          self.tokenizer = CLIPTokenizer.from_pretrained(path)\n",
        "          #set\n",
        "          config.bos_token_id = self.tokenizer.bos_token_id\n",
        "          config.eos_token_id = self.tokenizer.eos_token_id\n",
        "        else:\n",
        "          #get tokenizer\n",
        "          self.tokenizer = tokenizer\n",
        "          #set\n",
        "          config.bos_token_id = self.tokenizer.bos_token_id\n",
        "          config.eos_token_id = self.tokenizer.eos_token_id\n",
        "        #set\n",
        "        config.num_attention_heads = num_attention_heads\n",
        "        config.num_hidden_layers = num_hidden_layers\n",
        "        config.projection_dim = projection_dim\n",
        "        config.intermediate_size = intermediate_size\n",
        "        config.max_position_embeddings = max_position_embeddings\n",
        "        #set hidden state\n",
        "        config.hidden_size = embed_dim\n",
        "        #model\n",
        "        self.text_model = CLIPTextModel(config)\n",
        "\n",
        "    def get_dense_pe(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns the positional encoding used to encode point prompts,\n",
        "        applied to a dense set of points the shape of the image encoding.\n",
        "\n",
        "        Returns:\n",
        "          torch.Tensor: Positional encoding with shape\n",
        "            1x(embed_dim)x(embedding_h)x(embedding_w)x(embedding_d)\n",
        "        \"\"\"\n",
        "        return self.pe_layer(self.image_embedding_size).unsqueeze(0)\n",
        "\n",
        "    def _embed_points(\n",
        "        self,\n",
        "        points: torch.Tensor,\n",
        "        labels: torch.Tensor,\n",
        "        pad: bool,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Embeds point prompts.\"\"\"\n",
        "        points = points + 0.5  # Shift to center of pixel\n",
        "        if pad:\n",
        "            padding_point = torch.zeros((points.shape[0], 1, 3), device=points.device)\n",
        "            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)\n",
        "            points = torch.cat([points, padding_point], dim=1)\n",
        "            labels = torch.cat([labels, padding_label], dim=1)\n",
        "        point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)\n",
        "        point_embedding[labels == -1] = 0.0\n",
        "        point_embedding[labels == -1] += self.not_a_point_embed.weight\n",
        "        point_embedding[labels == 0] += self.point_embeddings[0].weight\n",
        "        point_embedding[labels == 1] += self.point_embeddings[1].weight\n",
        "        return point_embedding\n",
        "\n",
        "    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Embeds box prompts.\"\"\"\n",
        "        boxes = boxes + 0.5  # Shift to center of pixel\n",
        "        coords = boxes.reshape(-1, 2, 3)\n",
        "        corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size)\n",
        "        corner_embedding[:, 0, :] += self.point_embeddings[2].weight\n",
        "        corner_embedding[:, 1, :] += self.point_embeddings[3].weight\n",
        "        return corner_embedding\n",
        "\n",
        "    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Embeds mask inputs.\"\"\"\n",
        "        mask_embedding = self.mask_downscaling(masks)\n",
        "        return mask_embedding\n",
        "\n",
        "    def _embed_text(self, text):\n",
        "        \"\"\"Embeds text inputs.\"\"\"\n",
        "        inputs = self.tokenizer(text, padding=True, return_tensors=\"pt\").to(self._get_device())\n",
        "        outputs = self.text_model(**inputs)\n",
        "        return outputs.pooler_output[:, None, :]\n",
        "\n",
        "    def _get_batch_size(\n",
        "        self,\n",
        "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
        "        boxes: Optional[torch.Tensor],\n",
        "        masks: Optional[torch.Tensor],\n",
        "        text: Optional[List]\n",
        "    ) -> int:\n",
        "        \"\"\"\n",
        "        Gets the batch size of the output given the batch size of the input prompts.\n",
        "        \"\"\"\n",
        "        if points[0] is not None:\n",
        "          return points[0].shape[0]\n",
        "        elif boxes is not None:\n",
        "          return boxes.shape[0]\n",
        "        elif text is not None:\n",
        "          return len(text)\n",
        "        elif masks is not None:\n",
        "          return masks.shape[0]\n",
        "        else:\n",
        "          return 1\n",
        "\n",
        "    def _get_device(self) -> torch.device:\n",
        "        return self.point_embeddings[0].weight.device\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
        "        boxes: Optional[torch.Tensor],\n",
        "        masks: Optional[torch.Tensor],\n",
        "        text: Optional[List],\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Embeds different types of prompts, returning both sparse and dense\n",
        "        embeddings.\n",
        "\n",
        "        Arguments:\n",
        "          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n",
        "            and labels to embed.\n",
        "          boxes (torch.Tensor or none): boxes to embed\n",
        "          masks (torch.Tensor or none): masks to embed\n",
        "          text  (list): text to embed\n",
        "\n",
        "        Returns:\n",
        "          torch.Tensor: sparse embeddings for the points and boxes, with shape\n",
        "            BxNx(embed_dim), where N is determined by the number of input points\n",
        "            and boxes.\n",
        "          torch.Tensor: dense embeddings for the masks, in the shape\n",
        "            Bx(embed_dim)x(embed_H)x(embed_W)x(embed_D)\n",
        "        \"\"\"\n",
        "        bs = self._get_batch_size(points, boxes, masks, text)\n",
        "        sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())\n",
        "        if points[0] is not None:\n",
        "            coords, labels = points\n",
        "            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n",
        "            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\n",
        "        if boxes is not None:\n",
        "            box_embeddings = self._embed_boxes(boxes)\n",
        "            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\n",
        "        if text is not None:\n",
        "            text_embeddings = self._embed_text(text)\n",
        "            sparse_embeddings = torch.cat([sparse_embeddings, text_embeddings], dim=1)\n",
        "        if masks is not None:\n",
        "            dense_embeddings = self._embed_masks(masks)\n",
        "        else:\n",
        "            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1, 1).expand(\n",
        "                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1], self.image_embedding_size[2]\n",
        "                )\n",
        "\n",
        "        return sparse_embeddings, dense_embeddings\n",
        "\n",
        "\n",
        "class PositionEmbeddingRandom(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional encoding using random spatial frequencies.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:\n",
        "        super().__init__()\n",
        "        if scale is None or scale <= 0.0:\n",
        "            scale = 1.0\n",
        "        self.register_buffer(\n",
        "            \"positional_encoding_gaussian_matrix\",\n",
        "            scale * torch.randn((3, num_pos_feats)),\n",
        "        )\n",
        "\n",
        "    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n",
        "        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n",
        "        coords = 2 * coords - 1\n",
        "        coords = coords @ self.positional_encoding_gaussian_matrix\n",
        "        coords = 2 * np.pi * coords\n",
        "        # outputs d_1 x ... x d_n x C shape\n",
        "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n",
        "\n",
        "    def forward(self, size: Tuple[int, int, int]) -> torch.Tensor:\n",
        "        \"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\n",
        "        h, w, d = size\n",
        "        device: Any = self.positional_encoding_gaussian_matrix.device\n",
        "        grid = torch.ones((h, w, d), device=device, dtype=torch.float32)\n",
        "        #embed\n",
        "        x_embed = grid.cumsum(dim=0) - 0.5\n",
        "        y_embed = grid.cumsum(dim=1) - 0.5\n",
        "        z_embed = grid.cumsum(dim=2) - 0.5\n",
        "        #norm\n",
        "        x_embed = x_embed / h\n",
        "        y_embed = y_embed / w\n",
        "        z_embed = z_embed / d\n",
        "        #encode\n",
        "        pe = self._pe_encoding(torch.stack([x_embed, y_embed, z_embed], dim=-1))\n",
        "        return pe.permute(3, 0, 1, 2)  # C x H x W x D\n",
        "\n",
        "    def forward_with_coords(\n",
        "        self, coords_input: torch.Tensor, image_size: Tuple[int, int, int]\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"\n",
        "        coords = coords_input.clone()\n",
        "        coords[:, :, 0] = coords[:, :, 0] / image_size[0]\n",
        "        coords[:, :, 1] = coords[:, :, 1] / image_size[1]\n",
        "        coords[:, :, 2] = coords[:, :, 2] / image_size[2]\n",
        "        return self._pe_encoding(coords.to(torch.float))  # B x N x C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md1SXO7SVQuh"
      },
      "outputs": [],
      "source": [
        "#Transformer\n",
        "\n",
        "# Copyright (c) ImageRx and Anish Salvi.\n",
        "# All rights reserved.\n",
        "\n",
        "class TwoWayTransformerMLPBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        mlp_dim: int,\n",
        "        act: Type[nn.Module] = nn.GELU,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n",
        "        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n",
        "        self.act = act()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.lin2(self.act(self.lin1(x)))\n",
        "\n",
        "\n",
        "class TwoWayTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        depth: int,\n",
        "        embedding_dim: int,\n",
        "        num_heads: int,\n",
        "        mlp_dim: int,\n",
        "        activation: Type[nn.Module] = nn.ReLU,\n",
        "        attention_downsample_rate: int = 2,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        A transformer decoder that attends to an input image using\n",
        "        queries whose positional embedding is supplied.\n",
        "\n",
        "        Args:\n",
        "          depth (int): number of layers in the transformer\n",
        "          embedding_dim (int): the channel dimension for the input embeddings\n",
        "          num_heads (int): the number of heads for multihead attention. Must\n",
        "            divide embedding_dim\n",
        "          mlp_dim (int): the channel dimension internal to the MLP block\n",
        "          activation (nn.Module): the activation to use in the MLP block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(depth):\n",
        "            self.layers.append(\n",
        "                TwoWayAttentionBlock(\n",
        "                    embedding_dim=embedding_dim,\n",
        "                    num_heads=num_heads,\n",
        "                    mlp_dim=mlp_dim,\n",
        "                    activation=activation,\n",
        "                    attention_downsample_rate=attention_downsample_rate,\n",
        "                    skip_first_layer_pe=(i == 0),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.final_attn_token_to_image = TwoWayTransformerAttention(\n",
        "            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n",
        "        )\n",
        "        self.norm_final_attn = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        image_embedding: Tensor,\n",
        "        image_pe: Tensor,\n",
        "        point_embedding: Tensor,\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          image_embedding (torch.Tensor): image to attend to. Should be shape\n",
        "            B x embedding_dim x h x w x d for any h and w and d.\n",
        "          image_pe (torch.Tensor): the positional encoding to add to the image. Must\n",
        "            have the same shape as image_embedding.\n",
        "          point_embedding (torch.Tensor): the embedding to add to the query points.\n",
        "            Must have shape B x N_points x embedding_dim for any N_points.\n",
        "\n",
        "        Returns:\n",
        "          torch.Tensor: the processed point_embedding\n",
        "          torch.Tensor: the processed image_embedding\n",
        "        \"\"\"\n",
        "        # BxCxHxW -> BxHWxC == B x N_image_tokens x C\n",
        "        #bs, c, h, w, d = image_embedding.shape\n",
        "        image_embedding = image_embedding.flatten(2).permute(0, 2, 1)\n",
        "        image_pe = image_pe.flatten(2).permute(0, 2, 1)\n",
        "\n",
        "        # Prepare queries\n",
        "        queries = point_embedding\n",
        "        keys = image_embedding\n",
        "\n",
        "        # Apply transformer blocks and final layernorm\n",
        "        for layer in self.layers:\n",
        "            queries, keys = layer(\n",
        "                queries=queries,\n",
        "                keys=keys,\n",
        "                query_pe=point_embedding,\n",
        "                key_pe=image_pe,\n",
        "            )\n",
        "\n",
        "        # Apply the final attention layer from the points to the image\n",
        "        q = queries + point_embedding\n",
        "        k = keys + image_pe\n",
        "        attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)\n",
        "        queries = queries + attn_out\n",
        "        queries = self.norm_final_attn(queries)\n",
        "\n",
        "        return queries, keys\n",
        "\n",
        "\n",
        "class TwoWayAttentionBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        num_heads: int,\n",
        "        mlp_dim: int = 2048,\n",
        "        activation: Type[nn.Module] = nn.ReLU,\n",
        "        attention_downsample_rate: int = 2,\n",
        "        skip_first_layer_pe: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        A transformer block with four layers: (1) self-attention of sparse\n",
        "        inputs, (2) cross attention of sparse inputs to dense inputs, (3) mlp\n",
        "        block on sparse inputs, and (4) cross attention of dense inputs to sparse\n",
        "        inputs.\n",
        "\n",
        "        Arguments:\n",
        "          embedding_dim (int): the channel dimension of the embeddings\n",
        "          num_heads (int): the number of heads in the attention layers\n",
        "          mlp_dim (int): the hidden dimension of the mlp block\n",
        "          activation (nn.Module): the activation of the mlp block\n",
        "          skip_first_layer_pe (bool): skip the PE on the first layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.self_attn = TwoWayTransformerAttention(embedding_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.cross_attn_token_to_image = TwoWayTransformerAttention(\n",
        "            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.mlp = TwoWayTransformerMLPBlock(embedding_dim, mlp_dim, activation)\n",
        "        self.norm3 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.norm4 = nn.LayerNorm(embedding_dim)\n",
        "        self.cross_attn_image_to_token = TwoWayTransformerAttention(\n",
        "            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n",
        "        )\n",
        "\n",
        "        self.skip_first_layer_pe = skip_first_layer_pe\n",
        "\n",
        "    def forward(\n",
        "        self, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        # Self attention block\n",
        "        if self.skip_first_layer_pe:\n",
        "            queries = self.self_attn(q=queries, k=queries, v=queries)\n",
        "        else:\n",
        "            q = queries + query_pe\n",
        "            attn_out = self.self_attn(q=q, k=q, v=queries)\n",
        "            queries = queries + attn_out\n",
        "        queries = self.norm1(queries)\n",
        "\n",
        "        # Cross attention block, tokens attending to image embedding\n",
        "        q = queries + query_pe\n",
        "        k = keys + key_pe\n",
        "        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\n",
        "        queries = queries + attn_out\n",
        "        queries = self.norm2(queries)\n",
        "\n",
        "        # MLP block\n",
        "        mlp_out = self.mlp(queries)\n",
        "        queries = queries + mlp_out\n",
        "        queries = self.norm3(queries)\n",
        "\n",
        "        # Cross attention block, image embedding attending to tokens\n",
        "        q = queries + query_pe\n",
        "        k = keys + key_pe\n",
        "        attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\n",
        "        keys = keys + attn_out\n",
        "        keys = self.norm4(keys)\n",
        "\n",
        "        return queries, keys\n",
        "\n",
        "\n",
        "class TwoWayTransformerAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention layer that allows for downscaling the size of the embedding\n",
        "    after projection to queries, keys, and values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        num_heads: int,\n",
        "        downsample_rate: int = 1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.internal_dim = embedding_dim // downsample_rate\n",
        "        self.num_heads = num_heads\n",
        "        assert self.internal_dim % num_heads == 0, \"num_heads must divide embedding_dim.\"\n",
        "\n",
        "        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
        "        self.k_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
        "        self.v_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
        "        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
        "\n",
        "    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
        "        b, n, c = x.shape\n",
        "        x = x.reshape(b, n, num_heads, c // num_heads)\n",
        "        return x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\n",
        "\n",
        "    def _recombine_heads(self, x: Tensor) -> Tensor:\n",
        "        b, n_heads, n_tokens, c_per_head = x.shape\n",
        "        x = x.transpose(1, 2)\n",
        "        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n",
        "\n",
        "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
        "        # Input projections\n",
        "        q = self.q_proj(q)\n",
        "        k = self.k_proj(k)\n",
        "        v = self.v_proj(v)\n",
        "\n",
        "        # Separate into heads\n",
        "        q = self._separate_heads(q, self.num_heads)\n",
        "        k = self._separate_heads(k, self.num_heads)\n",
        "        v = self._separate_heads(v, self.num_heads)\n",
        "\n",
        "        # Attention\n",
        "        _, _, _, c_per_head = q.shape\n",
        "        attn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\n",
        "        attn = attn / math.sqrt(c_per_head)\n",
        "        attn = torch.softmax(attn, dim=-1)\n",
        "\n",
        "        # Get output\n",
        "        out = attn @ v\n",
        "        out = self._recombine_heads(out)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-WobkW_VRUu"
      },
      "outputs": [],
      "source": [
        "#Mask Decoder\n",
        "\n",
        "# Copyright (c) ImageRx and Anish Salvi.\n",
        "# All rights reserved.\n",
        "\n",
        "def get_output_scaling(kernel_stride_size, transformer_dim):\n",
        "      output_upscaling = nn.Sequential()\n",
        "      #iterate\n",
        "      for i in range(len(kernel_stride_size)):\n",
        "        output_upscaling.add_module(\"conv_\" + str(i), nn.ConvTranspose3d(transformer_dim[i], transformer_dim[i+1],\n",
        "                                                                         kernel_size = kernel_stride_size[i], stride = kernel_stride_size[i]))\n",
        "        if len(kernel_stride_size) - 1 == i:\n",
        "          output_upscaling.add_module(\"act_\" + str(i), torch.nn.GELU())\n",
        "          continue\n",
        "        else:\n",
        "          output_upscaling.add_module(\"norm_\" + str(i), LayerNorm3d(transformer_dim[i + 1]))\n",
        "          output_upscaling.add_module(\"act_\" + str(i), torch.nn.GELU())\n",
        "      #return\n",
        "      return output_upscaling\n",
        "\n",
        "def get_MLP(start_dim, end_dim, num_mask_tokens, num_layers):\n",
        "  output_hypernetworks_mlps = nn.ModuleList([\n",
        "      MLP(start_dim, start_dim, end_dim, num_layers)\n",
        "      for i in range(num_mask_tokens)\n",
        "      ])\n",
        "  return output_hypernetworks_mlps\n",
        "\n",
        "\n",
        "class MaskDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_stride_size: List,\n",
        "        transformer_dim: List,\n",
        "        patch_size: Tuple[int, int, int],\n",
        "        transformer: nn.Module,\n",
        "        num_multimask_outputs: int = 3,\n",
        "        activation: Type[nn.Module] = nn.GELU,\n",
        "        iou_head_depth: int = 3,\n",
        "        iou_head_hidden_dim: int = 256,\n",
        "        num_layers: int = 3\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Predicts masks given an image and prompt embeddings, using a\n",
        "        transformer architecture.\n",
        "\n",
        "        Arguments:\n",
        "          transformer_dim (int): the channel dimension of the transformer\n",
        "          patch_size Tuple(int, int, int): the patch size used in the encoder\n",
        "          transformer (nn.Module): the transformer used to predict masks\n",
        "          num_multimask_outputs (int): the number of masks to predict\n",
        "            when disambiguating masks\n",
        "          activation (nn.Module): the type of activation to use when\n",
        "            upscaling masks\n",
        "          iou_head_depth (int): the depth of the MLP used to predict\n",
        "            mask quality\n",
        "          iou_head_hidden_dim (int): the hidden dimension of the MLP\n",
        "            used to predict mask quality\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.transformer = transformer\n",
        "        self.num_multimask_outputs = num_multimask_outputs\n",
        "\n",
        "        self.iou_token = nn.Embedding(1, transformer_dim[0])\n",
        "        self.num_mask_tokens = num_multimask_outputs + 1\n",
        "        self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim[0])\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.output_upscaling = get_output_scaling(kernel_stride_size, transformer_dim)\n",
        "        self.output_hypernetworks_mlps = get_MLP(transformer_dim[0], transformer_dim[-1], self.num_mask_tokens, num_layers)\n",
        "\n",
        "        self.iou_prediction_head = MLP(\n",
        "            transformer_dim[0], iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        image_embeddings: torch.Tensor,\n",
        "        image_pe: torch.Tensor,\n",
        "        sparse_prompt_embeddings: torch.Tensor,\n",
        "        dense_prompt_embeddings: torch.Tensor,\n",
        "        multimask_output: bool,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Predict masks given image and prompt embeddings.\n",
        "\n",
        "        Arguments:\n",
        "          image_embeddings (torch.Tensor): the embeddings from the image encoder\n",
        "          image_pe (torch.Tensor): positional encoding with the shape of image_embeddings\n",
        "          sparse_prompt_embeddings (torch.Tensor): the embeddings of the points and boxes\n",
        "          dense_prompt_embeddings (torch.Tensor): the embeddings of the mask inputs\n",
        "          multimask_output (bool): Whether to return multiple masks or a single\n",
        "            mask.\n",
        "\n",
        "        Returns:\n",
        "          torch.Tensor: batched predicted masks\n",
        "          torch.Tensor: batched predictions of mask quality\n",
        "        \"\"\"\n",
        "        masks, iou_pred = self.predict_masks(\n",
        "            image_embeddings=image_embeddings,\n",
        "            image_pe=image_pe,\n",
        "            sparse_prompt_embeddings=sparse_prompt_embeddings,\n",
        "            dense_prompt_embeddings=dense_prompt_embeddings,\n",
        "        )\n",
        "\n",
        "        # Select the correct mask or masks for output\n",
        "        if multimask_output:\n",
        "            mask_slice = slice(1, None)\n",
        "        else:\n",
        "            mask_slice = slice(0, 1)\n",
        "        masks = masks[:, mask_slice, :, :, :]\n",
        "        iou_pred = iou_pred[:, mask_slice]\n",
        "\n",
        "        # Prepare output\n",
        "        return masks, iou_pred\n",
        "\n",
        "    def predict_masks(\n",
        "        self,\n",
        "        image_embeddings: torch.Tensor,\n",
        "        image_pe: torch.Tensor,\n",
        "        sparse_prompt_embeddings: torch.Tensor,\n",
        "        dense_prompt_embeddings: torch.Tensor,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Predicts masks. See 'forward' for more details.\"\"\"\n",
        "        # Concatenate output tokens\n",
        "        output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n",
        "        output_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1)\n",
        "        tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)\n",
        "\n",
        "        # Expand per-image data in batch direction to be per-mask\n",
        "        src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n",
        "        src = src + dense_prompt_embeddings\n",
        "        pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n",
        "        b, c, h, w, d = src.shape\n",
        "\n",
        "        # Run the transformer\n",
        "        hs, src = self.transformer(src, pos_src, tokens)\n",
        "        iou_token_out = hs[:, 0, :]\n",
        "        mask_tokens_out = hs[:, 1 : (1 + self.num_mask_tokens), :]\n",
        "\n",
        "        # Upscale mask embeddings and predict masks using the mask tokens\n",
        "        src = src.transpose(1, 2).view(b, c, h, w, d)\n",
        "        upscaled_embedding = self.output_upscaling(src)\n",
        "        hyper_in_list: List[torch.Tensor] = []\n",
        "        #iterate\n",
        "        for i in range(self.num_mask_tokens):\n",
        "            hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n",
        "        hyper_in = torch.stack(hyper_in_list, dim=1)\n",
        "        #b, c, h, w = upscaled_embedding.shape this is likely if the upscaled embedding does not have the same size if altering upsampling\n",
        "        b, c, h, w, d = upscaled_embedding.shape\n",
        "        masks = (hyper_in @ upscaled_embedding.view(b, c, h * w * d)).view(b, -1, h, w, d)\n",
        "\n",
        "        # Generate mask quality predictions\n",
        "        iou_pred = self.iou_prediction_head(iou_token_out)\n",
        "\n",
        "        return masks, iou_pred\n",
        "\n",
        "\n",
        "# Lightly adapted from\n",
        "# https://github.com/facebookresearch/MaskFormer/blob/main/mask_former/modeling/transformer/transformer_predictor.py # noqa\n",
        "class MLP(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        output_dim: int,\n",
        "        num_layers: int,\n",
        "        sigmoid_output: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(\n",
        "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
        "        )\n",
        "        self.sigmoid_output = sigmoid_output\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        if self.sigmoid_output:\n",
        "            x = F.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbqaE8zdVTGA"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "\n",
        "class AnishSalviModel(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: Tuple[int, int, int] = (64, 64, 64),\n",
        "        patch_size: Tuple[int, int, int] = (8, 8, 8),\n",
        "        in_chans: int = 1,\n",
        "        image_embed_dim: int = 768, #has to be 768?\n",
        "        image_encoder_depth: int = 12,\n",
        "        image_encoder_num_heads: int = 12,\n",
        "        image_encoder_mlp_ratio: int = 4,\n",
        "        image_encoder_out_chans: int = 512, #same?\n",
        "        prompt_encoder_embed_dim: int = 512, #same?\n",
        "        prompt_encoder_mask_in_chans: List = [1, 3, 5],\n",
        "        prompt_encoder_kernel_stride_size: List = [(2, 2, 2), (4, 4, 4)],\n",
        "        prompt_encoder_num_attention_heads: int = 4,\n",
        "        prompt_encoder_num_hidden_layers: int = 12,\n",
        "        prompt_encoder_projection_dim: int = 64,\n",
        "        prompt_encoder_intermediate_size: int = 64,\n",
        "        prompt_encoder_max_position_embeddings: int = 10,\n",
        "        transformer_depth: int = 16,\n",
        "        transformer_embedding_dim: int = 512, #same?\n",
        "        transformer_num_heads: int = 16,\n",
        "        transformer_mlp_dim: int = 8,\n",
        "        mask_decoder_kernel_stride_size: List = [(4, 4, 4), (2, 2, 2)],\n",
        "        mask_decoder_transformer_dim: List = [512, 512, 512],\n",
        "        mask_decoder_num_multimask_outputs: int = 3,\n",
        "        mask_decoder_num_layers: int = 3,\n",
        "        tokenizer: Optional[AutoTokenizer] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        SAM predicts object masks from an image and input prompts.\n",
        "\n",
        "        Arguments:\n",
        "          image_encoder (ImageEncoderViT): The backbone used to encode the\n",
        "            image into image embeddings that allow for efficient mask prediction.\n",
        "          prompt_encoder (PromptEncoder): Encodes various types of input prompts.\n",
        "          transformer (TwoWayTransformer): Processes embeddings within mask_decoder\n",
        "          mask_decoder (MaskDecoder): Predicts masks from the image embeddings\n",
        "            and encoded prompts.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        #image encoder\n",
        "        self.image_encoder = ImageEncoderViT(img_size, patch_size, in_chans, image_embed_dim,\n",
        "                                image_encoder_depth, image_encoder_num_heads, image_encoder_mlp_ratio, image_encoder_out_chans)\n",
        "\n",
        "        #prompt encoder\n",
        "        self.prompt_encoder = PromptEncoder(prompt_encoder_embed_dim,\n",
        "                                            img_size, patch_size, prompt_encoder_mask_in_chans, prompt_encoder_kernel_stride_size,\n",
        "                                            prompt_encoder_num_attention_heads,\n",
        "                                            prompt_encoder_num_hidden_layers, prompt_encoder_projection_dim,\n",
        "                                            prompt_encoder_intermediate_size, prompt_encoder_max_position_embeddings, tokenizer)\n",
        "        #transformer\n",
        "        transformer = TwoWayTransformer(transformer_depth, transformer_embedding_dim,\n",
        "                                        transformer_num_heads, transformer_mlp_dim)\n",
        "        #mask decoder\n",
        "        self.mask_decoder = MaskDecoder(mask_decoder_kernel_stride_size, mask_decoder_transformer_dim, patch_size, transformer,\n",
        "                                        mask_decoder_num_multimask_outputs, mask_decoder_num_layers)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        batched_input: List[Dict[str, Any]],\n",
        "        multimask_output: bool,\n",
        "    ) -> List[Dict[str, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Predicts masks end-to-end from provided images and prompts.\n",
        "        If prompts are not known in advance, using ASMpredictor is\n",
        "        recommended over calling the model directly.\n",
        "\n",
        "        Arguments:\n",
        "          batched_input (list(dict)): A list over input images, each a\n",
        "            dictionary with the following keys. A prompt key can be\n",
        "            excluded if it is not present.\n",
        "              'image': The image as a torch tensor in CxHxWxD format,\n",
        "                already transformed for input to the model.\n",
        "              'point_coords': (torch.Tensor) Batched point prompts for\n",
        "                this image, with shape BxNx3. Already transformed to the\n",
        "                input frame of the model.\n",
        "              'point_labels': (torch.Tensor) Batched labels for point prompts,\n",
        "                with shape BxN.\n",
        "              'boxes': (torch.Tensor) Batched box inputs, with shape Bx6.\n",
        "                Already transformed to the input frame of the model.\n",
        "              'mask_inputs': (torch.Tensor) Batched mask inputs to the model,\n",
        "                in the form Bx1xHxWxD.\n",
        "          multimask_output (bool): Whether the model should predict multiple\n",
        "            disambiguating masks, or return a single mask.\n",
        "\n",
        "        Returns:\n",
        "          (list(dict)): A list over input images, where each element is\n",
        "            as dictionary with the following keys.\n",
        "              'iou_predictions': (torch.Tensor) The model's predictions\n",
        "                of mask quality, in shape BxC.\n",
        "              'low_res_logits': (torch.Tensor) Low resolution logits with\n",
        "                shape BxCxHxW. Can be passed as mask input\n",
        "                to subsequent iterations of prediction.\n",
        "        \"\"\"\n",
        "        input_images = torch.stack([x[\"image\"] for x in batched_input], dim=0)\n",
        "        image_embeddings = self.image_encoder(input_images)\n",
        "\n",
        "        outputs = []\n",
        "        for image_record, curr_embedding in zip(batched_input, image_embeddings):\n",
        "            #encode embeddings\n",
        "            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
        "                points=(image_record[\"point_coords\"], image_record[\"point_labels\"]),\n",
        "                boxes=image_record.get(\"boxes\", None),\n",
        "                masks=image_record.get(\"mask_inputs\", None),\n",
        "                text=image_record.get(\"text\", None)\n",
        "            )\n",
        "            #decode embeddings\n",
        "            low_res_masks, iou_predictions = self.mask_decoder(\n",
        "                image_embeddings=curr_embedding.unsqueeze(0),\n",
        "                image_pe=self.prompt_encoder.get_dense_pe(),\n",
        "                sparse_prompt_embeddings=sparse_embeddings,\n",
        "                dense_prompt_embeddings=dense_embeddings,\n",
        "                multimask_output=multimask_output,\n",
        "            )\n",
        "            outputs.append(\n",
        "                {\n",
        "                    \"iou_predictions\": iou_predictions,\n",
        "                    \"logits\": low_res_masks,\n",
        "                }\n",
        "            )\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIlmOvRrHGfO"
      },
      "outputs": [],
      "source": [
        "#functions\n",
        "\n",
        "#collate fn\n",
        "def collate_fn(input_dicts):\n",
        "  ls_dict = []\n",
        "  for input_dict in input_dicts:\n",
        "    ls_dict.append(input_dict)\n",
        "  return ls_dict\n",
        "\n",
        "#random sort rows batch\n",
        "def rand_batch_row(coords_points, num_points):\n",
        "  ls_coords = []\n",
        "  for coord_point in coords_points:\n",
        "    point_labels = coord_point[:, 3]\n",
        "    pos_labels = coord_point[point_labels == 1]\n",
        "    neg_labels = coord_point[point_labels == 0]\n",
        "    back_labels = coord_point[point_labels == -1]\n",
        "    #random rows (can't store all values)\n",
        "    pos_labels = rand_row(pos_labels, num_points)\n",
        "    neg_labels = rand_row(neg_labels, num_points)\n",
        "    back_labels = rand_row(back_labels, num_points)\n",
        "    #append\n",
        "    coords_labels = torch.concatenate([pos_labels, neg_labels, back_labels], dim = 0)\n",
        "    #sort\n",
        "    coords_labels = rand_row(coords_labels, coords_labels.shape[0])\n",
        "    ls_coords.append(torch.unsqueeze(coords_labels, dim = 0))\n",
        "  return torch.concatenate(ls_coords, dim = 0)\n",
        "\n",
        "#rand rows\n",
        "def rand_row(a, n = 10000):\n",
        "  return a[torch.randperm(a.size()[0])][0:n,:]\n",
        "\n",
        "#get bounding boxes\n",
        "def bounding_boxes(pos_labels):\n",
        "  max_val = torch.amax(pos_labels[:,0:3], dim = 0)\n",
        "  min_val = torch.amin(pos_labels[:,0:3], dim = 0)\n",
        "  return torch.concatenate([min_val, max_val], dim = 0).unsqueeze(0)\n",
        "\n",
        "#get the coordinates, labels, boxes\n",
        "def get_coords_points_boxes(label, num_points, num_labels = [1, 2]):\n",
        "  #get sparse labels\n",
        "  label = label + 1\n",
        "  sparse_label = label.to_sparse()\n",
        "  #get indices and vals\n",
        "  point_labels = sparse_label.values() - 1\n",
        "  coord_points = torch.concatenate([sparse_label.indices().T, point_labels.unsqueeze(1)], dim = 1)\n",
        "  #init\n",
        "  ls_coords = []\n",
        "  ls_boxes = []\n",
        "  #iterate\n",
        "  for num_label in num_labels:\n",
        "    #get pos labels\n",
        "    pos_labels = coord_points[point_labels == num_label]\n",
        "    #get neg labels\n",
        "    neg_labels = coord_points[(point_labels != num_label) & (point_labels != 0)]\n",
        "    #get background labels\n",
        "    back_labels = coord_points[point_labels == 0]\n",
        "    #get bounding boxes from pos labels\n",
        "    boxes = bounding_boxes(pos_labels)\n",
        "    #encode\n",
        "    pos_labels[:, 3] = 1\n",
        "    neg_labels[:, 3] = 0\n",
        "    back_labels[:, 3] = -1\n",
        "    #random rows (can't store all values)\n",
        "    pos_labels = rand_row(pos_labels, num_points)\n",
        "    neg_labels = rand_row(neg_labels, num_points)\n",
        "    back_labels = rand_row(back_labels, num_points)\n",
        "    #concat\n",
        "    coords_labels = torch.concatenate([pos_labels, neg_labels, back_labels], dim = 0)\n",
        "    #sort\n",
        "    coords_labels = rand_row(coords_labels, coords_labels.shape[0])\n",
        "    #ls\n",
        "    ls_coords.append(torch.unsqueeze(coords_labels, dim = 0))\n",
        "    #ls\n",
        "    ls_boxes.append(boxes)\n",
        "  #concat\n",
        "  output_coords = torch.concatenate(ls_coords, dim = 0)\n",
        "  #concat\n",
        "  output_boxes = torch.concatenate(ls_boxes, dim = 0)\n",
        "  #return\n",
        "  return output_boxes, output_coords\n",
        "\n",
        "#class\n",
        "class CustomImageDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df, data, aug = False, sub_points = 5):\n",
        "    #df\n",
        "    self.df = df[df['DATA'] == data]\n",
        "    #aug\n",
        "    self.aug = aug\n",
        "    #sub points\n",
        "    self.sub_points = sub_points\n",
        "    #angle\n",
        "    self.k = [1, 2, 3]\n",
        "    #dims\n",
        "    self.dims = [0, 1, 2]\n",
        "    #classes\n",
        "    self.num_labels = [1, 2]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "  def __getitem__(self, idx):\n",
        "    #row\n",
        "    row = self.df.iloc[idx]\n",
        "    #image height x width x depth\n",
        "    image = torch.Tensor(sitk.GetArrayFromImage(sitk.ReadImage(row['Norm_Image_64'])))\n",
        "    #label height x width x depth\n",
        "    label = torch.Tensor(sitk.GetArrayFromImage(sitk.ReadImage(row['Label_64'])))\n",
        "    #text\n",
        "    text = row['Input_Text']\n",
        "    #if\n",
        "    if self.aug:\n",
        "      #if random\n",
        "      if random.random() > 0.5:\n",
        "        k = random.choice(self.k)\n",
        "        dims = random.sample(self.dims, 2)\n",
        "        #roatet\n",
        "        image = torch.rot90(image, k, dims)\n",
        "        label = torch.rot90(label, k, dims)\n",
        "      #get augmentation stuff\n",
        "      boxes, coords_points = get_coords_points_boxes(label, self.sub_points, self.num_labels)\n",
        "      #subsample\n",
        "      coords_points = coords_points.to(torch.float32)\n",
        "      boxes = boxes.to(torch.float32)\n",
        "      #split num_mask x num_points x (num_coords + num_labels)\n",
        "      point_coords = coords_points[:, :, 0:3]\n",
        "      point_labels = coords_points[:, :, 3]\n",
        "    else:\n",
        "      #bounding boxes\n",
        "      boxes = torch.Tensor(row['Input_BBoxes']).to(torch.float32)\n",
        "      #input coords points\n",
        "      coords_points = torch.Tensor(row['Input_Coords_Points']).to(torch.float32)\n",
        "      #subsample\n",
        "      coords_points = rand_batch_row(coords_points, self.sub_points)\n",
        "      #split num_mask x num_points x (num_coords + num_labels)\n",
        "      point_coords = coords_points[:, :, 0:3]\n",
        "      point_labels = coords_points[:, :, 3]\n",
        "\n",
        "    #image\n",
        "    image = image.unsqueeze(0).to(torch.float32)\n",
        "    #one hot remove background sigmoid multilabel\n",
        "    label = torch.nn.functional.one_hot(label.to(torch.int64))[:, :, :, 1:].to(torch.float32)\n",
        "    #move and add axis\n",
        "    label = torch.moveaxis(label, 3, 0).unsqueeze(1)\n",
        "    #insert\n",
        "    dict0 = {\n",
        "        'image': image,\n",
        "        'point_coords': point_coords,\n",
        "        'point_labels': point_labels,\n",
        "        'boxes': boxes,\n",
        "        'mask_inputs': label,\n",
        "        'text': text\n",
        "        }\n",
        "    #return dict0\n",
        "    return dict0\n",
        "\n",
        "#dice focal loss multilabel\n",
        "class DiceFocalIOULoss:\n",
        "  def __init__(self, include_background = True, to_onehot_y = False, thresh = 0.5,\n",
        "               gamma = 2.0, focal_weight = None, lambda_dice = 1.0, lambda_focal = 1.0, lambda_iou = 1.0):\n",
        "    #dice focal loss\n",
        "    self.loss_fn1 =  monai.losses.DiceFocalLoss(include_background = include_background, to_onehot_y = to_onehot_y,\n",
        "                                               sigmoid = True, softmax = False,\n",
        "                                               gamma = gamma, focal_weight = focal_weight,\n",
        "                                                lambda_dice = lambda_dice, lambda_focal = lambda_focal)\n",
        "    self.fn2 = monai.metrics.MeanIoU(reduction = 'none')\n",
        "    #l1 loss\n",
        "    self.loss_fn3 = torch.nn.MSELoss()\n",
        "    #set\n",
        "    self.thresh = thresh\n",
        "    self.lambda_iou = lambda_iou\n",
        "    self.dicefocal_loss = 0\n",
        "    self.mse_loss = 0\n",
        "\n",
        "  def __call__(self, batched_output, batched_input):\n",
        "    #init\n",
        "    loss_fn1 = 0\n",
        "    loss_fn3 = 0\n",
        "    #iterate y_pred, y_true\n",
        "    for i, (output, input) in enumerate(zip(batched_output, batched_input)):\n",
        "      #if sigmoid else softmax\n",
        "      y_pred = torch.swapaxes(output['logits'], 0, 1)\n",
        "      #y_true\n",
        "      y_true = torch.swapaxes(input['mask_inputs'], 0, 1)\n",
        "      #dice focal loss\n",
        "      loss_fn1 += self.loss_fn1(y_pred, y_true)\n",
        "      #proxy for iou loss\n",
        "      y_pred = torch.sigmoid(y_pred)\n",
        "      iou_y_true = self.fn2(torch.where(y_pred > self.thresh, 1, 0), y_true)\n",
        "      #apply\n",
        "      loss_fn3 += self.loss_fn3(torch.sigmoid(output['iou_predictions']), iou_y_true.T)\n",
        "    #divide\n",
        "    self.dicefocal_loss = loss_fn1.item() / len(batched_input)\n",
        "    self.mse_loss = loss_fn3.item() / len(batched_input)\n",
        "    return (loss_fn1 + self.lambda_iou * loss_fn3) / len(batched_input)\n",
        "\n",
        "#load tokenizer\n",
        "def load_tokenizer(save_folder):\n",
        "  return AutoTokenizer.from_pretrained(save_folder)\n",
        "\n",
        "#save json file\n",
        "def save_params(hyper_params, save_path):\n",
        "  json_string = json.dumps(hyper_params)\n",
        "  with open(save_path, 'w') as outfile:\n",
        "    outfile.write(json_string)\n",
        "\n",
        "#load json file\n",
        "def load_params(fpath):\n",
        "  with open(fpath) as json_file:\n",
        "    data = json.load(json_file)\n",
        "  return data\n",
        "\n",
        "#get the optimizer\n",
        "def get_optimizer(model_config, model):\n",
        "  #AdamW\n",
        "  if model_config['optimizer'] == 'AdamW':\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr = model_config['init_lr'], weight_decay = model_config['weight_decay'])\n",
        "  #return\n",
        "  return optimizer\n",
        "\n",
        "#model saving policy\n",
        "def save_model(model_config, model):\n",
        "  #save model use weights instead\n",
        "  model.eval()\n",
        "  #depending on choice\n",
        "  if model_config['save_weights_only']:\n",
        "    torch.save(model.state_dict(), model_config['save_folder'] + 'model_weights.pth')\n",
        "  else:\n",
        "    torch.save(model, model_config['save_folder'] + 'model.pth')\n",
        "  #save info\n",
        "  save_params(model_config, model_config['save_folder'] + 'model_config.json')\n",
        "\n",
        "#update previously saved config only\n",
        "def update_config_stopearly(save_path):\n",
        "  #load\n",
        "  model_config = load_params(save_path + 'model_config.json')\n",
        "  #update\n",
        "  model_config['early_stopping']['stopped_early'] = True\n",
        "  #save\n",
        "  save_params(model_config, save_path + 'model_config.json')\n",
        "\n",
        "#new saving policy\n",
        "def new_saving_policy(early_stop, best_model, model_config, model, epoch):\n",
        "  #if there is an early stop\n",
        "  if early_stop:\n",
        "    #exit training\n",
        "    exit_training = True\n",
        "    #has the model been already saved?\n",
        "    if model_config['early_stopping']['model_criteria']:\n",
        "      #save just the config with update\n",
        "      update_config_stopearly(model_config['save_folder'])\n",
        "    #if not already saved\n",
        "    else:\n",
        "      #update\n",
        "      model_config['early_stopping']['stopped_early'] = True\n",
        "      #save the model and config\n",
        "      save_model(model_config, model)\n",
        "  #if there is not early stop\n",
        "  else:\n",
        "    #exit\n",
        "    exit_training = False\n",
        "    #need to log that we did not exit training early\n",
        "    model_config['early_stopping']['stopped_early'] = False\n",
        "    #you want to save the model every n_epochs often\n",
        "    if model_config['save_best_model'] == False:\n",
        "      #check if epocch is divisible and nonzero\n",
        "      if (epoch % model_config['save_after_n_epochs'] == 0) and (epoch != 0):\n",
        "        #indicate the model was saved\n",
        "        model_config['early_stopping']['model_criteria'] = True\n",
        "        #save\n",
        "        save_model(model_config, model)\n",
        "    else:\n",
        "      #you want to save the best model\n",
        "      if model_config['epochs_trained'] >= model_config['save_after_n_epochs']:\n",
        "        #check if current model is the best model\n",
        "        if best_model:\n",
        "          #log that it is the best model\n",
        "          model_config['early_stopping']['best_model'] = True\n",
        "          #indicate the model was saved\n",
        "          model_config['early_stopping']['model_criteria'] = True\n",
        "          #then save\n",
        "          save_model(model_config, model)\n",
        "        #if current model is not the best model but want to save for the initital run\n",
        "        if (best_model == False) and (model_config['epochs_trained'] == model_config['save_after_n_epochs']):\n",
        "          #inidicate the model was saved\n",
        "          model_config['early_stopping']['model_criteria'] = True\n",
        "          #save the model and config\n",
        "          save_model(model_config, model)\n",
        "  #return\n",
        "  return model_config, exit_training\n",
        "\n",
        "#class earlystopping\n",
        "class EarlyStopping:\n",
        "  #early stop if validation does not improve for given patience\n",
        "  def __init__(self, model_config, verbose = True, trace_func = print):\n",
        "    #set up\n",
        "    self.patience = model_config['early_stopping']['patience']\n",
        "    self.delta = model_config['early_stopping']['delta']\n",
        "    self.verbose = verbose\n",
        "    self.trace_func = trace_func\n",
        "    self.counter = 0\n",
        "    self.best_score = None\n",
        "    self.best_model = False\n",
        "    self.early_stop = False\n",
        "  #call\n",
        "  def __call__(self, val_loss):\n",
        "    #neg val loss\n",
        "    score = -val_loss\n",
        "    #init condition\n",
        "    if self.best_score is None:\n",
        "      self.best_score = score\n",
        "    #count number of times model failed to meet the condition\n",
        "    elif score < self.best_score + self.delta:\n",
        "      self.counter += 1\n",
        "      self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "      self.best_model = False\n",
        "      #identify when early stopping is required\n",
        "      if self.counter >= self.patience:\n",
        "        self.early_stop = True\n",
        "    #if the model shows best score\n",
        "    else:\n",
        "      #get the score and counter\n",
        "      self.best_score = score\n",
        "      self.counter = 0\n",
        "      self.best_model = True\n",
        "    #return the interl\n",
        "    return self.early_stop, self.best_model\n",
        "\n",
        "#get scheduler\n",
        "def get_scheduler(model_config, optimizer):\n",
        "  #plateau\n",
        "  if model_config['scheduler']['description'] == 'plateau':\n",
        "    #get the scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = model_config['scheduler']['mode'],\n",
        "                                                           factor = model_config['scheduler']['factor'],\n",
        "                                                           patience = model_config['scheduler']['patience'],\n",
        "                                                           threshold = model_config['scheduler']['threshold'],\n",
        "                                                           threshold_mode = model_config['scheduler']['threshold_mode'],\n",
        "                                                           cooldown = model_config['scheduler']['cooldown'],\n",
        "                                                           min_lr = model_config['scheduler']['min_lr'],\n",
        "                                                           eps = model_config['scheduler']['eps'],\n",
        "                                                           verbose = True)\n",
        "  #cosine\n",
        "  if model_config['scheduler']['description'] == 'cosine':\n",
        "    #get the scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                           T_max = model_config['epochs'],\n",
        "                                                           eta_min = model_config['scheduler']['eta_min'],\n",
        "                                                           verbose = True\n",
        "                                                           )\n",
        "  #steplr\n",
        "  if model_config['scheduler']['description'] == 'steplr':\n",
        "    #get the scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size = model_config['scheduler']['step_size'],\n",
        "                                                gamma = model_config['scheduler']['gamma'],\n",
        "                                                last_epoch = -1, verbose = True)\n",
        "  #return\n",
        "  return scheduler\n",
        "\n",
        "#get the model\n",
        "def get_model(model_config):\n",
        "  #get tokenizer\n",
        "  if model_config['architecture']['tokenizer'] is not None:\n",
        "    tokenizer = load_tokenizer(model_config['architecture']['tokenizer'])\n",
        "  else:\n",
        "    tokenizer = None\n",
        "  #get model\n",
        "  if model_config['architecture']['description'] == 'AnishSalviModel':\n",
        "    model = AnishSalviModel(\n",
        "        img_size = model_config['architecture']['img_size'],\n",
        "        patch_size = model_config['architecture']['patch_size'],\n",
        "        in_chans = model_config['architecture']['in_chans'],\n",
        "        image_embed_dim = model_config['architecture']['image_embed_dim'],\n",
        "        image_encoder_depth = model_config['architecture']['image_encoder_depth'],\n",
        "        image_encoder_num_heads = model_config['architecture']['image_encoder_depth'],\n",
        "        image_encoder_mlp_ratio = model_config['architecture']['image_encoder_mlp_ratio'],\n",
        "        image_encoder_out_chans = model_config['architecture']['image_encoder_out_chans'],\n",
        "        prompt_encoder_embed_dim = model_config['architecture']['prompt_encoder_embed_dim'],\n",
        "        prompt_encoder_mask_in_chans = model_config['architecture']['prompt_encoder_mask_in_chans'],\n",
        "        prompt_encoder_kernel_stride_size = model_config['architecture']['prompt_encoder_kernel_stride_size'],\n",
        "        prompt_encoder_num_attention_heads = model_config['architecture']['prompt_encoder_num_attention_heads'],\n",
        "        prompt_encoder_num_hidden_layers = model_config['architecture']['prompt_encoder_num_hidden_layers'],\n",
        "        prompt_encoder_projection_dim = model_config['architecture']['prompt_encoder_projection_dim'],\n",
        "        prompt_encoder_intermediate_size = model_config['architecture']['prompt_encoder_intermediate_size'],\n",
        "        prompt_encoder_max_position_embeddings = model_config['architecture']['prompt_encoder_max_position_embeddings'],\n",
        "        transformer_depth = model_config['architecture']['transformer_depth'],\n",
        "        transformer_embedding_dim = model_config['architecture']['transformer_embedding_dim'], #same?\n",
        "        transformer_num_heads = model_config['architecture']['transformer_num_heads'],\n",
        "        transformer_mlp_dim = model_config['architecture']['transformer_mlp_dim'],\n",
        "        mask_decoder_kernel_stride_size = model_config['architecture']['mask_decoder_kernel_stride_size'],\n",
        "        mask_decoder_transformer_dim = model_config['architecture']['mask_decoder_transformer_dim'],\n",
        "        mask_decoder_num_multimask_outputs = model_config['architecture']['mask_decoder_num_multimask_outputs'],\n",
        "        mask_decoder_num_layers = model_config['architecture']['mask_decoder_num_layers'],\n",
        "        tokenizer = tokenizer)\n",
        "  #return\n",
        "  return model\n",
        "\n",
        "#load pretrained model\n",
        "def load_pretrained_model(save_folder, device):\n",
        "  #load\n",
        "  model_config = load_params(save_folder + 'model_config.json')\n",
        "  #weights\n",
        "  if model_config['save_weights_only']:\n",
        "    model = get_model(model_config)\n",
        "    #important to do prior\n",
        "    model.eval()\n",
        "    #load weights\n",
        "    model.load_state_dict(torch.load(model_config['save_folder'] + 'model_weights.pth', map_location = device))\n",
        "  else:\n",
        "    model = torch.load(model_config['save_folder'] + 'model.pth', map_location = device)\n",
        "  #send\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  #return\n",
        "  return model, model_config\n",
        "\n",
        "#get the loss function\n",
        "def get_loss(model_config):\n",
        "  #dice focal iou multilabel\n",
        "  if model_config['loss']['description'] == 'dicefocalioumultilabel':\n",
        "    criterion = DiceFocalIOULoss()\n",
        "  #return\n",
        "  return criterion\n",
        "\n",
        "#send and clear gpu mem\n",
        "def clear_input(ls_dict, device):\n",
        "  for dict0 in ls_dict:\n",
        "    dict0['image'] = None\n",
        "    dict0['point_coords'] = None\n",
        "    dict0['point_labels'] = None\n",
        "    dict0['boxes'] = None\n",
        "    dict0['mask_inputs'] = None\n",
        "    #reset\n",
        "    if device == 'cuda':\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "#send and clear gpu mem\n",
        "def clear_output(ls_dict, device):\n",
        "  for dict0 in ls_dict:\n",
        "    dict0['iou_predictions'] = None\n",
        "    dict0['logits'] = None\n",
        "    #reset\n",
        "    if device == 'cuda':\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "#send to gpu mem\n",
        "def send_data(ls_dict, device):\n",
        "  ls_out = []\n",
        "  for dict0 in ls_dict:\n",
        "    dict0['image'] = dict0['image'].to(device)\n",
        "    dict0['point_coords'] = dict0['point_coords'].to(device)\n",
        "    dict0['point_labels'] = dict0['point_labels'].to(device)\n",
        "    dict0['boxes'] = dict0['boxes'].to(device)\n",
        "    dict0['mask_inputs'] = dict0['mask_inputs'].to(device)\n",
        "    ls_out.append(dict0)\n",
        "  return ls_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNSZ25khax_p"
      },
      "outputs": [],
      "source": [
        "#main script\n",
        "def main(config = None):\n",
        "  #clear workspace when finished with a single model run\n",
        "  model, batched_input, batched_output, loss = (None, None, None, None)\n",
        "  dset_train, train_loader, dset_val, val_loader = (None, None, None, None)\n",
        "  criterion, optimizer, scheduler, early_stopper = (None, None, None, None)\n",
        "  #reset\n",
        "  if device == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  #name the model\n",
        "  model_name = datetime.now().strftime('3d-asm-%Y-%m-%d-%H-%M-%S')\n",
        "\n",
        "  #init a new wandb run (config = sweep_config)\n",
        "  with wandb.init(config = config, name = model_name):\n",
        "    #set up the config (WandB, locked)\n",
        "    config = wandb.config\n",
        "    #dict (not locked)\n",
        "    model_config = dict(config)\n",
        "    #name the model\n",
        "    model_config['model'] = model_name\n",
        "    #save location\n",
        "    model_config['save_folder'] = model_config['save_folder'] +  model_config['model'] + '/'\n",
        "    #create the model folder\n",
        "    if os.path.isdir(model_config['save_folder']) == False:\n",
        "      os.mkdir(model_config['save_folder'])\n",
        "\n",
        "    #get the training data\n",
        "    dset_train = CustomImageDataset(df = df, data = 'TRAIN', aug = model_config['aug'], sub_points = model_config['sub_points'])\n",
        "    #get the validation data\n",
        "    dset_val = CustomImageDataset(df = df, data = 'VAL', aug = model_config['aug'], sub_points = model_config['sub_points'])\n",
        "    #get the training loader\n",
        "    train_loader = torch.utils.data.DataLoader(dset_train, batch_size = model_config['train_batch_size'], collate_fn = collate_fn)\n",
        "    #get the validation loader\n",
        "    val_loader = torch.utils.data.DataLoader(dset_val, batch_size = model_config['val_batch_size'], collate_fn = collate_fn)\n",
        "\n",
        "    #init the model\n",
        "    model = get_model(model_config)\n",
        "    #send\n",
        "    model.to(device)\n",
        "    #optimizer\n",
        "    optimizer = get_optimizer(model_config, model)\n",
        "    #scheduler\n",
        "    scheduler = get_scheduler(model_config, optimizer)\n",
        "    #early stopping (save time during the sweep)\n",
        "    early_stopper = EarlyStopping(model_config)\n",
        "    #criterion\n",
        "    criterion = get_loss(model_config)\n",
        "\n",
        "    #track in Jupter Notebook\n",
        "    liveloss = PlotLosses()\n",
        "    #logs\n",
        "    logs = {}\n",
        "\n",
        "    #track for later\n",
        "    log_train_loss = []\n",
        "    log_train_acc = []\n",
        "    log_val_loss = []\n",
        "    log_val_acc = []\n",
        "\n",
        "    #iterate through the entire dataset\n",
        "    #+1 for shifting (python starts at 0)\n",
        "    for epoch in range(model_config['epochs_trained'] + 1, model_config['epochs'] + 1):\n",
        "\n",
        "      #determine train losses\n",
        "      train_epoch_loss = 0\n",
        "      #set for training\n",
        "      model.train()\n",
        "      #iterate through the training data\n",
        "      for i, batched_input in enumerate(train_loader):\n",
        "        #zero optimizer\n",
        "        optimizer.zero_grad()\n",
        "        #forward pass with autograd\n",
        "        batched_input = send_data(batched_input, device)\n",
        "        #with torch.amp.autocast(device, torch.float16):\n",
        "        batched_output = model(batched_input, False)\n",
        "        #get loss\n",
        "        loss = criterion(batched_output, batched_input)\n",
        "        #backward\n",
        "        loss.backward()\n",
        "        #step\n",
        "        optimizer.step()\n",
        "        #update metrics\n",
        "        train_epoch_loss = train_epoch_loss + loss.item()\n",
        "        #clear\n",
        "        batched_input = clear_input(batched_input, device)\n",
        "        batched_output = clear_output(batched_output, device)\n",
        "        batched_input, batched_output, loss = (None, None, None)\n",
        "        #clear\n",
        "        if device == 'cuda':\n",
        "          torch.cuda.empty_cache()\n",
        "      #loss\n",
        "      train_loss = train_epoch_loss / len(train_loader)\n",
        "      #accuracy\n",
        "      train_acc = 1 - train_loss\n",
        "\n",
        "      #determine validation losses\n",
        "      val_epoch_loss = 0\n",
        "      #specify eval\n",
        "      model.eval()\n",
        "      #iterate through the validation data\n",
        "      for i, batched_input in enumerate(val_loader):\n",
        "        #no grad\n",
        "        with torch.no_grad():\n",
        "          #forward pass with autograd\n",
        "          batched_input = send_data(batched_input, device)\n",
        "          #with torch.amp.autocast(device, torch.float16):\n",
        "          batched_output = model(batched_input, False)\n",
        "          #get loss\n",
        "          loss = criterion(batched_output, batched_input)\n",
        "        #print temp\n",
        "        #print(criterion.mse_loss)\n",
        "        #print(criterion.dicefocal_loss)\n",
        "        #update metrics\n",
        "        val_epoch_loss = val_epoch_loss + loss.item()\n",
        "        #clear\n",
        "        batched_input = clear_input(batched_input, device)\n",
        "        batched_output = clear_output(batched_output, device)\n",
        "        batched_input, batched_output, loss = (None, None, None)\n",
        "        #clear\n",
        "        if device == 'cuda':\n",
        "          torch.cuda.empty_cache()\n",
        "      #loss\n",
        "      val_loss = val_epoch_loss / len(val_loader)\n",
        "      #accuracy\n",
        "      val_acc = 1 - val_loss\n",
        "\n",
        "      #scheduler\n",
        "      if model_config['scheduler']['description'] == 'plateau':\n",
        "        scheduler.step(train_loss)\n",
        "      if (model_config['scheduler']['description'] == 'cosine') or (model_config['scheduler']['description'] == 'steplr'):\n",
        "        scheduler.step()\n",
        "\n",
        "      #record for training\n",
        "      log_train_loss.append(train_loss)\n",
        "      log_train_acc.append(train_acc)\n",
        "      #record for validation\n",
        "      log_val_loss.append(val_loss)\n",
        "      log_val_acc.append(val_acc)\n",
        "      #wont log lossess or acc after early stopping or save best model\n",
        "      #log the most recent info\n",
        "      model_config['train_loss'] = train_loss\n",
        "      model_config['train_acc'] = train_acc\n",
        "      model_config['val_loss'] = val_loss\n",
        "      model_config['val_acc'] = val_acc\n",
        "      #log all the info\n",
        "      model_config['log_train_loss'] = log_train_loss\n",
        "      model_config['log_train_acc'] = log_train_acc\n",
        "      model_config['log_val_loss'] = log_val_loss\n",
        "      model_config['log_val_acc'] = log_val_acc\n",
        "      #keep track of each epoch\n",
        "      model_config['epochs_trained'] = epoch\n",
        "      #print\n",
        "      print('Epoch {0} of {1}: Train Loss {2:.2g} & Acc {3:.2g} v Val Loss {4:.2g} and Acc {5:.2g}'.format(epoch,\n",
        "                                                                                                           model_config['epochs'],\n",
        "                                                                                                           train_loss, train_acc,\n",
        "                                                                                                           val_loss, val_acc))\n",
        "      #specify the logs\n",
        "      prefix = ''\n",
        "      logs['Loss'] = train_loss\n",
        "      logs['Acc'] = train_acc\n",
        "      #logs\n",
        "      prefix = 'val_'\n",
        "      logs[prefix + 'Loss'] = val_loss\n",
        "      logs[prefix + 'Acc'] = val_acc\n",
        "\n",
        "      #living loss\n",
        "      liveloss.update(logs)\n",
        "      #send\n",
        "      liveloss.send()\n",
        "\n",
        "      #wandb\n",
        "      wandb.log(model_config)\n",
        "\n",
        "      #determine if early stopping is required by training loss\n",
        "      early_stop, best_model = early_stopper(train_loss)\n",
        "      #saving policy and determine if training should be exited based on early stop and best model\n",
        "      model_config, exit_training = new_saving_policy(early_stop, best_model, model_config, model, epoch)\n",
        "      #exit training early\n",
        "      if exit_training:\n",
        "        print('Early Stop: Exit Training')\n",
        "        break\n",
        "\n",
        "    #clear workspace when finished with a single model run\n",
        "    model, batched_input, batched_output, loss = (None, None, None, None)\n",
        "    dset_train, train_loader, dset_val, val_loader = (None, None, None, None)\n",
        "    criterion, optimizer, scheduler, early_stopper = (None, None, None, None)\n",
        "    #reset\n",
        "    if device == 'cuda':\n",
        "      torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frjx6Ic-lyxC"
      },
      "outputs": [],
      "source": [
        "#parameters in wandb format\n",
        "sweep_config = {\n",
        "    #name decided later (sweep name)\n",
        "    'name': None,\n",
        "    #sweep method\n",
        "    'method': 'grid',\n",
        "    #metric\n",
        "    'metric': {\n",
        "        'name': 'val_acc',\n",
        "        'goal': 'maximize',\n",
        "    },\n",
        "    #values which may be altered wandb wants all components\n",
        "    'parameters': {\n",
        "        #description\n",
        "        'description': {'value': 'Any Segmentation Model for 3D Hippocampus Segmentation'},\n",
        "        #project in wandb\n",
        "        'project':{'value': 'ASM'},\n",
        "        'model': {'value': None}, #placeholder for actual name\n",
        "        #documentation\n",
        "        'data_path': {'value': '/content/gdrive/MyDrive/SAMMI/preprocessing/df_inputs_64.pkl'}, #the input csv with filepaths\n",
        "        #path to save the results of the sweep\n",
        "        'save_folder': {'value': '/content/gdrive/MyDrive/SAMMI/results/'},\n",
        "        'device': {'value': gpu},\n",
        "        #model architecture\n",
        "        'architecture': {'values': [\n",
        "            {'description': 'AnishSalviModel',\n",
        "             'tokenizer': '/content/gdrive/MyDrive/SAMMI/preprocessing/tokenizer/',\n",
        "             'img_size': (64, 64, 64),\n",
        "             'patch_size': (8, 8, 8),\n",
        "             'in_chans': 1,\n",
        "             'image_embed_dim': 768,\n",
        "             'image_encoder_depth': 12,\n",
        "             'image_encoder_num_heads': 12,\n",
        "             'image_encoder_mlp_ratio': 4,\n",
        "             'image_encoder_out_chans': 512, #same?\n",
        "             'prompt_encoder_embed_dim': 512, #same?\n",
        "             'prompt_encoder_mask_in_chans': [1, 3, 5],\n",
        "             'prompt_encoder_kernel_stride_size': [(2, 2, 2), (4, 4, 4)],\n",
        "             'prompt_encoder_num_attention_heads': 4,\n",
        "             'prompt_encoder_num_hidden_layers': 8,\n",
        "             'prompt_encoder_projection_dim': 64,\n",
        "             'prompt_encoder_intermediate_size': 64,\n",
        "             'prompt_encoder_max_position_embeddings': 64,\n",
        "             'transformer_depth': 16,\n",
        "             'transformer_embedding_dim': 512, #same?\n",
        "             'transformer_num_heads': 16,\n",
        "             'transformer_mlp_dim': 8,\n",
        "             'mask_decoder_kernel_stride_size': [(4, 4, 4), (2, 2, 2)],\n",
        "             'mask_decoder_transformer_dim': [512, 512, 512],\n",
        "             'mask_decoder_num_multimask_outputs': 2,\n",
        "             'mask_decoder_num_layers': 3\n",
        "             }\n",
        "        ]},\n",
        "        #training params\n",
        "        'train_batch_size': {'values': [2]},\n",
        "        'val_batch_size': {'values': [2]},\n",
        "        'aug': {'values': [True]},\n",
        "        'sub_points': {'values': [1]},\n",
        "        'init_lr': {'values': [5e-6]},\n",
        "        'epochs': {'values': [50]}, #max epochs to train\n",
        "        'epochs_trained': {'value': 0}, #this is updated in the script!\n",
        "        'save_after_n_epochs': {'value': 5}, #depends on if you want to save the best model\n",
        "        'weight_decay': {'values': [1e-9]},\n",
        "        'optimizer': {'values': ['AdamW']},\n",
        "        'scheduler': {'values': [\n",
        "            {'description': 'plateau',\n",
        "             'mode': 'min',\n",
        "             'factor': 25e-2,\n",
        "             'patience': 3, #2\n",
        "             'threshold': 5e-2, #1e-2\n",
        "             'threshold_mode': 'rel',\n",
        "             'cooldown': 0,\n",
        "             'min_lr': 0,\n",
        "             'eps': 1}]},\n",
        "        'loss': {'values': [\n",
        "            {'description': 'dicefocalioumultilabel'},\n",
        "            ]},\n",
        "        #saving\n",
        "        'save_weights_only': {'value': True},\n",
        "        'save_best_model': {'value': True},\n",
        "        #early stopping\n",
        "        'early_stopping': {'value':\n",
        "            {'patience': 7,\n",
        "            'delta': 1e-3, #1e-2 or 5e-3\n",
        "            'stopped_early': None, #indicate if stopped early\n",
        "            'best_model': None, #indicate if best model (if save best model)\n",
        "            'model_criteria': False}\n",
        "        },\n",
        "        #log the model loss and acc\n",
        "        'log_train_loss': {'value': None},\n",
        "        'log_train_acc': {'value': None},\n",
        "        'log_val_loss': {'value': None},\n",
        "        'log_val_acc': {'value': None},\n",
        "        #updating performance in WandB\n",
        "        'train_loss': {'value': None},\n",
        "        'train_acc': {'value': None},\n",
        "        'val_loss': {'value': None},\n",
        "        'val_acc': {'value': None}\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3121081a20bd430c93446e381ef0d5d6",
            "5eb900d7931a4436920606573d4738c9",
            "19e6acb7f53447328c5ffebf07cc3a3c",
            "245e32bc992f41118eed2230701f06e9",
            "c87ebe6d551e42e28fdb01e3a30e9b07",
            "c980cc79789442b8ae3524c5ba68c9a1",
            "cae9966d28ff4f65b11488adb1487590",
            "1816a4e188c847a998717cb487b04026"
          ]
        },
        "id": "cltCn8MnqADJ",
        "outputId": "246e0feb-2de3-487a-c900-ad777d00e67f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc\n",
            "\ttraining         \t (min:   -0.153, max:    0.772, cur:    0.772)\n",
            "\tvalidation       \t (min:   -0.064, max:    0.756, cur:    0.756)\n",
            "Loss\n",
            "\ttraining         \t (min:    0.228, max:    1.153, cur:    0.228)\n",
            "\tvalidation       \t (min:    0.244, max:    1.064, cur:    0.244)\n",
            "EarlyStopping counter: 7 out of 7\n",
            "Early Stop: Exit Training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.028 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.040134"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3121081a20bd430c93446e381ef0d5d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>aug</td><td></td></tr><tr><td>epochs</td><td></td></tr><tr><td>epochs_trained</td><td></td></tr><tr><td>init_lr</td><td></td></tr><tr><td>save_after_n_epochs</td><td></td></tr><tr><td>save_best_model</td><td></td></tr><tr><td>save_weights_only</td><td></td></tr><tr><td>sub_points</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_batch_size</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_batch_size</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>weight_decay</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>aug</td><td>True</td></tr><tr><td>data_path</td><td>/content/gdrive/MyDr...</td></tr><tr><td>description</td><td>Any Segmentation Mod...</td></tr><tr><td>device</td><td>cuda</td></tr><tr><td>epochs</td><td>50</td></tr><tr><td>epochs_trained</td><td>32</td></tr><tr><td>init_lr</td><td>1e-05</td></tr><tr><td>model</td><td>3d-asm-2023-08-05-01...</td></tr><tr><td>optimizer</td><td>AdamW</td></tr><tr><td>project</td><td>ASM</td></tr><tr><td>save_after_n_epochs</td><td>5</td></tr><tr><td>save_best_model</td><td>True</td></tr><tr><td>save_folder</td><td>/content/gdrive/MyDr...</td></tr><tr><td>save_weights_only</td><td>True</td></tr><tr><td>sub_points</td><td>1</td></tr><tr><td>train_acc</td><td>0.77219</td></tr><tr><td>train_batch_size</td><td>2</td></tr><tr><td>train_loss</td><td>0.22781</td></tr><tr><td>val_acc</td><td>0.75624</td></tr><tr><td>val_batch_size</td><td>2</td></tr><tr><td>val_loss</td><td>0.24376</td></tr><tr><td>weight_decay</td><td>0.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">3d-asm-2023-08-05-01-22-37</strong> at: <a href='https://wandb.ai/anishsalvi-osail/ASM/runs/dkq7q68c' target=\"_blank\">https://wandb.ai/anishsalvi-osail/ASM/runs/dkq7q68c</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230805_012243-dkq7q68c/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 56min 37s, sys: 11min 32s, total: 1h 8min 9s\n",
            "Wall time: 1h 17min 13s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#main script\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  #read the pickle file\n",
        "  df = pd.read_pickle(sweep_config['parameters']['data_path']['value'])\n",
        "\n",
        "  #specify the sweep save location\n",
        "  sweep_config['name'] = datetime.now().strftime('3d-asm-sweep-%Y-%m-%d-%H-%M-%S')\n",
        "  #set\n",
        "  sweep_config['parameters']['save_folder']['value'] = sweep_config['parameters']['save_folder']['value'] + sweep_config['name'] + '/'\n",
        "  #create the sweep folder\n",
        "  if os.path.isdir(sweep_config['parameters']['save_folder']['value']) == False:\n",
        "    os.mkdir(sweep_config['parameters']['save_folder']['value'])\n",
        "  #save the sweep config in the sweep folder\n",
        "  save_params(sweep_config, sweep_config['parameters']['save_folder']['value'] + 'sweep_config.json')\n",
        "  #now run the main script\n",
        "\n",
        "  #select the project folder\n",
        "  sweep_id = wandb.sweep(sweep_config, project = sweep_config['parameters']['project']['value'])\n",
        "  #execute the search\n",
        "  wandb.agent(sweep_id, main)\n",
        "  #finish\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "mXXF-T2TXkek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount='True')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKUKiMkAXopJ",
        "outputId": "9ac29225-6b40-4832-99d4-53e0632b4472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install\n",
        "!pip install --quiet SimpleITK monai transformers einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-Y6YRK6XpYg",
        "outputId": "6a902873-d86f-42d7-d53d-0e6a8b318e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import SimpleITK as sitk\n",
        "import monai\n",
        "import transformers\n",
        "import einops\n",
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from transformers import CLIPTextConfig, CLIPTokenizer, CLIPTextModel, AutoTokenizer\n",
        "import einops\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Any, Optional, Tuple, Type, List, Dict\n",
        "from torch import Tensor\n",
        "import math\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  gpu = torch.cuda.get_device_name(0)\n",
        "  print('Device: ', gpu)\n",
        "else:\n",
        "  device = 'cpu'\n",
        "  gpu = None\n",
        "  print('Device', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5tUakFUX3oS",
        "outputId": "bcfc94ce-83f2-4f43-f41b-5d0ef27a9d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Image Encoder\n",
        "\n",
        "# Copyright (c) ImageRx and Anish Salvi.\n",
        "# All rights reserved.\n",
        "\n",
        "class LayerNorm3d(nn.Module):\n",
        "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
        "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        u = x.mean(1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.eps)\n",
        "        x = self.weight[:, None, None, None] * x + self.bias[:, None, None, None]\n",
        "        return x\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        mlp_dim: int,\n",
        "        act: Type[nn.Module] = nn.GELU,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n",
        "        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n",
        "        self.act = act()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.lin2(self.act(self.lin1(x)))\n",
        "\n",
        "\n",
        "# This class and its supporting functions below lightly adapted from the ViTDet backbone available at: https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py # noqa\n",
        "class ImageEncoderViT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: Tuple[int, int, int] = (64, 64, 64),\n",
        "        patch_size: Tuple[int, int, int] = (16, 16, 16),\n",
        "        in_chans: int = 1,\n",
        "        embed_dim: int = 768, #embed_dim / num_heads = img_size? at least 64, not depth\n",
        "        depth: int = 12,\n",
        "        num_heads: int = 12,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        out_chans: int = 256,\n",
        "        qkv_bias: bool = True,\n",
        "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
        "        act_layer: Type[nn.Module] = nn.GELU,\n",
        "        use_abs_pos: bool = True,\n",
        "        use_rel_pos: bool = False,\n",
        "        rel_pos_zero_init: bool = True,\n",
        "        window_size: int = 0,\n",
        "        global_attn_indexes: Tuple[int, ...] = (),\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_size (int): Input image size.\n",
        "            patch_size (int): Patch size.\n",
        "            in_chans (int): Number of input image channels.\n",
        "            embed_dim (int): Patch embedding dimension.\n",
        "            depth (int): Depth of ViT.\n",
        "            num_heads (int): Number of attention heads in each ViT block.\n",
        "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "            norm_layer (nn.Module): Normalization layer.\n",
        "            act_layer (nn.Module): Activation layer.\n",
        "            use_abs_pos (bool): If True, use absolute positional embeddings.\n",
        "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
        "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
        "            window_size (int): Window size for window attention blocks.\n",
        "            global_attn_indexes (list): Indexes for blocks using global attention.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        #store\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.out_chans = out_chans\n",
        "\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            kernel_size = patch_size,\n",
        "            stride = patch_size,\n",
        "            in_chans = in_chans,\n",
        "            embed_dim = embed_dim,\n",
        "        )\n",
        "\n",
        "        self.pos_embed: Optional[nn.Parameter] = None\n",
        "        if use_abs_pos:\n",
        "            # Initialize absolute positional embedding with pretrain image size.\n",
        "            self.pos_embed = nn.Parameter(\n",
        "                torch.zeros(1, img_size[0] // patch_size[0], img_size[1] // patch_size[1], img_size[2] // patch_size[2], embed_dim)\n",
        "            )\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            block = Block(\n",
        "                dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                norm_layer=norm_layer,\n",
        "                act_layer=act_layer,\n",
        "                use_rel_pos=use_rel_pos,\n",
        "                rel_pos_zero_init=rel_pos_zero_init,\n",
        "                window_size=window_size if i not in global_attn_indexes else 0,\n",
        "                input_size=(img_size[0] // patch_size[0], img_size[1] // patch_size[1], img_size[2] // patch_size[2]),\n",
        "            )\n",
        "            self.blocks.append(block)\n",
        "\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                embed_dim,\n",
        "                out_chans,\n",
        "                kernel_size=1,\n",
        "                bias=False,\n",
        "            ),\n",
        "            LayerNorm3d(out_chans),\n",
        "            nn.Conv3d(\n",
        "                out_chans,\n",
        "                out_chans,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "                bias=False,\n",
        "            ),\n",
        "            LayerNorm3d(out_chans),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.patch_embed(x)\n",
        "        if self.pos_embed is not None:\n",
        "            x = x + self.pos_embed\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.neck(x.permute(0, 4, 1, 2, 3))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer blocks with support of window attention and residual propagation blocks\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        qkv_bias: bool = True,\n",
        "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
        "        act_layer: Type[nn.Module] = nn.GELU,\n",
        "        use_rel_pos: bool = False,\n",
        "        rel_pos_zero_init: bool = True,\n",
        "        window_size: int = 0,\n",
        "        input_size: Optional[Tuple[int, int, int]] = None,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            num_heads (int): Number of attention heads in each ViT block.\n",
        "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "            norm_layer (nn.Module): Normalization layer.\n",
        "            act_layer (nn.Module): Activation layer.\n",
        "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
        "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
        "            window_size (int): Window size for window attention blocks. If it equals 0, then\n",
        "                use global attention.\n",
        "            input_size (tuple(int, int, int) or None): Input resolution for calculating the relative\n",
        "                positional parameter size.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = AttentionViT(\n",
        "            dim,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            use_rel_pos=use_rel_pos,\n",
        "            rel_pos_zero_init=rel_pos_zero_init,\n",
        "            input_size=input_size if window_size == 0 else (window_size, window_size, window_size),\n",
        "        )\n",
        "\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer)\n",
        "\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        # Window partition\n",
        "        if self.window_size > 0:\n",
        "            H, W, D = x.shape[1], x.shape[2], x.shape[3]\n",
        "            x, pad_hwd = window_partition(x, self.window_size)\n",
        "\n",
        "        x = self.attn(x)\n",
        "        # Reverse window partition\n",
        "        if self.window_size > 0:\n",
        "            x = window_unpartition(x, self.window_size, pad_hwd, (H, W, D))\n",
        "\n",
        "        x = shortcut + x\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionViT(nn.Module):\n",
        "    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int = 8,\n",
        "        qkv_bias: bool = True,\n",
        "        use_rel_pos: bool = False,\n",
        "        rel_pos_zero_init: bool = True,\n",
        "        input_size: Optional[Tuple[int, int, int]] = None,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n",
        "            rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
        "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
        "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
        "                positional parameter size.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim**-0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "        self.use_rel_pos = use_rel_pos\n",
        "        if self.use_rel_pos:\n",
        "            assert (\n",
        "                input_size is not None\n",
        "            ), \"Input size must be provided if using relative positional encoding.\"\n",
        "            # initialize relative positional embeddings\n",
        "            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n",
        "            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n",
        "            self.rel_pos_d = nn.Parameter(torch.zeros(2 * input_size[2] - 1, head_dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, H, W, D, _ = x.shape\n",
        "        # qkv with shape (3, B, nHead, H * W * D, C)\n",
        "        qkv = self.qkv(x).reshape(B, H * W * D, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
        "        # q, k, v with shape (B * nHead, H * W * D, C)\n",
        "        q, k, v = qkv.reshape(3, B * self.num_heads, H * W * D, -1).unbind(0)\n",
        "\n",
        "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
        "\n",
        "        if self.use_rel_pos:\n",
        "            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, self.rel_pos_d, (H, W, D), (H, W, D))\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        x = (attn @ v).view(B, self.num_heads, H, W, D, -1).permute(0, 2, 3, 4, 1, 5).reshape(B, H, W, D, -1)\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def window_partition(x: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int, int]]:\n",
        "    \"\"\"\n",
        "    Partition into non-overlapping windows with padding if needed.\n",
        "    Args:\n",
        "        x (tensor): input tokens with [B, C, H, W, D].\n",
        "        window_size (int): window size.\n",
        "\n",
        "    Returns:\n",
        "        windows: windows after partition with [B * num_windows, window_size, window_size, window_size, C].\n",
        "        (Hp, Wp, Dp): padded height and width before partition\n",
        "    \"\"\"\n",
        "    B, C, H, W, D = x.shape\n",
        "\n",
        "    pad_h = (window_size - H % window_size) % window_size\n",
        "    pad_w = (window_size - W % window_size) % window_size\n",
        "    pad_d = (window_size - D % window_size) % window_size\n",
        "    if pad_h > 0 or pad_w > 0 or pad_d > 0:\n",
        "        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h, 0, pad_d))\n",
        "    Hp, Wp, Dp = H + pad_h, W + pad_w, D + pad_d\n",
        "\n",
        "    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, Dp // window_size, window_size, C)\n",
        "    windows = x.contiguous().view(-1, window_size, window_size, window_size, C) #permute removed\n",
        "    return windows, (Hp, Wp, Dp)\n",
        "\n",
        "\n",
        "def window_unpartition(\n",
        "    windows: torch.Tensor, window_size: int, pad_hwd: Tuple[int, int, int], hwd: Tuple[int, int, int]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Window unpartition into original sequences and removing padding.\n",
        "    Args:\n",
        "        windows (tensor): input tokens with [B * num_windows, window_size, window_size, window_size, C].\n",
        "        window_size (int): window size.\n",
        "        pad_hwd (Tuple): padded height and width (Hp, Wp, Dp).\n",
        "        hwd (Tuple): original height and width (H, W, D) before padding.\n",
        "\n",
        "    Returns:\n",
        "        x: unpartitioned sequences with [B, C, H, W, D].\n",
        "    \"\"\"\n",
        "    Hp, Wp, Dp = pad_hwd\n",
        "    H, W, D = hwd\n",
        "    B = windows.shape[0] // (Hp * Wp * Dp // window_size // window_size // window_size)\n",
        "    x = windows.view(B, Hp // window_size, Wp // window_size, Dp // window_size, window_size, window_size, window_size, -1)\n",
        "    x = x.contiguous().view(B, Hp, Wp, Dp, -1) #permute removed\n",
        "\n",
        "    if Hp > H or Wp > W or Dp > D:\n",
        "        x = x[:, :H, :W, :D, :].contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Get relative positional embeddings according to the relative positions of\n",
        "        query and key sizes.\n",
        "    Args:\n",
        "        q_size (int): size of query q.\n",
        "        k_size (int): size of key k.\n",
        "        rel_pos (Tensor): relative position embeddings (L, C).\n",
        "\n",
        "    Returns:\n",
        "        Extracted positional embeddings according to relative positions.\n",
        "    \"\"\"\n",
        "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
        "    # Interpolate rel pos if needed.\n",
        "    if rel_pos.shape[0] != max_rel_dist:\n",
        "        # Interpolate rel pos.\n",
        "        rel_pos_resized = F.interpolate(\n",
        "            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n",
        "            size=max_rel_dist,\n",
        "            mode=\"linear\",\n",
        "        )\n",
        "        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n",
        "    else:\n",
        "        rel_pos_resized = rel_pos\n",
        "\n",
        "    # Scale the coords with short length if shapes for q and k are different.\n",
        "    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
        "    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
        "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
        "\n",
        "    return rel_pos_resized[relative_coords.long()]\n",
        "\n",
        "\n",
        "def add_decomposed_rel_pos(\n",
        "    attn: torch.Tensor,\n",
        "    q: torch.Tensor,\n",
        "    rel_pos_h: torch.Tensor,\n",
        "    rel_pos_w: torch.Tensor,\n",
        "    rel_pos_d: torch.Tensor,\n",
        "    q_size: Tuple[int, int, int],\n",
        "    k_size: Tuple[int, int, int],\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n",
        "    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950\n",
        "    Args:\n",
        "        attn (Tensor): attention map.\n",
        "        q (Tensor): query q in the attention layer with shape (B, q_h * q_w * q_d, C).\n",
        "        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
        "        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
        "        rel_pos_d (Tensor): relative position embeddings (Ld, C) for depth axis.\n",
        "        q_size (Tuple): spatial sequence size of query q with (q_h, q_w, q_d).\n",
        "        k_size (Tuple): spatial sequence size of key k with (k_h, k_w, k_d).\n",
        "\n",
        "    Returns:\n",
        "        attn (Tensor): attention map with added relative positional embeddings.\n",
        "    \"\"\"\n",
        "    q_h, q_w, q_d = q_size\n",
        "    k_h, k_w, k_d = k_size\n",
        "\n",
        "    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n",
        "    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n",
        "    Rd = get_rel_pos(q_d, k_d, rel_pos_d)\n",
        "\n",
        "    B, _, dim = q.shape\n",
        "    r_q = q.reshape(B, q_h, q_w, q_d, dim)\n",
        "    rel_h = torch.einsum(\"bhwdc,hkc->bhwdk\", r_q, Rh)\n",
        "    rel_w = torch.einsum(\"bhwdc,wkc->bhwdk\", r_q, Rw)\n",
        "    rel_d = torch.einsum(\"bhwdc,dkc->bhwdk\", r_q, Rd)\n",
        "\n",
        "    attn = (\n",
        "        attn.view(B, q_h, q_w, q_d, k_h, k_w, k_d)\n",
        "        + rel_h[:, :, :, :, :, None] + rel_w[:, :, :, :, None, :] + rel_d[:, :, :, None, :, :]\n",
        "        ).view(B, q_h * q_w * q_d, k_h * k_w * k_d)\n",
        "\n",
        "    return attn\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    Image to Patch Embedding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size: Tuple[int, int, int] = (16, 16, 16),\n",
        "        stride: Tuple[int, int, int] = (16, 16, 16),\n",
        "        padding: Tuple[int, int, int] = (0, 0, 0),\n",
        "        in_chans: int = 1,\n",
        "        embed_dim: int = 768,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            kernel_size (Tuple): kernel size of the projection layer.\n",
        "            stride (Tuple): stride of the projection layer.\n",
        "            padding (Tuple): padding size of the projection layer.\n",
        "            in_chans (int): Number of input image channels.\n",
        "            embed_dim (int): Patch embedding dimension.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.proj = nn.Conv3d(\n",
        "            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.proj(x)\n",
        "        # B C H W D -> B H W D C\n",
        "        x = x.permute(0, 2, 3, 4, 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "M_nLwlBGYE19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Encoder\n",
        "\n",
        "# Copyright (c) ImageRx and Anish Salvi.\n",
        "# All rights reserved.\n",
        "\n",
        "def divide(test_tup1, test_tup2):\n",
        "  return tuple(ele1 // ele2 for ele1, ele2 in zip(test_tup1, test_tup2))\n",
        "\n",
        "def get_down_scaling(kernel_stride_size, mask_in_chans, embed_dim):\n",
        "      max_downscaling = nn.Sequential()\n",
        "      #iterate\n",
        "      for i in range(len(kernel_stride_size)):\n",
        "        max_downscaling.add_module(\"conv_\" + str(i), nn.Conv3d(mask_in_chans[i], mask_in_chans[i + 1],\n",
        "                                                           kernel_size = kernel_stride_size[i], stride = kernel_stride_size[i]))\n",
        "        max_downscaling.add_module(\"norm_\" + str(i), LayerNorm3d(mask_in_chans[i + 1]))\n",
        "        max_downscaling.add_module(\"act_\" + str(i), torch.nn.GELU())\n",
        "      max_downscaling.add_module(\"embed_\" + str(0), torch.nn.Conv3d(mask_in_chans[i + 1], embed_dim, kernel_size=1))\n",
        "      #return\n",
        "      return max_downscaling\n",
        "\n",
        "\n",
        "class PromptEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        input_image_size: Tuple[int, int, int],\n",
        "        patch_size: Tuple[int, int, int],\n",
        "        mask_in_chans: List,\n",
        "        kernel_stride_size: List,\n",
        "        num_attention_heads: int,\n",
        "        num_hidden_layers: int,\n",
        "        projection_dim: int,\n",
        "        intermediate_size: int,\n",
        "        max_position_embeddings: int,\n",
        "        activation: Type[nn.Module] = nn.GELU,\n",
        "        tokenizer: Optional[AutoTokenizer] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Encodes prompts for input to SAM's mask decoder.\n",
        "\n",
        "        Arguments:\n",
        "          embed_dim (int): The prompts' embedding dimension\n",
        "          input_image_size (int): The padded size of the image as input\n",
        "            to the image encoder, as (H, W, D).\n",
        "          patch_size (tuple(int, int, int)): The spatial size of the\n",
        "            image embedding, as (H, W, D).\n",
        "          mask_in_chans (int): The number of hidden channels used for\n",
        "            encoding input masks.\n",
        "          activation (nn.Module): The activation to use when encoding\n",
        "            input masks.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.input_image_size = input_image_size\n",
        "        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)\n",
        "\n",
        "        self.num_point_embeddings: int = 4  # pos/neg point + 2 box corners\n",
        "        point_embeddings = [nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)]\n",
        "        self.point_embeddings = nn.ModuleList(point_embeddings)\n",
        "        self.not_a_point_embed = nn.Embedding(1, embed_dim)\n",
        "        self.image_embedding_size = divide(self.input_image_size, patch_size)\n",
        "\n",
        "        #might need deeper network?\n",
        "        self.mask_downscaling = get_down_scaling(kernel_stride_size, mask_in_chans, embed_dim)\n",
        "        self.no_mask_embed = nn.Embedding(1, embed_dim)\n",
        "\n",
        "        #init\n",
        "        path = \"openai/clip-vit-base-patch32\"\n",
        "        #get config\n",
        "        config = CLIPTextConfig.from_pretrained(path)\n",
        "        #if\n",
        "        if tokenizer is None:\n",
        "          #get tokenizer\n",
        "          self.tokenizer = CLIPTokenizer.from_pretrained(path)\n",
        "          #set\n",
        "          config.bos_token_id = self.tokenizer.bos_token_id\n",
        "          config.eos_token_id = self.tokenizer.eos_token_id\n",
        "        else:\n",
        "          #get tokenizer\n",
        "          self.tokenizer = tokenizer\n",
        "          #set\n",
        "          config.bos_token_id = self.tokenizer.bos_token_id\n",
        "          config.eos_token_id = self.tokenizer.eos_token_id\n",
        "        #set\n",
        "        config.num_attention_heads = num_attention_heads\n",
        "        config.num_hidden_layers = num_hidden_layers\n",
        "        config.projection_dim = projection_dim\n",
        "        config.intermediate_size = intermediate_size\n",
        "        config.max_position_embeddings = max_position_embeddings\n",
        "        #set hidden state\n",
        "        config.hidden_size = embed_dim\n",
        "        #model\n",
        "        self.text_model = CLIPTextModel(config)\n",
        "\n",
        "    def get_dense_pe(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns the positional encoding used to encode point prompts,\n",
        "        applied to a dense set of points the shape of the image encoding.\n",
        "\n",
        "        Returns:\n",
        "          torch.Tensor: Positional encoding with shape\n",
        "            1x(embed_dim)x(embedding_h)x(embedding_w)x(embedding_d)\n",
        "        \"\"\"\n",
        "        return self.pe_layer(self.image_embedding_size).unsqueeze(0)\n",
        "\n",
        "    def _embed_points(\n",
        "        self,\n",
        "        points: torch.Tensor,\n",
        "        labels: torch.Tensor,\n",
        "        pad: bool,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Embeds point prompts.\"\"\"\n",
        "        points = points + 0.5  # Shift to center of pixel\n",
        "        if pad:\n",
        "            padding_point = torch.zeros((points.shape[0], 1, 3), device=points.device)\n",
        "            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)\n",
        "            points = torch.cat([points, padding_point], dim=1)\n",
        "            labels = torch.cat([labels, padding_label], dim=1)\n",
        "        point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)\n",
        "        point_embedding[labels == -1] = 0.0\n",
        "        point_embedding[labels == -1] += self.not_a_point_embed.weight\n",
        "        point_embedding[labels == 0] += self.point_embeddings[0].weight\n",
        "        point_embedding[labels == 1] += self.point_embeddings[1].weight\n",
        "        return point_embedding\n",
        "\n",
        "    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Embeds box prompts.\"\"\"\n",
        "        boxes = boxes + 0.5  # Shift to center of pixel\n",
        "        coords = boxes.reshape(-1, 2, 3)\n",
        "        corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size)\n",
        "        corner_embedding[:, 0, :] += self.point_embeddings[2].weight\n",
        "        corner_embedding[:, 1, :] += self.point_embeddings[3].weight\n",
        "        return corner_embedding\n",
        "\n",
        "    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Embeds mask inputs.\"\"\"\n",
        "        mask_embedding = self.mask_downscaling(masks)\n",
        "        return mask_embedding\n",
        "\n",
        "    def _embed_text(self, text):\n",
        "        \"\"\"Embeds text inputs.\"\"\"\n",
        "        inputs = self.tokenizer(text, padding=True, return_tensors=\"pt\").to(self._get_device())\n",
        "        outputs = self.text_model(**inputs)\n",
        "        return outputs.pooler_output[:, None, :]\n",
        "\n",
        "    def _get_batch_size(\n",
        "        self,\n",
        "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
        "        boxes: Optional[torch.Tensor],\n",
        "        masks: Optional[torch.Tensor],\n",
        "        text: Optional[List]\n",
        "    ) -> int:\n",
        "        \"\"\"\n",
        "        Gets the batch size of the output given the batch size of the input prompts.\n",
        "        \"\"\"\n",
        "        if points[0] is not None:\n",
        "          return points[0].shape[0]\n",
        "        elif boxes is not None:\n",
        "          return boxes.shape[0]\n",
        "        elif text is not None:\n",
        "          return len(text)\n",
        "        elif masks is not None:\n",
        "          return masks.shape[0]\n",
        "        else:\n",
        "          return 1\n",
        "\n",
        "    def _get_device(self) -> torch.device:\n",
        "        return self.point_embeddings[0].weight.device\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
        "        boxes: Optional[torch.Tensor],\n",
        "        masks: Optional[torch.Tensor],\n",
        "        text: Optional[List],\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Embeds different types of prompts, returning both sparse and dense\n",
        "        embeddings.\n",
        "\n",
        "        Arguments:\n",
        "          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n",
        "            and labels to embed.\n",
        "          boxes (torch.Tensor or none): boxes to embed\n",
        "          masks (torch.Tensor or none): masks to embed\n",
        "          text  (list): text to embed\n",
        "\n",
        "        Returns:\n",
        "          torch.Tensor: sparse embeddings for the points and boxes, with shape\n",
        "            BxNx(embed_dim), where N is determined by the number of input points\n",
        "            and boxes.\n",
        "          torch.Tensor: dense embeddings for the masks, in the shape\n",
        "            Bx(embed_dim)x(embed_H)x(embed_W)x(embed_D)\n",
        "        \"\"\"\n",
        "        bs = self._get_batch_size(points, boxes, masks, text)\n",
        "        sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())\n",
        "        if points[0] is not None:\n",
        "            coords, labels = points\n",
        "            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n",
        "            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\n",
        "        if boxes is not None:\n",
        "            box_embeddings = self._embed_boxes(boxes)\n",
        "            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\n",
        "        if text is not None:\n",
        "            text_embeddings = self._embed_text(text)\n",
        "            sparse_embeddings = torch.cat([sparse_embeddings, text_embeddings], dim=1)\n",
        "        if masks is not None:\n",
        "            dense_embeddings = self._embed_masks(masks)\n",
        "        else:\n",
        "            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1, 1).expand(\n",
        "                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1], self.image_embedding_size[2]\n",
        "                )\n",
        "\n",
        "        return sparse_embeddings, dense_embeddings\n",
        "\n",
        "\n",
        "class PositionEmbeddingRandom(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional encoding using random spatial frequencies.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:\n",
        "        super().__init__()\n",
        "        if scale is None or scale <= 0.0:\n",
        "            scale = 1.0\n",
        "        self.register_buffer(\n",
        "            \"positional_encoding_gaussian_matrix\",\n",
        "            scale * torch.randn((3, num_pos_feats)),\n",
        "        )\n",
        "\n",
        "    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n",
        "        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n",
        "        coords = 2 * coords - 1\n",
        "        coords = coords @ self.positional_encoding_gaussian_matrix\n",
        "        coords = 2 * np.pi * coords\n",
        "        # outputs d_1 x ... x d_n x C shape\n",
        "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n",
        "\n",
        "    def forward(self, size: Tuple[int, int, int]) -> torch.Tensor:\n",
        "        \"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\n",
        "        h, w, d = size\n",
        "        device: Any = self.positional_encoding_gaussian_matrix.device\n",
        "        grid = torch.ones((h, w, d), device=device, dtype=torch.float32)\n",
        "        #embed\n",
        "        x_embed = grid.cumsum(dim=0) - 0.5\n",
        "        y_embed = grid.cumsum(dim=1) - 0.5\n",
        "        z_embed = grid.cumsum(dim=2) - 0.5\n",
        "        #norm\n",
        "        x_embed = x_embed / h\n",
        "        y_embed = y_embed / w\n",
        "        z_embed = z_embed / d\n",
        "        #encode\n",
        "        pe = self._pe_encoding(torch.stack([x_embed, y_embed, z_embed], dim=-1))\n",
        "        return pe.permute(3, 0, 1, 2)  # C x H x W x D\n",
        "\n",
        "    def forward_with_coords(\n",
        "        self, coords_input: torch.Tensor, image_size: Tuple[int, int, int]\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"\n",
        "        coords = coords_input.clone()\n",
        "        coords[:, :, 0] = coords[:, :, 0] / image_size[0]\n",
        "        coords[:, :, 1] = coords[:, :, 1] / image_size[1]\n",
        "        coords[:, :, 2] = coords[:, :, 2] / image_size[2]\n",
        "        return self._pe_encoding(coords.to(torch.float))  # B x N x C"
      ],
      "metadata": {
        "id": "oRnsuy--YI2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer\n",
        "\n",
        "# Copyright (c) ImageRx and Anish Salvi.\n",
        "# All rights reserved.\n",
        "\n",
        "class TwoWayTransformerMLPBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        mlp_dim: int,\n",
        "        act: Type[nn.Module] = nn.GELU,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n",
        "        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n",
        "        self.act = act()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.lin2(self.act(self.lin1(x)))\n",
        "\n",
        "\n",
        "class TwoWayTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        depth: int,\n",
        "        embedding_dim: int,\n",
        "        num_heads: int,\n",
        "        mlp_dim: int,\n",
        "        activation: Type[nn.Module] = nn.ReLU,\n",
        "        attention_downsample_rate: int = 2,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        A transformer decoder that attends to an input image using\n",
        "        queries whose positional embedding is supplied.\n",
        "\n",
        "        Args:\n",
        "          depth (int): number of layers in the transformer\n",
        "          embedding_dim (int): the channel dimension for the input embeddings\n",
        "          num_heads (int): the number of heads for multihead attention. Must\n",
        "            divide embedding_dim\n",
        "          mlp_dim (int): the channel dimension internal to the MLP block\n",
        "          activation (nn.Module): the activation to use in the MLP block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(depth):\n",
        "            self.layers.append(\n",
        "                TwoWayAttentionBlock(\n",
        "                    embedding_dim=embedding_dim,\n",
        "                    num_heads=num_heads,\n",
        "                    mlp_dim=mlp_dim,\n",
        "                    activation=activation,\n",
        "                    attention_downsample_rate=attention_downsample_rate,\n",
        "                    skip_first_layer_pe=(i == 0),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.final_attn_token_to_image = TwoWayTransformerAttention(\n",
        "            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n",
        "        )\n",
        "        self.norm_final_attn = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        image_embedding: Tensor,\n",
        "        image_pe: Tensor,\n",
        "        point_embedding: Tensor,\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          image_embedding (torch.Tensor): image to attend to. Should be shape\n",
        "            B x embedding_dim x h x w x d for any h and w and d.\n",
        "          image_pe (torch.Tensor): the positional encoding to add to the image. Must\n",
        "            have the same shape as image_embedding.\n",
        "          point_embedding (torch.Tensor): the embedding to add to the query points.\n",
        "            Must have shape B x N_points x embedding_dim for any N_points.\n",
        "\n",
        "        Returns:\n",
        "          torch.Tensor: the processed point_embedding\n",
        "          torch.Tensor: the processed image_embedding\n",
        "        \"\"\"\n",
        "        # BxCxHxW -> BxHWxC == B x N_image_tokens x C\n",
        "        #bs, c, h, w, d = image_embedding.shape\n",
        "        image_embedding = image_embedding.flatten(2).permute(0, 2, 1)\n",
        "        image_pe = image_pe.flatten(2).permute(0, 2, 1)\n",
        "\n",
        "        # Prepare queries\n",
        "        queries = point_embedding\n",
        "        keys = image_embedding\n",
        "\n",
        "        # Apply transformer blocks and final layernorm\n",
        "        for layer in self.layers:\n",
        "            queries, keys = layer(\n",
        "                queries=queries,\n",
        "                keys=keys,\n",
        "                query_pe=point_embedding,\n",
        "                key_pe=image_pe,\n",
        "            )\n",
        "\n",
        "        # Apply the final attention layer from the points to the image\n",
        "        q = queries + point_embedding\n",
        "        k = keys + image_pe\n",
        "        attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)\n",
        "        queries = queries + attn_out\n",
        "        queries = self.norm_final_attn(queries)\n",
        "\n",
        "        return queries, keys\n",
        "\n",
        "\n",
        "class TwoWayAttentionBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        num_heads: int,\n",
        "        mlp_dim: int = 2048,\n",
        "        activation: Type[nn.Module] = nn.ReLU,\n",
        "        attention_downsample_rate: int = 2,\n",
        "        skip_first_layer_pe: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        A transformer block with four layers: (1) self-attention of sparse\n",
        "        inputs, (2) cross attention of sparse inputs to dense inputs, (3) mlp\n",
        "        block on sparse inputs, and (4) cross attention of dense inputs to sparse\n",
        "        inputs.\n",
        "\n",
        "        Arguments:\n",
        "          embedding_dim (int): the channel dimension of the embeddings\n",
        "          num_heads (int): the number of heads in the attention layers\n",
        "          mlp_dim (int): the hidden dimension of the mlp block\n",
        "          activation (nn.Module): the activation of the mlp block\n",
        "          skip_first_layer_pe (bool): skip the PE on the first layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.self_attn = TwoWayTransformerAttention(embedding_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.cross_attn_token_to_image = TwoWayTransformerAttention(\n",
        "            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.mlp = TwoWayTransformerMLPBlock(embedding_dim, mlp_dim, activation)\n",
        "        self.norm3 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.norm4 = nn.LayerNorm(embedding_dim)\n",
        "        self.cross_attn_image_to_token = TwoWayTransformerAttention(\n",
        "            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n",
        "        )\n",
        "\n",
        "        self.skip_first_layer_pe = skip_first_layer_pe\n",
        "\n",
        "    def forward(\n",
        "        self, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        # Self attention block\n",
        "        if self.skip_first_layer_pe:\n",
        "            queries = self.self_attn(q=queries, k=queries, v=queries)\n",
        "        else:\n",
        "            q = queries + query_pe\n",
        "            attn_out = self.self_attn(q=q, k=q, v=queries)\n",
        "            queries = queries + attn_out\n",
        "        queries = self.norm1(queries)\n",
        "\n",
        "        # Cross attention block, tokens attending to image embedding\n",
        "        q = queries + query_pe\n",
        "        k = keys + key_pe\n",
        "        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\n",
        "        queries = queries + attn_out\n",
        "        queries = self.norm2(queries)\n",
        "\n",
        "        # MLP block\n",
        "        mlp_out = self.mlp(queries)\n",
        "        queries = queries + mlp_out\n",
        "        queries = self.norm3(queries)\n",
        "\n",
        "        # Cross attention block, image embedding attending to tokens\n",
        "        q = queries + query_pe\n",
        "        k = keys + key_pe\n",
        "        attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\n",
        "        keys = keys + attn_out\n",
        "        keys = self.norm4(keys)\n",
        "\n",
        "        return queries, keys\n",
        "\n",
        "\n",
        "class TwoWayTransformerAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention layer that allows for downscaling the size of the embedding\n",
        "    after projection to queries, keys, and values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        num_heads: int,\n",
        "        downsample_rate: int = 1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.internal_dim = embedding_dim // downsample_rate\n",
        "        self.num_heads = num_heads\n",
        "        assert self.internal_dim % num_heads == 0, \"num_heads must divide embedding_dim.\"\n",
        "\n",
        "        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
        "        self.k_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
        "        self.v_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
        "        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
        "\n",
        "    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
        "        b, n, c = x.shape\n",
        "        x = x.reshape(b, n, num_heads, c // num_heads)\n",
        "        return x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\n",
        "\n",
        "    def _recombine_heads(self, x: Tensor) -> Tensor:\n",
        "        b, n_heads, n_tokens, c_per_head = x.shape\n",
        "        x = x.transpose(1, 2)\n",
        "        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n",
        "\n",
        "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
        "        # Input projections\n",
        "        q = self.q_proj(q)\n",
        "        k = self.k_proj(k)\n",
        "        v = self.v_proj(v)\n",
        "\n",
        "        # Separate into heads\n",
        "        q = self._separate_heads(q, self.num_heads)\n",
        "        k = self._separate_heads(k, self.num_heads)\n",
        "        v = self._separate_heads(v, self.num_heads)\n",
        "\n",
        "        # Attention\n",
        "        _, _, _, c_per_head = q.shape\n",
        "        attn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\n",
        "        attn = attn / math.sqrt(c_per_head)\n",
        "        attn = torch.softmax(attn, dim=-1)\n",
        "\n",
        "        # Get output\n",
        "        out = attn @ v\n",
        "        out = self._recombine_heads(out)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "ozvglsgGYRAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mask Decoder\n",
        "\n",
        "# Copyright (c) ImageRx and Anish Salvi.\n",
        "# All rights reserved.\n",
        "\n",
        "def get_output_scaling(kernel_stride_size, transformer_dim):\n",
        "      output_upscaling = nn.Sequential()\n",
        "      #iterate\n",
        "      for i in range(len(kernel_stride_size)):\n",
        "        output_upscaling.add_module(\"conv_\" + str(i), nn.ConvTranspose3d(transformer_dim[i], transformer_dim[i+1],\n",
        "                                                                         kernel_size = kernel_stride_size[i], stride = kernel_stride_size[i]))\n",
        "        if len(kernel_stride_size) - 1 == i:\n",
        "          output_upscaling.add_module(\"act_\" + str(i), torch.nn.GELU())\n",
        "          continue\n",
        "        else:\n",
        "          output_upscaling.add_module(\"norm_\" + str(i), LayerNorm3d(transformer_dim[i + 1]))\n",
        "          output_upscaling.add_module(\"act_\" + str(i), torch.nn.GELU())\n",
        "      #return\n",
        "      return output_upscaling\n",
        "\n",
        "def get_MLP(start_dim, end_dim, num_mask_tokens, num_layers):\n",
        "  output_hypernetworks_mlps = nn.ModuleList([\n",
        "      MLP(start_dim, start_dim, end_dim, num_layers)\n",
        "      for i in range(num_mask_tokens)\n",
        "      ])\n",
        "  return output_hypernetworks_mlps\n",
        "\n",
        "\n",
        "class MaskDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_stride_size: List,\n",
        "        transformer_dim: List,\n",
        "        patch_size: Tuple[int, int, int],\n",
        "        transformer: nn.Module,\n",
        "        num_multimask_outputs: int = 3,\n",
        "        activation: Type[nn.Module] = nn.GELU,\n",
        "        iou_head_depth: int = 3,\n",
        "        iou_head_hidden_dim: int = 256,\n",
        "        num_layers: int = 3\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Predicts masks given an image and prompt embeddings, using a\n",
        "        transformer architecture.\n",
        "\n",
        "        Arguments:\n",
        "          transformer_dim (int): the channel dimension of the transformer\n",
        "          patch_size Tuple(int, int, int): the patch size used in the encoder\n",
        "          transformer (nn.Module): the transformer used to predict masks\n",
        "          num_multimask_outputs (int): the number of masks to predict\n",
        "            when disambiguating masks\n",
        "          activation (nn.Module): the type of activation to use when\n",
        "            upscaling masks\n",
        "          iou_head_depth (int): the depth of the MLP used to predict\n",
        "            mask quality\n",
        "          iou_head_hidden_dim (int): the hidden dimension of the MLP\n",
        "            used to predict mask quality\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.transformer = transformer\n",
        "        self.num_multimask_outputs = num_multimask_outputs\n",
        "\n",
        "        self.iou_token = nn.Embedding(1, transformer_dim[0])\n",
        "        self.num_mask_tokens = num_multimask_outputs + 1\n",
        "        self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim[0])\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.output_upscaling = get_output_scaling(kernel_stride_size, transformer_dim)\n",
        "        self.output_hypernetworks_mlps = get_MLP(transformer_dim[0], transformer_dim[-1], self.num_mask_tokens, num_layers)\n",
        "\n",
        "        self.iou_prediction_head = MLP(\n",
        "            transformer_dim[0], iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        image_embeddings: torch.Tensor,\n",
        "        image_pe: torch.Tensor,\n",
        "        sparse_prompt_embeddings: torch.Tensor,\n",
        "        dense_prompt_embeddings: torch.Tensor,\n",
        "        multimask_output: bool,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Predict masks given image and prompt embeddings.\n",
        "\n",
        "        Arguments:\n",
        "          image_embeddings (torch.Tensor): the embeddings from the image encoder\n",
        "          image_pe (torch.Tensor): positional encoding with the shape of image_embeddings\n",
        "          sparse_prompt_embeddings (torch.Tensor): the embeddings of the points and boxes\n",
        "          dense_prompt_embeddings (torch.Tensor): the embeddings of the mask inputs\n",
        "          multimask_output (bool): Whether to return multiple masks or a single\n",
        "            mask.\n",
        "\n",
        "        Returns:\n",
        "          torch.Tensor: batched predicted masks\n",
        "          torch.Tensor: batched predictions of mask quality\n",
        "        \"\"\"\n",
        "        masks, iou_pred = self.predict_masks(\n",
        "            image_embeddings=image_embeddings,\n",
        "            image_pe=image_pe,\n",
        "            sparse_prompt_embeddings=sparse_prompt_embeddings,\n",
        "            dense_prompt_embeddings=dense_prompt_embeddings,\n",
        "        )\n",
        "\n",
        "        # Select the correct mask or masks for output\n",
        "        if multimask_output:\n",
        "            mask_slice = slice(1, None)\n",
        "        else:\n",
        "            mask_slice = slice(0, 1)\n",
        "        masks = masks[:, mask_slice, :, :, :]\n",
        "        iou_pred = iou_pred[:, mask_slice]\n",
        "\n",
        "        # Prepare output\n",
        "        return masks, iou_pred\n",
        "\n",
        "    def predict_masks(\n",
        "        self,\n",
        "        image_embeddings: torch.Tensor,\n",
        "        image_pe: torch.Tensor,\n",
        "        sparse_prompt_embeddings: torch.Tensor,\n",
        "        dense_prompt_embeddings: torch.Tensor,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Predicts masks. See 'forward' for more details.\"\"\"\n",
        "        # Concatenate output tokens\n",
        "        output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n",
        "        output_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1)\n",
        "        tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)\n",
        "\n",
        "        # Expand per-image data in batch direction to be per-mask\n",
        "        src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n",
        "        src = src + dense_prompt_embeddings\n",
        "        pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n",
        "        b, c, h, w, d = src.shape\n",
        "\n",
        "        # Run the transformer\n",
        "        hs, src = self.transformer(src, pos_src, tokens)\n",
        "        iou_token_out = hs[:, 0, :]\n",
        "        mask_tokens_out = hs[:, 1 : (1 + self.num_mask_tokens), :]\n",
        "\n",
        "        # Upscale mask embeddings and predict masks using the mask tokens\n",
        "        src = src.transpose(1, 2).view(b, c, h, w, d)\n",
        "        upscaled_embedding = self.output_upscaling(src)\n",
        "        hyper_in_list: List[torch.Tensor] = []\n",
        "        #iterate\n",
        "        for i in range(self.num_mask_tokens):\n",
        "            hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n",
        "        hyper_in = torch.stack(hyper_in_list, dim=1)\n",
        "        #b, c, h, w = upscaled_embedding.shape this is likely if the upscaled embedding does not have the same size if altering upsampling\n",
        "        b, c, h, w, d = upscaled_embedding.shape\n",
        "        masks = (hyper_in @ upscaled_embedding.view(b, c, h * w * d)).view(b, -1, h, w, d)\n",
        "\n",
        "        # Generate mask quality predictions\n",
        "        iou_pred = self.iou_prediction_head(iou_token_out)\n",
        "\n",
        "        return masks, iou_pred\n",
        "\n",
        "\n",
        "# Lightly adapted from\n",
        "# https://github.com/facebookresearch/MaskFormer/blob/main/mask_former/modeling/transformer/transformer_predictor.py # noqa\n",
        "class MLP(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        output_dim: int,\n",
        "        num_layers: int,\n",
        "        sigmoid_output: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(\n",
        "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
        "        )\n",
        "        self.sigmoid_output = sigmoid_output\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        if self.sigmoid_output:\n",
        "            x = F.sigmoid(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oYq5S_adjZJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "\n",
        "class AnishSalviModel(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: Tuple[int, int, int] = (64, 64, 64),\n",
        "        patch_size: Tuple[int, int, int] = (8, 8, 8),\n",
        "        in_chans: int = 1,\n",
        "        image_embed_dim: int = 768, #has to be 768?\n",
        "        image_encoder_depth: int = 12,\n",
        "        image_encoder_num_heads: int = 12,\n",
        "        image_encoder_mlp_ratio: int = 4,\n",
        "        image_encoder_out_chans: int = 512, #same?\n",
        "        prompt_encoder_embed_dim: int = 512, #same?\n",
        "        prompt_encoder_mask_in_chans: List = [1, 3, 5],\n",
        "        prompt_encoder_kernel_stride_size: List = [(2, 2, 2), (4, 4, 4)],\n",
        "        prompt_encoder_num_attention_heads: int = 4,\n",
        "        prompt_encoder_num_hidden_layers: int = 12,\n",
        "        prompt_encoder_projection_dim: int = 64,\n",
        "        prompt_encoder_intermediate_size: int = 64,\n",
        "        prompt_encoder_max_position_embeddings: int = 10,\n",
        "        transformer_depth: int = 16,\n",
        "        transformer_embedding_dim: int = 512, #same?\n",
        "        transformer_num_heads: int = 16,\n",
        "        transformer_mlp_dim: int = 8,\n",
        "        mask_decoder_kernel_stride_size: List = [(4, 4, 4), (2, 2, 2)],\n",
        "        mask_decoder_transformer_dim: List = [512, 512, 512],\n",
        "        mask_decoder_num_multimask_outputs: int = 3,\n",
        "        mask_decoder_num_layers: int = 3,\n",
        "        tokenizer: Optional[AutoTokenizer] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        SAM predicts object masks from an image and input prompts.\n",
        "\n",
        "        Arguments:\n",
        "          image_encoder (ImageEncoderViT): The backbone used to encode the\n",
        "            image into image embeddings that allow for efficient mask prediction.\n",
        "          prompt_encoder (PromptEncoder): Encodes various types of input prompts.\n",
        "          transformer (TwoWayTransformer): Processes embeddings within mask_decoder\n",
        "          mask_decoder (MaskDecoder): Predicts masks from the image embeddings\n",
        "            and encoded prompts.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        #image encoder\n",
        "        self.image_encoder = ImageEncoderViT(img_size, patch_size, in_chans, image_embed_dim,\n",
        "                                image_encoder_depth, image_encoder_num_heads, image_encoder_mlp_ratio, image_encoder_out_chans)\n",
        "\n",
        "        #prompt encoder\n",
        "        self.prompt_encoder = PromptEncoder(prompt_encoder_embed_dim,\n",
        "                                            img_size, patch_size, prompt_encoder_mask_in_chans, prompt_encoder_kernel_stride_size,\n",
        "                                            prompt_encoder_num_attention_heads,\n",
        "                                            prompt_encoder_num_hidden_layers, prompt_encoder_projection_dim,\n",
        "                                            prompt_encoder_intermediate_size, prompt_encoder_max_position_embeddings, tokenizer)\n",
        "        #transformer\n",
        "        transformer = TwoWayTransformer(transformer_depth, transformer_embedding_dim,\n",
        "                                        transformer_num_heads, transformer_mlp_dim)\n",
        "        #mask decoder\n",
        "        self.mask_decoder = MaskDecoder(mask_decoder_kernel_stride_size, mask_decoder_transformer_dim, patch_size, transformer,\n",
        "                                        mask_decoder_num_multimask_outputs, mask_decoder_num_layers)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        batched_input: List[Dict[str, Any]],\n",
        "        multimask_output: bool,\n",
        "    ) -> List[Dict[str, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Predicts masks end-to-end from provided images and prompts.\n",
        "        If prompts are not known in advance, using ASMpredictor is\n",
        "        recommended over calling the model directly.\n",
        "\n",
        "        Arguments:\n",
        "          batched_input (list(dict)): A list over input images, each a\n",
        "            dictionary with the following keys. A prompt key can be\n",
        "            excluded if it is not present.\n",
        "              'image': The image as a torch tensor in CxHxWxD format,\n",
        "                already transformed for input to the model.\n",
        "              'point_coords': (torch.Tensor) Batched point prompts for\n",
        "                this image, with shape BxNx3. Already transformed to the\n",
        "                input frame of the model.\n",
        "              'point_labels': (torch.Tensor) Batched labels for point prompts,\n",
        "                with shape BxN.\n",
        "              'boxes': (torch.Tensor) Batched box inputs, with shape Bx6.\n",
        "                Already transformed to the input frame of the model.\n",
        "              'mask_inputs': (torch.Tensor) Batched mask inputs to the model,\n",
        "                in the form Bx1xHxWxD.\n",
        "          multimask_output (bool): Whether the model should predict multiple\n",
        "            disambiguating masks, or return a single mask.\n",
        "\n",
        "        Returns:\n",
        "          (list(dict)): A list over input images, where each element is\n",
        "            as dictionary with the following keys.\n",
        "              'iou_predictions': (torch.Tensor) The model's predictions\n",
        "                of mask quality, in shape BxC.\n",
        "              'low_res_logits': (torch.Tensor) Low resolution logits with\n",
        "                shape BxCxHxW. Can be passed as mask input\n",
        "                to subsequent iterations of prediction.\n",
        "        \"\"\"\n",
        "        input_images = torch.stack([x[\"image\"] for x in batched_input], dim=0)\n",
        "        image_embeddings = self.image_encoder(input_images)\n",
        "\n",
        "        outputs = []\n",
        "        for image_record, curr_embedding in zip(batched_input, image_embeddings):\n",
        "            #encode embeddings\n",
        "            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
        "                points=(image_record[\"point_coords\"], image_record[\"point_labels\"]),\n",
        "                boxes=image_record.get(\"boxes\", None),\n",
        "                masks=image_record.get(\"mask_inputs\", None),\n",
        "                text=image_record.get(\"text\", None)\n",
        "            )\n",
        "            #decode embeddings\n",
        "            low_res_masks, iou_predictions = self.mask_decoder(\n",
        "                image_embeddings=curr_embedding.unsqueeze(0),\n",
        "                image_pe=self.prompt_encoder.get_dense_pe(),\n",
        "                sparse_prompt_embeddings=sparse_embeddings,\n",
        "                dense_prompt_embeddings=dense_embeddings,\n",
        "                multimask_output=multimask_output,\n",
        "            )\n",
        "            outputs.append(\n",
        "                {\n",
        "                    \"iou_predictions\": iou_predictions,\n",
        "                    \"logits\": low_res_masks,\n",
        "                }\n",
        "            )\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "u3neajX0Ybx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#functions\n",
        "\n",
        "#collate fn\n",
        "def collate_fn(input_dicts):\n",
        "  ls_dict0 = []\n",
        "  ls_dict1 = []\n",
        "  for input_dict, mask_dict in input_dicts:\n",
        "    ls_dict0.append(input_dict)\n",
        "    ls_dict1.append(mask_dict)\n",
        "  return ls_dict0, ls_dict1\n",
        "\n",
        "#random sort rows batch\n",
        "def rand_batch_row(coords_points, num_points):\n",
        "  ls_coords = []\n",
        "  for coord_point in coords_points:\n",
        "    point_labels = coord_point[:, 3]\n",
        "    pos_labels = coord_point[point_labels == 1]\n",
        "    neg_labels = coord_point[point_labels == 0]\n",
        "    back_labels = coord_point[point_labels == -1]\n",
        "    #random rows (can't store all values)\n",
        "    pos_labels = rand_row(pos_labels, num_points)\n",
        "    neg_labels = rand_row(neg_labels, num_points)\n",
        "    back_labels = rand_row(back_labels, num_points)\n",
        "    #append\n",
        "    coords_labels = torch.concatenate([pos_labels, neg_labels, back_labels], dim = 0)\n",
        "    #sort\n",
        "    coords_labels = rand_row(coords_labels, coords_labels.shape[0])\n",
        "    ls_coords.append(torch.unsqueeze(coords_labels, dim = 0))\n",
        "  return torch.concatenate(ls_coords, dim = 0)\n",
        "\n",
        "#rand rows\n",
        "def rand_row(a, n = 10000):\n",
        "  return a[torch.randperm(a.size()[0])][0:n,:]\n",
        "\n",
        "#get bounding boxes\n",
        "def bounding_boxes(pos_labels):\n",
        "  max_val = torch.amax(pos_labels[:,0:3], dim = 0)\n",
        "  min_val = torch.amin(pos_labels[:,0:3], dim = 0)\n",
        "  return torch.concatenate([min_val, max_val], dim = 0).unsqueeze(0)\n",
        "\n",
        "#get the coordinates, labels, boxes\n",
        "def get_coords_points_boxes(label, num_points, num_labels = [1, 2]):\n",
        "  #get sparse labels\n",
        "  label = label + 1\n",
        "  sparse_label = label.to_sparse()\n",
        "  #get indices and vals\n",
        "  point_labels = sparse_label.values() - 1\n",
        "  coord_points = torch.concatenate([sparse_label.indices().T, point_labels.unsqueeze(1)], dim = 1)\n",
        "  #init\n",
        "  ls_coords = []\n",
        "  ls_boxes = []\n",
        "  #iterate\n",
        "  for num_label in num_labels:\n",
        "    #get pos labels\n",
        "    pos_labels = coord_points[point_labels == num_label]\n",
        "    #get neg labels\n",
        "    #neg_labels = coord_points[(point_labels != num_label) & (point_labels != 0)]\n",
        "    #get background labels\n",
        "    #back_labels = coord_points[point_labels == 0]\n",
        "    #get bounding boxes from pos labels\n",
        "    boxes = bounding_boxes(pos_labels)\n",
        "    #encode\n",
        "    pos_labels[:, 3] = 1\n",
        "    #neg_labels[:, 3] = 0\n",
        "    #back_labels[:, 3] = -1\n",
        "    #random rows (can't store all values)\n",
        "    pos_labels = rand_row(pos_labels, num_points)\n",
        "    #neg_labels = rand_row(neg_labels, num_points)\n",
        "    #back_labels = rand_row(back_labels, num_points)\n",
        "    #concat\n",
        "    coords_labels = pos_labels #torch.concatenate([pos_labels, neg_labels, back_labels], dim = 0)\n",
        "    #sort\n",
        "    #coords_labels = rand_row(coords_labels, coords_labels.shape[0])\n",
        "    #ls\n",
        "    ls_coords.append(torch.unsqueeze(coords_labels, dim = 0))\n",
        "    #ls\n",
        "    ls_boxes.append(boxes)\n",
        "  #concat\n",
        "  output_coords = torch.concatenate(ls_coords, dim = 0)\n",
        "  #concat\n",
        "  output_boxes = torch.concatenate(ls_boxes, dim = 0)\n",
        "  #return\n",
        "  return output_boxes, output_coords\n",
        "\n",
        "#class\n",
        "class CustomImageDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df, input_type = None, pos_points = 1):\n",
        "    self.df = df\n",
        "    #sub points\n",
        "    self.pos_points = pos_points\n",
        "    #classes\n",
        "    self.num_labels = [1, 2]\n",
        "    #input type\n",
        "    self.input_type = input_type\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "  def __getitem__(self, idx):\n",
        "    #row\n",
        "    row = self.df.iloc[idx]\n",
        "    #image height x width x depth\n",
        "    image = torch.Tensor(sitk.GetArrayFromImage(sitk.ReadImage(row['Norm_Image_64'])))\n",
        "    #label height x width x depth\n",
        "    label = torch.Tensor(sitk.GetArrayFromImage(sitk.ReadImage(row['Label_64'])))\n",
        "    #text\n",
        "    text = row['Input_Text']\n",
        "    #get augmentation stuff\n",
        "    boxes, coords_points = get_coords_points_boxes(label, self.pos_points, self.num_labels)\n",
        "    #subsample\n",
        "    coords_points = coords_points.to(torch.float32)\n",
        "    boxes = boxes.to(torch.float32)\n",
        "    #split num_mask x num_points x (num_coords + num_labels)\n",
        "    point_coords = coords_points[:, :, 0:3]\n",
        "    point_labels = coords_points[:, :, 3]\n",
        "    #image\n",
        "    image = image.unsqueeze(0).to(torch.float32)\n",
        "    #one hot remove background sigmoid multilabel\n",
        "    label = torch.nn.functional.one_hot(label.to(torch.int64))[:, :, :, 1:].to(torch.float32)\n",
        "    #move and add axis\n",
        "    label = torch.moveaxis(label, 3, 0).unsqueeze(1)\n",
        "\n",
        "    #control input type\n",
        "    #if self.input_type == 'point_coords':\n",
        "      #boxes = None\n",
        "      #text = None\n",
        "    #control input type\n",
        "    #if self.input_type == 'boxes':\n",
        "      #point_coords = None\n",
        "      #point_labels = None\n",
        "      #text = None\n",
        "    #control input type\n",
        "    #if self.input_type == 'text':\n",
        "      #point_coords = None\n",
        "      #point_labels = None\n",
        "      #boxes = None\n",
        "    #insert\n",
        "    dict0 = {\n",
        "        'image': image,\n",
        "        'point_coords': point_coords,\n",
        "        'point_labels': point_labels,\n",
        "        'boxes': boxes,\n",
        "        'mask_inputs': label,\n",
        "        'text': text\n",
        "        }\n",
        "    dict1 = {\n",
        "        'mask_inputs': label\n",
        "    }\n",
        "    #return dict0\n",
        "    return dict0, dict1\n",
        "\n",
        "#save json file\n",
        "def save_params(hyper_params, save_path):\n",
        "  json_string = json.dumps(hyper_params)\n",
        "  with open(save_path, 'w') as outfile:\n",
        "    outfile.write(json_string)\n",
        "\n",
        "#load json file\n",
        "def load_params(fpath):\n",
        "  with open(fpath) as json_file:\n",
        "    data = json.load(json_file)\n",
        "  return data\n",
        "\n",
        "#get the model\n",
        "def get_model(model_config):\n",
        "  #get model\n",
        "  if model_config['architecture']['description'] == 'AnishSalviModel':\n",
        "    model = AnishSalviModel(\n",
        "        img_size = model_config['architecture']['img_size'],\n",
        "        patch_size = model_config['architecture']['patch_size'],\n",
        "        in_chans = model_config['architecture']['in_chans'],\n",
        "        image_embed_dim = model_config['architecture']['image_embed_dim'],\n",
        "        image_encoder_depth = model_config['architecture']['image_encoder_depth'],\n",
        "        image_encoder_num_heads = model_config['architecture']['image_encoder_depth'],\n",
        "        image_encoder_mlp_ratio = model_config['architecture']['image_encoder_mlp_ratio'],\n",
        "        image_encoder_out_chans = model_config['architecture']['image_encoder_out_chans'],\n",
        "        prompt_encoder_embed_dim = model_config['architecture']['prompt_encoder_embed_dim'],\n",
        "        prompt_encoder_mask_in_chans = model_config['architecture']['prompt_encoder_mask_in_chans'],\n",
        "        prompt_encoder_kernel_stride_size = model_config['architecture']['prompt_encoder_kernel_stride_size'],\n",
        "        prompt_encoder_num_attention_heads = model_config['architecture']['prompt_encoder_num_attention_heads'],\n",
        "        prompt_encoder_num_hidden_layers = model_config['architecture']['prompt_encoder_num_hidden_layers'],\n",
        "        prompt_encoder_projection_dim = model_config['architecture']['prompt_encoder_projection_dim'],\n",
        "        prompt_encoder_intermediate_size = model_config['architecture']['prompt_encoder_intermediate_size'],\n",
        "        prompt_encoder_max_position_embeddings = model_config['architecture']['prompt_encoder_max_position_embeddings'],\n",
        "        transformer_depth = model_config['architecture']['transformer_depth'],\n",
        "        transformer_embedding_dim = model_config['architecture']['transformer_embedding_dim'], #same?\n",
        "        transformer_num_heads = model_config['architecture']['transformer_num_heads'],\n",
        "        transformer_mlp_dim = model_config['architecture']['transformer_mlp_dim'],\n",
        "        mask_decoder_kernel_stride_size = model_config['architecture']['mask_decoder_kernel_stride_size'],\n",
        "        mask_decoder_transformer_dim = model_config['architecture']['mask_decoder_transformer_dim'],\n",
        "        mask_decoder_num_multimask_outputs = model_config['architecture']['mask_decoder_num_multimask_outputs'],\n",
        "        mask_decoder_num_layers = model_config['architecture']['mask_decoder_num_layers'],\n",
        "        tokenizer = None)\n",
        "\n",
        "  #return\n",
        "  return model\n",
        "\n",
        "#load pretrained model\n",
        "def load_pretrained_model(save_folder, device):\n",
        "  #load\n",
        "  model_config = load_params(save_folder + 'model_config.json')\n",
        "  #weights\n",
        "  if model_config['save_weights_only']:\n",
        "    model = get_model(model_config)\n",
        "    #important to do prior\n",
        "    model.eval()\n",
        "    #load weights\n",
        "    model.load_state_dict(torch.load(model_config['save_folder'] + 'model_weights.pth', map_location = device))\n",
        "  else:\n",
        "    model = torch.load(model_config['save_folder'] + 'model.pth', map_location = device)\n",
        "  #send\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  #return\n",
        "  return model, model_config\n",
        "\n",
        "#send and clear gpu mem\n",
        "def clear_input(ls_dict, device):\n",
        "  for dict0 in ls_dict:\n",
        "    dict0['image'] = None\n",
        "    dict0['point_coords'] = None\n",
        "    dict0['point_labels'] = None\n",
        "    dict0['boxes'] = None\n",
        "    dict0['mask_inputs'] = None\n",
        "    #reset\n",
        "    if device == 'cuda':\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "#send and clear gpu mem\n",
        "def clear_output(ls_dict, device):\n",
        "  for dict0 in ls_dict:\n",
        "    dict0['iou_predictions'] = None\n",
        "    dict0['logits'] = None\n",
        "    #reset\n",
        "    if device == 'cuda':\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "#send to gpu mem\n",
        "def send_data(ls_dict, device):\n",
        "  ls_out = []\n",
        "  for dict0 in ls_dict:\n",
        "    dict0['image'] = dict0['image'].to(device)\n",
        "    if dict0['point_coords'] is not None:\n",
        "      dict0['point_coords'] = dict0['point_coords'].to(device)\n",
        "      dict0['point_labels'] = dict0['point_labels'].to(device)\n",
        "    if dict0['boxes'] is not None:\n",
        "      dict0['boxes'] = dict0['boxes'].to(device)\n",
        "    if dict0['mask_inputs'] is not None:\n",
        "      dict0['mask_inputs'] = dict0['mask_inputs'].to(device)\n",
        "    ls_out.append(dict0)\n",
        "  return ls_out\n",
        "\n",
        "#split\n",
        "def split_given_size(a, size):\n",
        "  return np.split(a, np.arange(size, len(a), size))"
      ],
      "metadata": {
        "id": "xrAS1WeoY7hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#init sweep path\n",
        "sweep_path = '/content/gdrive/MyDrive/SAMMI/results/3d-asm-sweep-2023-08-05-01-22-33/'\n",
        "#model tag\n",
        "model_tag = '3d-asm-'\n",
        "#batch size\n",
        "batch_size = 1\n",
        "#data type\n",
        "input_type = 'point_coords'"
      ],
      "metadata": {
        "id": "wm6XF19na1M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main\n",
        "\n",
        "%%time\n",
        "\n",
        "#init\n",
        "model = None\n",
        "#reset\n",
        "if device == 'cuda':\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "#get all the models in the sweep\n",
        "sweep_folders = sorted(glob.glob(sweep_path + model_tag + '*'))\n",
        "#iterate\n",
        "for sweep_folder in sweep_folders:\n",
        "  #load in the params\n",
        "  model, model_config = load_pretrained_model(sweep_folder + '/', device)\n",
        "  #load in the input file\n",
        "  df = pd.read_pickle(model_config['data_path'])\n",
        "  #need to obtain the data\n",
        "  ls_df = split_given_size(df, batch_size) #num patients in batch\n",
        "  #prep\n",
        "  model.to(device)\n",
        "  #eval\n",
        "  model.eval()\n",
        "  #init\n",
        "  ls_results = []\n",
        "  #load the data\n",
        "  dset_infer = CustomImageDataset(df, input_type = input_type)\n",
        "  #load\n",
        "  infer_loader = torch.utils.data.DataLoader(dset_infer, batch_size = batch_size, collate_fn = collate_fn) #can change the batch size!\n",
        "  #infer\n",
        "  for i, (batched_input, batched_label) in enumerate(infer_loader):\n",
        "    #get the small df\n",
        "    df_small = ls_df[i]\n",
        "    #less compute\n",
        "    with torch.no_grad():\n",
        "      #forward pass with autograd\n",
        "      batched_input = send_data(batched_input, device)\n",
        "      #with torch.amp.autocast(device, torch.float16):\n",
        "      batched_output = model(batched_input, False)\n",
        "      #compute dsc\n",
        "      dsc = compute_batch_dsc(batched_output, batched_label)\n",
        "      #set\n",
        "      df_small['DSC']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #reset\n",
        "      if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if i > 1:\n",
        "      break\n",
        "\n",
        "\n",
        "\n",
        "    #set\n",
        "    #df_small['Prediction'] = answers\n",
        "    #append\n",
        "    #ls_results.append(df_small)\n",
        "    #print\n",
        "    #print('Finished Batch: {0} of {1}'.format (i+1, len(ls_df)))\n",
        "  #save the results\n",
        "  #df_results = pd.concat(ls_results)\n",
        "  #save\n",
        "  #df_results.to_pickle(model_config['save_folder'] + 'df_results.pkl')\n",
        "  #clear\n",
        "  #model = None\n",
        "  #reset\n",
        "  if device == 'cuda':\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "696e238c49e34844aa6adb36c7da9518",
            "2557fca2c23e4a749fbe6d007a822825",
            "9582e042e3ed422181505403f3f3b408",
            "e08b02d46ace40e8ab40b1b2fa659eb6",
            "f5ea83bbb2f14c738fb9460f9d4db321",
            "055215e266be4094b8493b6a1500172f",
            "366cc45330364b5294a779716be6a20d",
            "21d3ae6a402e41bd8ace19def7f3c8a5",
            "c493ecdcefa447c3b022480df812749c",
            "28fc127e8863437f9feccd594f554231",
            "19183a7c11584fd48b9ef55bf57cb044",
            "b74bb8370c044b93afb417214f54e9b9",
            "76af570310d74f5286cdb4694d858137",
            "2c6250382a1647369e7cd39ea44fbc8b",
            "3f0f4319470b4f63a4da2f54efa20fdc",
            "2740e9bba0d0440ab25f0e3b33699cb4",
            "0dbf2449c9504b03b5bb4d90b433d590",
            "4b3843fc9dde45eeb532cc7539efa752",
            "62e3b13cd7ba4bc68c446f9175477418",
            "b60544624996488fabb5cd2c70d053b3",
            "9f770fc6d0b9442a970e8d0cdf34240c",
            "eb9c5f2bd2434ab3b5a40be23d7430ba",
            "14d482cdbfc04c3eaee956d1fec1c9a7",
            "92f2dac001dd4e4eb713e888d051bcde",
            "b743e53b37ce457783657b36758e80af",
            "14d267eb276844c38521a834268ab9a8",
            "99a28c90d0de44578c96b356b1ab15ec",
            "41eea8dc906e49709c72a1717070a298",
            "f403521ae6294311b1cb37712bd7f45c",
            "a6a6c3989f804694ba74d53ec8a4f8ed",
            "474ae20a0cdc4dacb804afba022fc861",
            "ff4193df83c9443db5c14ec68d4d818f",
            "9b1087b025c14c99a2df2ccd411230c8",
            "9a6f6dd55e1345989b246e7171905232",
            "3cdfa1e8bdc74cefb680983b242dc6cf",
            "2edf3e28458b4b209f89d505e56685ee",
            "d076b2db8edf4b058eb2a4e1701742c1",
            "47da194bad8b450fb354d344c8de2707",
            "eaa18e96c58949159ef578cc56dd8986",
            "11fb7f4e9ecf4a0fa5f7c457ebd67aa8",
            "b6239552e2634085aa422cb2439a3076",
            "033da1d0a33548baad55e3a93e088531",
            "454f9e2bd09e4037b16aa2787b4a6c5a",
            "8a6687f5222f4e29bac794b60ef5baa8",
            "722da8c9061b414ca4c93b771f77fb73",
            "1898659c8dfc4e6f89ebe82725b8225a",
            "36945105b1074d9a91586793bf6256e2",
            "cc24b3d3be274b5fbb2109d5256b6be1",
            "8ed5735ef36849d58f0892f35e7dc114",
            "0da0c498c70f4a16bc4d9d8cdacda03b",
            "cb939b3901bd4633acd9943da174152d",
            "f80c92bfc71248f0bc41e3ca8e0a99bb",
            "4b6950c9c0d74089b4d2565d87391985",
            "76e308c1da98441ea5f1d447d819079c",
            "72782a1c5db740d38b18ab5f2b1a47e8"
          ]
        },
        "id": "GR9wpcRPZuTP",
        "outputId": "14975a4f-fac4-404d-b67e-a50bbcf53d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading ()lve/main/config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "696e238c49e34844aa6adb36c7da9518"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading ()olve/main/vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b74bb8370c044b93afb417214f54e9b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading ()olve/main/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14d482cdbfc04c3eaee956d1fec1c9a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading ()cial_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a6f6dd55e1345989b246e7171905232"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading ()okenizer_config.json:   0%|          | 0.00/568 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "722da8c9061b414ca4c93b771f77fb73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 45s, sys: 46.1 s, total: 3min 31s\n",
            "Wall time: 6min 7s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  def __call__(self, batched_output, batched_input):\n",
        "    #init\n",
        "    loss_fn1 = 0\n",
        "    loss_fn3 = 0\n",
        "    #iterate y_pred, y_true\n",
        "    for i, (output, input) in enumerate(zip(batched_output, batched_input)):\n",
        "      #if sigmoid else softmax\n",
        "      y_pred = torch.swapaxes(output['logits'], 0, 1)\n",
        "      #y_true\n",
        "      y_true = torch.swapaxes(input['mask_inputs'], 0, 1)\n",
        "      #dice focal loss\n",
        "      loss_fn1 += self.loss_fn1(y_pred, y_true)\n",
        "      #proxy for iou loss\n",
        "      y_pred = torch.sigmoid(y_pred)\n",
        "      iou_y_true = self.fn2(torch.where(y_pred > self.thresh, 1, 0), y_true)\n",
        "      #apply\n",
        "      loss_fn3 += self.loss_fn3(torch.sigmoid(output['iou_predictions']), iou_y_true.T)\n",
        "    #divide\n",
        "    self.dicefocal_loss = loss_fn1.item() / len(batched_input)\n",
        "    self.mse_loss = loss_fn3.item() / len(batched_input)\n",
        "    return (loss_fn1 + self.lambda_iou * loss_fn3) / len(batched_input)"
      ],
      "metadata": {
        "id": "mLRXZeRyyxUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute the batch dsc\n",
        "def compute_batch_dsc(batched_output, batched_input, metrics = monai.metrics.DiceMetric()):\n",
        "  #init\n",
        "  batch_dsc = []\n",
        "  #iterate\n",
        "  for output, label in zip(batched_output, batched_label):\n",
        "    #y_pred and y_true\n",
        "    y_pred = output['logits'].swapaxes(0, 1).detach().cpu()\n",
        "    y_true = label['mask_inputs'].swapaxes(0, 1).detach().cpu()\n",
        "    #binarize logits\n",
        "    y_pred = torch.where(torch.sigmoid(y_pred) > 0.5, 1, 0)\n",
        "    #compute\n",
        "    dsc = metrics(y_pred, y_true).squeeze(0)\n",
        "    #init\n",
        "    ls_dsc = []\n",
        "    #iterate\n",
        "    for i in range(len(dsc)):\n",
        "      ls_dsc.append(dsc[i].item())\n",
        "    #append\n",
        "    batch_dsc.append(ls_dsc)\n",
        "  #return\n",
        "  return batch_dsc"
      ],
      "metadata": {
        "id": "XFKlzNa6Vbih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set and iterate over\n",
        "output = batched_output[0]\n",
        "label = batched_label[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mh-Jyl6HTDR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls_dsc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umlM9aF2UBB7",
        "outputId": "8090554b-1c43-4164-bc5b-bfd11b6928c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8251908421516418, 0.8219626545906067]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls_dsc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGsY6paeUQ-5",
        "outputId": "cb56de9a-2a69-429f-ce2a-07f37a27ab60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8251908421516418, 0.8219626545906067]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defaults\n",
        "conf_thresh = 0.5\n",
        "overlap_thresh = 0.5\n",
        "calc_iou = monai.metrics.MeanIoU()\n",
        "\n",
        "#set and iterate over\n",
        "output = batched_output[0]\n",
        "label = batched_label[0]\n",
        "\n",
        "#y_pred and y_true\n",
        "y_pred = output['logits'].squeeze(1)\n",
        "y_true = label['mask_inputs'].squeeze(1)\n",
        "\n",
        "#get iou and mask predictions\n",
        "iou_pred = torch.sigmoid(output['iou_predictions']).detach().cpu().numpy()\n",
        "mask_pred = torch.where(torch.sigmoid(y_pred) > 0.5, 1, 0).detach().cpu()\n",
        "\n",
        "#get the quality predictions above the confidence thresholds\n",
        "b = iou_pred > conf_thresh\n",
        "indices = b.nonzero()[0]\n",
        "thresh_pred = []\n",
        "for idx in indices:\n",
        "  thresh_pred.append([iou_pred[idx, 0], mask_pred[idx]])\n",
        "\n",
        "#rank by confidence\n",
        "thresh_pred.sort(reverse = True)\n"
      ],
      "metadata": {
        "id": "u3tv8aReEDNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S3rRzdRRgUI",
        "outputId": "a6064e43-38cf-4ae4-8497-671934031756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 64, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4pYEkO5Rl4c",
        "outputId": "60183282-4440-4043-ef9c-5043df6cc585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 64, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "y_pred = torch.swapaxes(output['logits'], 0, 1)\n",
        "y_true = torch.swapaxes(input['mask_inputs'], 0, 1)\n",
        "\n",
        "\n",
        "_, C, H, W, D = pred.shape\n",
        "b = iou_pred > conf_thresh\n",
        "indices = b.nonzero()[0]\n",
        "thresh_pred = []\n",
        "for idx in indices:\n",
        "  thresh_pred.append([iou_pred[idx, 0], pred[0, idx, ]])\n",
        "\n"
      ],
      "metadata": {
        "id": "jwemH3Jsy3lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just append each item and delte the other"
      ],
      "metadata": {
        "id": "yc7xPkLiJ7Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_mask\n"
      ],
      "metadata": {
        "id": "kibd5xf_J_RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nms_thresh = []\n",
        "thresh_pred2 = thresh_pred\n",
        "for patient in thresh_pred2:\n",
        "  if thresh_pred:\n",
        "    iou_conf, pred_mask = thresh_pred.pop(0)\n",
        "    for i, (iou_conf_other, other_mask) in enumerate(thresh_pred):\n",
        "      #iou\n",
        "      iou_overlap = iou(pred_mask, other_mask)\n",
        "      #quality\n",
        "      if iou_overlap > overlap_thresh:\n",
        "        thresh_pred.remove(i)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Od20UECjEH0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresh_pred2 = [0.5, 0.75, 0.6, 0.9, 0.8, 0.4]\n",
        "thresh_pred = thresh_pred2"
      ],
      "metadata": {
        "id": "b6BtxjTWHtbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for patient in thresh_pred2:\n",
        "      iou_conf, pred_mask = thresh_pred.pop(0)\n",
        "      for i, (iou_conf_other, other_mask) in enumerate(thresh_pred):\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6_EBJkRhHqAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls = [9]"
      ],
      "metadata": {
        "id": "nFtK0tOMzHVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if ls:\n",
        "  print('True')\n",
        "else:\n",
        "  print('False')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36pUpQXaFtKt",
        "outputId": "f7be5cff-2e43-48a4-a99f-bedb5c5928dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compute iou\n",
        "def"
      ],
      "metadata": {
        "id": "J9ivzTjd5X9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Kh_z38DvXwA",
        "outputId": "f1a299bd-fe32-4c02-d06e-404f334f12d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.67189384,\n",
              " tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0]],\n",
              " \n",
              "         [[0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0]],\n",
              " \n",
              "         [[0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0]],\n",
              " \n",
              "         [[0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0]],\n",
              " \n",
              "         [[0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0],\n",
              "          [0, 0, 0,  ..., 0, 0, 0]]])]"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thresh_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbu6koxy4ziT",
        "outputId": "2b96c955-4a90-4417-81c9-c909c95fbbd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.67189384,\n",
              "  tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           ...,\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0]],\n",
              "  \n",
              "          [[0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           ...,\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0]],\n",
              "  \n",
              "          [[0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           ...,\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           ...,\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0]],\n",
              "  \n",
              "          [[0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           ...,\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0]],\n",
              "  \n",
              "          [[0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           ...,\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0],\n",
              "           [0, 0, 0,  ..., 0, 0, 0]]])]]"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = y_true.tolist()"
      ],
      "metadata": {
        "id": "XSn6Ib6PyBRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls = y_true.squeeze(0).tolist()"
      ],
      "metadata": {
        "id": "ik0GrVA1xvSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "vFSAG8IKxyg0",
        "outputId": "ac86f586-e03f-43b1-d645-d5f5d78c19b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-188-7330cb095d81>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KfPK7a06x3VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCi94N5ZxCa8",
        "outputId": "ed1c6413-e64b-4f49-c16a-47507bb4cea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6838889 ],\n",
              "       [0.67189384]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcc4ksYYvhMf",
        "outputId": "9322ab40-42a8-48b3-94b7-77f761ecd3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for"
      ],
      "metadata": {
        "id": "kJWDQzWIoTA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JrhLQ5yIrpjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMqZaYy9utf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iou_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHdzBXM0vMMi",
        "outputId": "526b1b9d-0d7d-4dd8-d028-cbc9b3415c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.68894744],\n",
              "       [0.68394554]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XvDUBy_YvkD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWBOeNJov-K-",
        "outputId": "66bbcb33-4ce4-4901-b687-ec3c342656a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#functions\n"
      ],
      "metadata": {
        "id": "5AkornuLX9Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnHwcY39SPN2"
      },
      "outputs": [],
      "source": [
        "model, model_config = load_pretrained_model('/content/gdrive/MyDrive/SAMMI/results/3d-asm-sweep-2023-08-05-01-22-33/3d-asm-2023-08-05-01-22-37/', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY80yY-KSdeM"
      },
      "outputs": [],
      "source": [
        "dset = CustomImageDataset(df = df, data = 'TRAIN', aug = False, sub_points = 10)\n",
        "loader = torch.utils.data.DataLoader(dset, batch_size = 2, collate_fn = collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHvB1HvrTREH"
      },
      "outputs": [],
      "source": [
        "for i, x in enumerate(loader):\n",
        "  x\n",
        "  if i > 1:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qU-8Oz4Tn35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ff1d9d-3a2b-44dc-c41c-914bd94174e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "type(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxjB6VBXTYFh"
      },
      "outputs": [],
      "source": [
        "batched_input = send_data(x, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNxaMKeMUhMW"
      },
      "outputs": [],
      "source": [
        "batched_input = send_text(batched_input, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vbdg-c8UE7I"
      },
      "outputs": [],
      "source": [
        "#send to gpu mem\n",
        "def send_text(ls_dict, device):\n",
        "  ls_out = []\n",
        "  for dict0 in ls_dict:\n",
        "    dict0['image'] = dict0['image'].to(device)\n",
        "    dict0['point_coords'] = None\n",
        "    dict0['point_labels'] = None\n",
        "    dict0['boxes'] = None\n",
        "    dict0['mask_inputs'] = None\n",
        "    ls_out.append(dict0)\n",
        "  return ls_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqb-AsSWUnq1"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  outputs = model(batched_input, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijv57hqCVRFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6152bfe4-7645-4981-d50a-fe799d45931a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0455],\n",
              "        [0.0152]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "outputs[0]['iou_predictions']"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cVjVdcu8knwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = sitk.GetImageFromArray(np.where(torch.sigmoid(outputs[0]['logits'][0, 0]).detach().cpu().numpy() > 0.5, 1, 0))"
      ],
      "metadata": {
        "id": "gCqfLQ61kkwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fpu_nvijV36g"
      },
      "outputs": [],
      "source": [
        "image = sitk.Cast(image, sitk.sitkUInt32)\n",
        "\n",
        "sitk.WriteImage(image, '/content/jjj56.nii.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOvOgg1IXZbM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WThKdyEhVWzL"
      },
      "outputs": [],
      "source": [
        "y_pred = torch.swapaxes(outputs[0]['logits'], 0, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3teOHrZTYsbh"
      },
      "outputs": [],
      "source": [
        "image.GetSize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz8kuKsCYIZA"
      },
      "outputs": [],
      "source": [
        "y_pred.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3121081a20bd430c93446e381ef0d5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5eb900d7931a4436920606573d4738c9",
              "IPY_MODEL_19e6acb7f53447328c5ffebf07cc3a3c"
            ],
            "layout": "IPY_MODEL_245e32bc992f41118eed2230701f06e9"
          }
        },
        "5eb900d7931a4436920606573d4738c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c87ebe6d551e42e28fdb01e3a30e9b07",
            "placeholder": "",
            "style": "IPY_MODEL_c980cc79789442b8ae3524c5ba68c9a1",
            "value": "0.004 MB of 0.028 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "19e6acb7f53447328c5ffebf07cc3a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cae9966d28ff4f65b11488adb1487590",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1816a4e188c847a998717cb487b04026",
            "value": 0.15521680914615305
          }
        },
        "245e32bc992f41118eed2230701f06e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c87ebe6d551e42e28fdb01e3a30e9b07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c980cc79789442b8ae3524c5ba68c9a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cae9966d28ff4f65b11488adb1487590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1816a4e188c847a998717cb487b04026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "696e238c49e34844aa6adb36c7da9518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2557fca2c23e4a749fbe6d007a822825",
              "IPY_MODEL_9582e042e3ed422181505403f3f3b408",
              "IPY_MODEL_e08b02d46ace40e8ab40b1b2fa659eb6"
            ],
            "layout": "IPY_MODEL_f5ea83bbb2f14c738fb9460f9d4db321"
          }
        },
        "2557fca2c23e4a749fbe6d007a822825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_055215e266be4094b8493b6a1500172f",
            "placeholder": "",
            "style": "IPY_MODEL_366cc45330364b5294a779716be6a20d",
            "value": "Downloading ()lve/main/config.json: 100%"
          }
        },
        "9582e042e3ed422181505403f3f3b408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21d3ae6a402e41bd8ace19def7f3c8a5",
            "max": 4186,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c493ecdcefa447c3b022480df812749c",
            "value": 4186
          }
        },
        "e08b02d46ace40e8ab40b1b2fa659eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28fc127e8863437f9feccd594f554231",
            "placeholder": "",
            "style": "IPY_MODEL_19183a7c11584fd48b9ef55bf57cb044",
            "value": " 4.19k/4.19k [00:00&lt;00:00, 50.4kB/s]"
          }
        },
        "f5ea83bbb2f14c738fb9460f9d4db321": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "055215e266be4094b8493b6a1500172f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "366cc45330364b5294a779716be6a20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21d3ae6a402e41bd8ace19def7f3c8a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c493ecdcefa447c3b022480df812749c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28fc127e8863437f9feccd594f554231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19183a7c11584fd48b9ef55bf57cb044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b74bb8370c044b93afb417214f54e9b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76af570310d74f5286cdb4694d858137",
              "IPY_MODEL_2c6250382a1647369e7cd39ea44fbc8b",
              "IPY_MODEL_3f0f4319470b4f63a4da2f54efa20fdc"
            ],
            "layout": "IPY_MODEL_2740e9bba0d0440ab25f0e3b33699cb4"
          }
        },
        "76af570310d74f5286cdb4694d858137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dbf2449c9504b03b5bb4d90b433d590",
            "placeholder": "",
            "style": "IPY_MODEL_4b3843fc9dde45eeb532cc7539efa752",
            "value": "Downloading ()olve/main/vocab.json: 100%"
          }
        },
        "2c6250382a1647369e7cd39ea44fbc8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62e3b13cd7ba4bc68c446f9175477418",
            "max": 862328,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b60544624996488fabb5cd2c70d053b3",
            "value": 862328
          }
        },
        "3f0f4319470b4f63a4da2f54efa20fdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f770fc6d0b9442a970e8d0cdf34240c",
            "placeholder": "",
            "style": "IPY_MODEL_eb9c5f2bd2434ab3b5a40be23d7430ba",
            "value": " 862k/862k [00:00&lt;00:00, 2.83MB/s]"
          }
        },
        "2740e9bba0d0440ab25f0e3b33699cb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dbf2449c9504b03b5bb4d90b433d590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b3843fc9dde45eeb532cc7539efa752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62e3b13cd7ba4bc68c446f9175477418": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b60544624996488fabb5cd2c70d053b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f770fc6d0b9442a970e8d0cdf34240c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb9c5f2bd2434ab3b5a40be23d7430ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14d482cdbfc04c3eaee956d1fec1c9a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92f2dac001dd4e4eb713e888d051bcde",
              "IPY_MODEL_b743e53b37ce457783657b36758e80af",
              "IPY_MODEL_14d267eb276844c38521a834268ab9a8"
            ],
            "layout": "IPY_MODEL_99a28c90d0de44578c96b356b1ab15ec"
          }
        },
        "92f2dac001dd4e4eb713e888d051bcde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41eea8dc906e49709c72a1717070a298",
            "placeholder": "",
            "style": "IPY_MODEL_f403521ae6294311b1cb37712bd7f45c",
            "value": "Downloading ()olve/main/merges.txt: 100%"
          }
        },
        "b743e53b37ce457783657b36758e80af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6a6c3989f804694ba74d53ec8a4f8ed",
            "max": 524657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_474ae20a0cdc4dacb804afba022fc861",
            "value": 524657
          }
        },
        "14d267eb276844c38521a834268ab9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff4193df83c9443db5c14ec68d4d818f",
            "placeholder": "",
            "style": "IPY_MODEL_9b1087b025c14c99a2df2ccd411230c8",
            "value": " 525k/525k [00:00&lt;00:00, 6.16MB/s]"
          }
        },
        "99a28c90d0de44578c96b356b1ab15ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41eea8dc906e49709c72a1717070a298": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f403521ae6294311b1cb37712bd7f45c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6a6c3989f804694ba74d53ec8a4f8ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "474ae20a0cdc4dacb804afba022fc861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff4193df83c9443db5c14ec68d4d818f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b1087b025c14c99a2df2ccd411230c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a6f6dd55e1345989b246e7171905232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cdfa1e8bdc74cefb680983b242dc6cf",
              "IPY_MODEL_2edf3e28458b4b209f89d505e56685ee",
              "IPY_MODEL_d076b2db8edf4b058eb2a4e1701742c1"
            ],
            "layout": "IPY_MODEL_47da194bad8b450fb354d344c8de2707"
          }
        },
        "3cdfa1e8bdc74cefb680983b242dc6cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaa18e96c58949159ef578cc56dd8986",
            "placeholder": "",
            "style": "IPY_MODEL_11fb7f4e9ecf4a0fa5f7c457ebd67aa8",
            "value": "Downloading ()cial_tokens_map.json: 100%"
          }
        },
        "2edf3e28458b4b209f89d505e56685ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6239552e2634085aa422cb2439a3076",
            "max": 389,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_033da1d0a33548baad55e3a93e088531",
            "value": 389
          }
        },
        "d076b2db8edf4b058eb2a4e1701742c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_454f9e2bd09e4037b16aa2787b4a6c5a",
            "placeholder": "",
            "style": "IPY_MODEL_8a6687f5222f4e29bac794b60ef5baa8",
            "value": " 389/389 [00:00&lt;00:00, 3.84kB/s]"
          }
        },
        "47da194bad8b450fb354d344c8de2707": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaa18e96c58949159ef578cc56dd8986": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11fb7f4e9ecf4a0fa5f7c457ebd67aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6239552e2634085aa422cb2439a3076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "033da1d0a33548baad55e3a93e088531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "454f9e2bd09e4037b16aa2787b4a6c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a6687f5222f4e29bac794b60ef5baa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "722da8c9061b414ca4c93b771f77fb73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1898659c8dfc4e6f89ebe82725b8225a",
              "IPY_MODEL_36945105b1074d9a91586793bf6256e2",
              "IPY_MODEL_cc24b3d3be274b5fbb2109d5256b6be1"
            ],
            "layout": "IPY_MODEL_8ed5735ef36849d58f0892f35e7dc114"
          }
        },
        "1898659c8dfc4e6f89ebe82725b8225a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0da0c498c70f4a16bc4d9d8cdacda03b",
            "placeholder": "",
            "style": "IPY_MODEL_cb939b3901bd4633acd9943da174152d",
            "value": "Downloading ()okenizer_config.json: 100%"
          }
        },
        "36945105b1074d9a91586793bf6256e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f80c92bfc71248f0bc41e3ca8e0a99bb",
            "max": 568,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b6950c9c0d74089b4d2565d87391985",
            "value": 568
          }
        },
        "cc24b3d3be274b5fbb2109d5256b6be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76e308c1da98441ea5f1d447d819079c",
            "placeholder": "",
            "style": "IPY_MODEL_72782a1c5db740d38b18ab5f2b1a47e8",
            "value": " 568/568 [00:00&lt;00:00, 6.17kB/s]"
          }
        },
        "8ed5735ef36849d58f0892f35e7dc114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0da0c498c70f4a16bc4d9d8cdacda03b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb939b3901bd4633acd9943da174152d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f80c92bfc71248f0bc41e3ca8e0a99bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b6950c9c0d74089b4d2565d87391985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76e308c1da98441ea5f1d447d819079c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72782a1c5db740d38b18ab5f2b1a47e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}