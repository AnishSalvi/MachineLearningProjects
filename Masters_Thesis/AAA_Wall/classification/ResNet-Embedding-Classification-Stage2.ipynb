{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFuSoHxk9TIa","executionInfo":{"status":"ok","timestamp":1662912231752,"user_tz":420,"elapsed":14970,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}},"outputId":"355e536a-298e-47fb-cf20-511fdcbdb9fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["#mount\n","from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)"]},{"cell_type":"code","source":["#need to generate \n","#during inference\n","#CTA --> Model 1 --> ROI CTA --> Model 2 --> Segmentation Prediction\n","#if we want the embeddings of model 2\n","#training set Cropped-CTA-64 embeddings(provided to the model during training)\n","#validation set localized training set predictions (helps with inference later)\n","#testing set locaalized testing set predictions (helps with inference later)\n","\n","#but first need to pass the quantities through model 1!\n","#note model 2 is not trained on the predictions of model 1"],"metadata":{"id":"wgvN3Twi9gmT","executionInfo":{"status":"ok","timestamp":1662912231753,"user_tz":420,"elapsed":11,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#generate the embeeddings by passing the 3 different quantities through model 1"],"metadata":{"id":"69HM73vP-2qx","executionInfo":{"status":"ok","timestamp":1662912231754,"user_tz":420,"elapsed":8,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#pip installations\n","!pip install --quiet SimpleITK\n","!pip install --quiet torch == 1.9\n","!pip install --quiet ctviewer\n","\n","#copy from github\n","!git clone https://github.com/kilgore92/PyTorch-UNet.git\n","%cd PyTorch-UNet\n","%cd src\n","from unet.model import UNet\n","%cd ..\n","%cd .."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"it_lSby7_l7q","executionInfo":{"status":"ok","timestamp":1662912249103,"user_tz":420,"elapsed":17356,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}},"outputId":"46abda88-d6b0-4a0f-eb49-6005e4470ac2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 52.8 MB 240 kB/s \n","\u001b[?25h\u001b[31mERROR: Invalid requirement: '=='\u001b[0m\n","  Building wheel for ctviewer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Cloning into 'PyTorch-UNet'...\n","remote: Enumerating objects: 213, done.\u001b[K\n","remote: Counting objects: 100% (48/48), done.\u001b[K\n","remote: Compressing objects: 100% (27/27), done.\u001b[K\n","remote: Total 213 (delta 21), reused 37 (delta 12), pack-reused 165\u001b[K\n","Receiving objects: 100% (213/213), 43.46 KiB | 10.86 MiB/s, done.\n","Resolving deltas: 100% (94/94), done.\n","/content/PyTorch-UNet\n","/content/PyTorch-UNet/src\n","/content/PyTorch-UNet\n","/content\n"]}]},{"cell_type":"code","source":["#imports\n","import SimpleITK as sitk\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","import matplotlib\n","import seaborn as sns\n","import pandas as pd\n","pd.options.mode.chained_assignment = None\n","import torch\n","import os\n","import sklearn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import json\n","import datetime\n","from datetime import datetime\n","from sklearn.metrics import confusion_matrix\n","import torchvision\n","import random\n","from ctviewer import CTViewer\n","\n","#set the Dice Loss for model learning\n","from keras import backend as K\n","from tensorflow.compat.v1 import enable_eager_execution\n","import tensorflow as tf\n","import numpy as np\n","import os\n","\n","#Dice Loss used for criterion\n","import torch\n","import torch.nn as nn\n","import torch.functional as f\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from math import pow\n","\n","#device\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqc1wxNa_0uD","executionInfo":{"status":"ok","timestamp":1662912254242,"user_tz":420,"elapsed":5144,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}},"outputId":"b7f3f112-71c0-49a2-8236-c71c7d67fbc0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["#load a pretrained model\n","def load_pretrained_model(save_folder, device):\n","  train_params = load_params(save_folder + 'train_params.json')\n","  model = torch.load(train_params['save_path'] + 'model.pth', map_location=torch.device(device))\n","  model.to(device)\n","  return train_params, model\n","\n","#load the train params back in\n","def load_params(fpath):\n","  # Opening JSON file\n","  with open(fpath) as json_file:\n","    data = json.load(json_file)\n","  return data\n","#image resampling\n","def resample_image_standardize(itk_image, out_size = (64,64,64), is_label = False):\n","  original_spacing = itk_image.GetSpacing()\n","  original_size = itk_image.GetSize()\n","  out_spacing = [original_size[0] * (original_spacing[0] / out_size[0]),\n","                 original_size[1] * (original_spacing[1] / out_size[1]),\n","                 original_size[2] * (original_spacing[2] / out_size[2])]\n","\n","  resample = sitk.ResampleImageFilter()\n","  resample.SetOutputSpacing(out_spacing)\n","  resample.SetOutputOrigin(itk_image.GetOrigin())\n","  resample.SetSize(out_size)\n","  resample.SetOutputDirection(itk_image.GetDirection())\n","  resample.SetTransform(sitk.Transform())\n","  #resample.SetDefaultPixelValue(itk_image.GetPixelIDValue())\n","  if is_label:\n","      resample.SetInterpolator(sitk.sitkNearestNeighbor)\n","  else:\n","      resample.SetInterpolator(sitk.sitkBSpline)\n","  return resample.Execute(itk_image)\n","#save image\n","def save_image(save_path, save_folder, patient, image):\n","  #save file\n","  if os.path.isdir(save_path + save_folder) == False:\n","    os.mkdir(save_path + save_folder)\n","  #save file\n","  save_loc = save_path + save_folder + '/' + patient + '.nii.gz'\n","  #write\n","  sitk.WriteImage(image, save_loc)\n","  #return\n","  return save_loc\n","\n","#localized ROI CTA inference\n","def localized_CTA_inference(row, train_params, model, device):\n","  resampler = sitk.ResampleImageFilter()\n","  otsu = sitk.OtsuThresholdImageFilter()\n","  z = sitk.NormalizeImageFilter()\n","  #read in the CTA-64 model\n","  orig_image = sitk.ReadImage(row['CTA'])\n","  image = sitk.ReadImage(row['CTA-64'])\n","  #already normalized\n","  x = torch.Tensor(np.expand_dims(sitk.GetArrayFromImage(image), axis = (0, 1))).to(device)\n","  #pass\n","  model.eval()\n","  with torch.no_grad():\n","    y_pred = model(x)\n","  #convert back\n","  prediction = sitk.GetImageFromArray(np.squeeze(torch.sigmoid(y_pred).cpu().detach().numpy()))\n","  #reset to the og\n","  prediction.CopyInformation(image)\n","  #upsample to the original\n","  resampler.SetReferenceImage(orig_image)\n","  resampler.SetTransform(sitk.Transform())\n","  resampler.SetInterpolator(sitk.sitkNearestNeighbor)\n","  prediction = resampler.Execute(prediction)\n","  prediction.CopyInformation(orig_image)\n","  #otsu threshold\n","  otsu.SetOutsideValue(1)\n","  otsu.SetInsideValue(0)\n","  prediction = otsu.Execute(prediction)\n","  #apply #the bounding box to the CTA\n","  #filter\n","  label_shape_filter = sitk.LabelShapeStatisticsImageFilter()\n","  #apply\n","  label_shape_filter.Execute(prediction)\n","  #get bbox\n","  bbox = label_shape_filter.GetBoundingBox(1) #in pixel coordinates\n","  #get ROI\n","  ROI_image = sitk.RegionOfInterest(orig_image, bbox[int(len(bbox)/2):], bbox[0:int(len(bbox)/2)])\n","  #resample the CTA\n","  ROI_image_64 = resample_image_standardize(ROI_image, out_size = (64, 64, 64), is_label = False)\n","  #z-norm the CTA\n","  ROI_image_64 = z.Execute(ROI_image_64)\n","  #return\n","  return save_image(train_params['classification_save_path'], 'localized-CTAs', row['Patient'], ROI_image_64)\n","\n","#save json file\n","def save_params(hyper_params, save_path):\n","  json_string = json.dumps(hyper_params)\n","  with open(save_path, 'w') as outfile:\n","    outfile.write(json_string)"],"metadata":{"id":"9L2KKFNOACwj","executionInfo":{"status":"ok","timestamp":1662912254243,"user_tz":420,"elapsed":24,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Class definitions for a standard U-Net Up-and Down-sampling blocks\n","http://arxiv.org/abs/1505.0.397\n","\n","\"\"\"\n","\n","\n","class EncoderBlock(nn.Module):\n","    \"\"\"\n","    Instances the Encoder block that forms a part of a U-Net\n","    Parameters:\n","        in_channels (int): Depth (or number of channels) of the tensor that the block acts on\n","        filter_num (int) : Number of filters used in the convolution ops inside the block,\n","                             depth of the output of the enc block\n","        dropout(bool) : Flag to decide whether a dropout layer should be applied\n","        dropout_rate (float) : Probability of dropping a convolution output feature channel\n","\n","    \"\"\"\n","    def __init__(self, filter_num=64, in_channels=1, dropout=False, dropout_rate=0.3):\n","\n","        super(EncoderBlock,self).__init__()\n","        self.filter_num = int(filter_num)\n","        self.in_channels = int(in_channels)\n","        self.dropout = dropout\n","        self.dropout_rate = dropout_rate\n","\n","        self.conv1 = nn.Conv2d(in_channels=self.in_channels,\n","                               out_channels=self.filter_num,\n","                               kernel_size=3,\n","                               padding=1)\n","\n","        self.conv2 = nn.Conv2d(in_channels=self.filter_num,\n","                               out_channels=self.filter_num,\n","                               kernel_size=3,\n","                               padding=1)\n","\n","        self.bn_op_1 = nn.InstanceNorm2d(num_features=self.filter_num)\n","        self.bn_op_2 = nn.InstanceNorm2d(num_features=self.filter_num)\n","\n","    def forward(self, x):\n","\n","        x = self.conv1(x)\n","        x = self.bn_op_1(x)\n","        x = F.leaky_relu(x)\n","        if self.dropout is True:\n","            x = F.dropout2d(x, p=self.dropout_rate)\n","\n","        x = self.conv2(x)\n","        x = self.bn_op_2(x)\n","        x = F.leaky_relu(x)\n","        if self.dropout is True:\n","            x = F.dropout2d(x, p=self.dropout_rate)\n","        return x\n","\n","\n","class DecoderBlock(nn.Module):\n","    \"\"\"\n","    Decoder block used in the U-Net\n","\n","    Parameters:\n","        in_channels (int) : Number of channels of the incoming tensor for the upsampling op\n","        concat_layer_depth (int) : Number of channels to be concatenated via skip connections\n","        filter_num (int) : Number of filters used in convolution, the depth of the output of the dec block\n","        interpolate (bool) : Decides if upsampling needs to performed via interpolation or transposed convolution\n","        dropout(bool) : Flag to decide whether a dropout layer should be applied\n","        dropout_rate (float) : Probability of dropping a convolution output feature channel\n","\n","    \"\"\"\n","    def __init__(self, in_channels, concat_layer_depth, filter_num, interpolate=False, dropout=False, dropout_rate=0.3):\n","\n","        # Up-sampling (interpolation or transposed conv) --> EncoderBlock\n","        super(DecoderBlock, self).__init__()\n","        self.filter_num = int(filter_num)\n","        self.in_channels = int(in_channels)\n","        self.concat_layer_depth = int(concat_layer_depth)\n","        self.interpolate = interpolate\n","        self.dropout = dropout\n","        self.dropout_rate = dropout_rate\n","\n","        # Upsample by interpolation followed by a 3x3 convolution to obtain desired depth\n","        self.up_sample_interpolate = nn.Sequential(nn.Upsample(scale_factor=2,\n","                                                               mode='bilinear',\n","                                                               align_corners=True),\n","\n","                                                   nn.Conv2d(in_channels=self.in_channels,\n","                                                             out_channels=self.in_channels,\n","                                                             kernel_size=3,\n","                                                             padding=1)\n","                                                  )\n","\n","        # Upsample via transposed convolution (know to produce artifacts)\n","        self.up_sample_tranposed = nn.ConvTranspose2d(in_channels=self.in_channels,\n","                                                      out_channels=self.in_channels,\n","                                                      kernel_size=3,\n","                                                      stride=2,\n","                                                      padding=1,\n","                                                      output_padding=1)\n","\n","        self.down_sample = EncoderBlock(in_channels=self.in_channels+self.concat_layer_depth,\n","                                        filter_num=self.filter_num,\n","                                        dropout=self.dropout,\n","                                        dropout_rate=self.dropout_rate)\n","\n","    def forward(self, x, skip_layer):\n","        if self.interpolate is True:\n","            up_sample_out = F.leaky_relu(self.up_sample_interpolate(x))\n","        else:\n","            up_sample_out = F.leaky_relu(self.up_sample_tranposed(x))\n","\n","        merged_out = torch.cat([up_sample_out, skip_layer], dim=1)\n","        out = self.down_sample(merged_out)\n","        return out\n","\n","\n","class EncoderBlock3D(nn.Module):\n","\n","    \"\"\"\n","    Instances the 3D Encoder block that forms a part of a 3D U-Net\n","    Parameters:\n","        in_channels (int): Depth (or number of channels) of the tensor that the block acts on\n","        filter_num (int) : Number of filters used in the convolution ops inside the block,\n","                             depth of the output of the enc block\n","\n","    \"\"\"\n","    def __init__(self, filter_num=64, in_channels=1, dropout=False):\n","\n","        super(EncoderBlock3D, self).__init__()\n","        self.filter_num = int(filter_num)\n","        self.in_channels = int(in_channels)\n","        self.dropout = dropout\n","\n","        self.conv1 = nn.Conv3d(in_channels=self.in_channels,\n","                               out_channels=self.filter_num,\n","                               kernel_size=3,\n","                               padding=1)\n","\n","        self.conv2 = nn.Conv3d(in_channels=self.filter_num,\n","                               out_channels=self.filter_num*2,\n","                               kernel_size=3,\n","                               padding=1)\n","\n","        self.bn_op_1 = nn.InstanceNorm3d(num_features=self.filter_num)\n","        self.bn_op_2 = nn.InstanceNorm3d(num_features=self.filter_num*2)\n","\n","    def forward(self, x):\n","\n","        x = self.conv1(x)\n","        x = self.bn_op_1(x)\n","        x = F.leaky_relu(x)\n","        if self.dropout is True:\n","            x = F.dropout3d(x, p=0.3)\n","\n","        x = self.conv2(x)\n","        x = self.bn_op_2(x)\n","        x = F.leaky_relu(x)\n","\n","        if self.dropout is True:\n","            x = F.dropout3d(x, p=0.3)\n","\n","        return x\n","\n","\n","class DecoderBlock3D(nn.Module):\n","    \"\"\"\n","    Decoder block used in the 3D U-Net\n","\n","    Parameters:\n","        in_channels (int) : Number of channels of the incoming tensor for the upsampling op\n","        concat_layer_depth (int) : Number of channels to be concatenated via skip connections\n","        filter_num (int) : Number of filters used in convolution, the depth of the output of the dec block\n","        interpolate (bool) : Decides if upsampling needs to performed via interpolation or transposed convolution\n","\n","    \"\"\"\n","    def __init__(self, in_channels, concat_layer_depth, filter_num, interpolate=False, dropout=False):\n","\n","        super(DecoderBlock3D, self).__init__()\n","        self.filter_num = int(filter_num)\n","        self.in_channels = int(in_channels)\n","        self.concat_layer_depth = int(concat_layer_depth)\n","        self.interpolate = interpolate\n","        self.dropout = dropout\n","\n","        # Upsample by interpolation followed by a 3x3x3 convolution to obtain desired depth\n","        self.up_sample_interpolate = nn.Sequential(nn.Upsample(scale_factor=2,\n","                                                               mode='nearest'),\n","\n","                                                  nn.Conv3d(in_channels=self.in_channels,\n","                                                            out_channels=self.in_channels,\n","                                                            kernel_size=3,\n","                                                            padding=1)\n","                                                 )\n","\n","        # Upsample via transposed convolution (know to produce artifacts)\n","        self.up_sample_transposed = nn.ConvTranspose3d(in_channels=self.in_channels,\n","                                                       out_channels=self.in_channels,\n","                                                       kernel_size=3,\n","                                                       stride=2,\n","                                                       padding=1,\n","                                                       output_padding=1)\n","\n","        if self.dropout is True:\n","            self.down_sample = nn.Sequential(nn.Conv3d(in_channels=self.in_channels+self.concat_layer_depth,\n","                                                       out_channels=self.filter_num,\n","                                                       kernel_size=3,\n","                                                       padding=1),\n","\n","                                            nn.InstanceNorm3d(num_features=self.filter_num),\n","\n","                                            nn.LeakyReLU(),\n","\n","                                            nn.Dropout3d(p=0.3),\n","\n","                                            nn.Conv3d(in_channels=self.filter_num,\n","                                                      out_channels=self.filter_num,\n","                                                      kernel_size=3,\n","                                                      padding=1),\n","\n","                                            nn.InstanceNorm3d(num_features=self.filter_num),\n","\n","                                            nn.LeakyReLU(),\n","\n","                                            nn.Dropout3d(p=0.3))\n","        else:\n","            self.down_sample = nn.Sequential(nn.Conv3d(in_channels=self.in_channels+self.concat_layer_depth,\n","                                                       out_channels=self.filter_num,\n","                                                       kernel_size=3,\n","                                                       padding=1),\n","\n","                                            nn.InstanceNorm3d(num_features=self.filter_num),\n","\n","                                            nn.LeakyReLU(),\n","\n","                                            nn.Conv3d(in_channels=self.filter_num,\n","                                                      out_channels=self.filter_num,\n","                                                      kernel_size=3,\n","                                                      padding=1),\n","\n","                                            nn.InstanceNorm3d(num_features=self.filter_num),\n","\n","                                            nn.LeakyReLU())\n","\n","    def forward(self, x, skip_layer):\n","\n","        if self.interpolate is True:\n","            up_sample_out = F.leaky_relu(self.up_sample_interpolate(x))\n","        else:\n","            up_sample_out = F.leaky_relu(self.up_sample_transposed(x))\n","\n","        merged_out = torch.cat([up_sample_out, skip_layer], dim=1)\n","        out = self.down_sample(merged_out)\n","        return out\n","\n","\"\"\"\n","A PyTorch Implementation of a U-Net.\n","\n","Supports 2D (https://arxiv.org/abs/1505.04597) and 3D(https://arxiv.org/abs/1606.06650) variants\n","\n","Author: Ishaan Bhat\n","Email: ishaan@isi.uu.nl\n","\n","\"\"\"\n","\n","\n","\n","class UNet_Bottleneck(nn.Module):\n","    \"\"\"\n","     PyTorch class definition for the U-Net architecture for image segmentation\n","\n","     Parameters:\n","         n_channels (int) : Number of image channels\n","         base_filter_num (int) : Number of filters for the first convolution (doubled for every subsequent block)\n","         num_blocks (int) : Number of encoder/decoder blocks\n","         num_classes(int) : Number of classes that need to be segmented\n","         mode (str): 2D or 3D\n","         use_pooling (bool): Set to 'True' to use MaxPool as downnsampling op.\n","                             If 'False', strided convolution would be used to downsample feature maps (http://arxiv.org/abs/1908.02182)\n","         dropout (bool) : Whether dropout should be added to central encoder and decoder blocks (eg: BayesianSegNet)\n","         dropout_rate (float) : Dropout probability\n","     Returns:\n","         out (torch.Tensor) : Prediction of the segmentation map\n","\n","     \"\"\"\n","    def __init__(self, n_channels=1, base_filter_num=64, num_blocks=4, num_classes=5, mode='2D', dropout=False, dropout_rate=0.3, use_pooling=True):\n","\n","        super(UNet_Bottleneck, self).__init__()\n","        self.contracting_path = nn.ModuleList()\n","        self.expanding_path = nn.ModuleList()\n","        self.downsampling_ops = nn.ModuleList()\n","\n","        self.num_blocks = num_blocks\n","        self.n_channels = int(n_channels)\n","        self.n_classes = int(num_classes)\n","        self.base_filter_num = int(base_filter_num)\n","        self.enc_layer_depths = []  # Keep track of the output depths of each encoder block\n","        self.mode = mode\n","        self.pooling = use_pooling\n","        self.dropout = dropout\n","        self.dropout_rate = dropout_rate\n","\n","        if mode == '2D':\n","            self.encoder = EncoderBlock\n","            self.decoder = DecoderBlock\n","            self.pool = nn.MaxPool2d\n","\n","        elif mode == '3D':\n","            self.encoder = EncoderBlock3D\n","            self.decoder = DecoderBlock3D\n","            self.pool = nn.MaxPool3d\n","        else:\n","            print('{} mode is invalid'.format(mode))\n","\n","        for block_id in range(num_blocks):\n","            enc_block_filter_num = int(pow(2, block_id)*self.base_filter_num)  # Output depth of current encoder stage of the 2-D variant\n","            if block_id == 0:\n","                enc_in_channels = self.n_channels\n","            else:\n","                if self.mode == '2D':\n","                    enc_in_channels = enc_block_filter_num//2\n","                else:\n","                    enc_in_channels = enc_block_filter_num  # In the 3D UNet arch, the encoder features double in the 2nd convolution op\n","\n","\n","            # Dropout only applied to central encoder blocks -- See BayesianSegNet by Kendall et al.\n","            if self.dropout is True and block_id >= num_blocks/2:\n","                self.contracting_path.append(self.encoder(in_channels=enc_in_channels,\n","                                                          filter_num=enc_block_filter_num,\n","                                                          dropout=True,\n","                                                          dropout_rate=self.dropout_rate))\n","            else:\n","                self.contracting_path.append(self.encoder(in_channels=enc_in_channels,\n","                                                          filter_num=enc_block_filter_num,\n","                                                          dropout=False))\n","            if self.mode == '2D':\n","                self.enc_layer_depths.append(enc_block_filter_num)\n","                if self.pooling is False:\n","                    self.downsampling_ops.append(nn.Sequential(nn.Conv2d(in_channels=self.enc_layer_depths[-1],\n","                                                                         out_channels=self.enc_layer_depths[-1],\n","                                                                         kernel_size=3,\n","                                                                         stride=2,\n","                                                                         padding=1),\n","                                                                nn.InstanceNorm2d(num_features=self.filter_num),\n","                                                                nn.LeakyReLU()))\n","            else:\n","                self.enc_layer_depths.append(enc_block_filter_num*2) # Specific to 3D U-Net architecture (due to doubling of #feature_maps inside the 3-D Encoder)\n","                if self.pooling is False:\n","                    self.downsampling_ops.append(nn.Sequential(nn.Conv3d(in_channels=self.enc_layer_depths[-1],\n","                                                                         out_channels=self.enc_layer_depths[-1],\n","                                                                         kernel_size=3,\n","                                                                         stride=2,\n","                                                                         padding=1),\n","                                                                nn.InstanceNorm3d(num_features=self.enc_layer_depths[-1]),\n","                                                                nn.LeakyReLU()))\n","\n","        # Bottleneck layer\n","        if self.mode == '2D':\n","            bottle_neck_filter_num = self.enc_layer_depths[-1]*2\n","            bottle_neck_in_channels = self.enc_layer_depths[-1]\n","            self.bottle_neck_layer = self.encoder(filter_num=bottle_neck_filter_num,\n","                                                  in_channels=bottle_neck_in_channels)\n","\n","        else:  # Modified for the 3D UNet architecture\n","            bottle_neck_in_channels = self.enc_layer_depths[-1]\n","            bottle_neck_filter_num = self.enc_layer_depths[-1]*2\n","            self.bottle_neck_layer =  nn.Sequential(nn.Conv3d(in_channels=bottle_neck_in_channels,\n","                                                              out_channels=bottle_neck_in_channels,\n","                                                              kernel_size=3,\n","                                                              padding=1),\n","\n","                                                    nn.InstanceNorm3d(num_features=bottle_neck_in_channels),\n","\n","                                                    nn.LeakyReLU(),\n","\n","                                                    nn.Conv3d(in_channels=bottle_neck_in_channels,\n","                                                              out_channels=bottle_neck_filter_num,\n","                                                              kernel_size=3,\n","                                                              padding=1),\n","\n","                                                    nn.InstanceNorm3d(num_features=bottle_neck_filter_num),\n","\n","                                                    nn.LeakyReLU())\n","\n","        # Decoder Path\n","        for block_id in range(num_blocks):\n","            dec_in_channels = int(bottle_neck_filter_num//pow(2, block_id))\n","            if self.dropout is True and block_id <= num_blocks/2:\n","                self.expanding_path.append(self.decoder(in_channels=dec_in_channels,\n","                                                        filter_num=self.enc_layer_depths[-1-block_id],\n","                                                        concat_layer_depth=self.enc_layer_depths[-1-block_id],\n","                                                        interpolate=False,\n","                                                        dropout=True,\n","                                                        dropout_rate=self.dropout_rate))\n","            else:\n","                self.expanding_path.append(self.decoder(in_channels=dec_in_channels,\n","                                                        filter_num=self.enc_layer_depths[-1-block_id],\n","                                                        concat_layer_depth=self.enc_layer_depths[-1-block_id],\n","                                                        interpolate=False,\n","                                                        dropout=False))\n","\n","        # Output Layer\n","        if mode == '2D':\n","            self.output = nn.Conv2d(in_channels=int(self.enc_layer_depths[0]),\n","                                    out_channels=self.n_classes,\n","                                    kernel_size=1)\n","        else:\n","            self.output = nn.Conv3d(in_channels=int(self.enc_layer_depths[0]),\n","                                    out_channels=self.n_classes,\n","                                    kernel_size=1)\n","\n","    def forward(self, x):\n","\n","        if self.mode == '2D':\n","            h, w = x.shape[-2:]\n","\n","        else:\n","            d, h, w = x.shape[-3:]\n","\n","        # Encoder\n","        enc_outputs = []\n","        for stage, enc_op in enumerate(self.contracting_path):\n","            x = enc_op(x)\n","            enc_outputs.append(x)\n","\n","            if self.pooling is True:\n","                x = self.pool(kernel_size=2)(x)\n","            else:\n","                x = self.downsampling_ops[stage](x)\n","\n","        # Bottle-neck layer\n","        x = self.bottle_neck_layer(x)\n","        #torch.save(x, file_save) \n","        #extract bottleneck layer\n","\n","\n","        # Decoder #this section was commented out (as per the classification notebook)\n","        #for block_id, dec_op in enumerate(self.expanding_path):\n","        #    x = dec_op(x, enc_outputs[-1-block_id])\n","\n","        # Output\n","        #x = self.output(x)\n","\n","        return x\n","\n","#generate the new embeddings\n","def generate_embeddings(row, device, train_params, new_model):\n","  #read in the data\n","  cropped = sitk.ReadImage(row['crop-CTA-64']) #fixed (not model predictions)\n","  ROI = sitk.ReadImage(row['infer-ROI-CTA-64']) #inference (actual model predictions)\n","  #prepare\n","  cropped_inputs = torch.Tensor(np.expand_dims(sitk.GetArrayFromImage(cropped), axis = (0, 1)))\n","  ROI_inputs = torch.Tensor(np.expand_dims(sitk.GetArrayFromImage(ROI), axis = (0, 1)))\n","  #pass through the bottleneck layer\n","  cropped_embeddings = new_model(cropped_inputs)\n","  ROI_embeddings = new_model(ROI_inputs)\n","  #reshape\n","  cropped_embeddings = torch.reshape(cropped_embeddings, (64, 64, 32)).to(device).detach().numpy()\n","  ROI_embeddings = torch.reshape(ROI_embeddings, (64, 64, 32)).to(device).detach().numpy()\n","  #save\n","  cropped_path = save_image(train_params['classification_save_path'], 'cropped-embeddings', row['Patient'], \n","                            sitk.GetImageFromArray(cropped_embeddings))\n","  ROI_path = save_image(train_params['classification_save_path'], 'ROI-embeddings', row['Patient'], \n","                        sitk.GetImageFromArray(ROI_embeddings))\n","\n","  #return\n","  return cropped_path, ROI_path\n","\n","#read in the embeddings\n","def read_embeddings(row):\n","  #embedding\n","  cropped_embedding = sitk.GetArrayFromImage(sitk.ReadImage(row['cropped-embeddings']))\n","  ROI_embedding = sitk.GetArrayFromImage(sitk.ReadImage(row['ROI-embeddings']))\n","  return cropped_embedding, ROI_embedding\n","\n","def read_embeddings2(row, type_embed):\n","  #embedding\n","  embedding = sitk.GetArrayFromImage(sitk.ReadImage(row[type_embed]))\n","  #other info\n","  if row['Status'] == 'Surveillance':\n","    rupture_risk = 0\n","  else:\n","    rupture_risk = 1\n","  patient = row['Patient']\n","  #return\n","  return embedding, rupture_risk, patient\n","\n","#ResNet script inputs\n","def get_ResNet_inputs(df, data, train_params):\n","  #get data\n","  df_sub = df[df['DATA'] == data]\n","  #get embeddings\n","  cropped_embeddings, ROI_embeddings = zip(*df_sub.apply(read_embeddings, axis = 1))\n","  #stack\n","  cropped_embeddings = np.stack(cropped_embeddings)\n","  ROI_embeddings = np.stack(ROI_embeddings)\n","  #save the information\n","  np.savez_compressed(train_params['classification_save_path'] + 'cropped-embeddings-' + data + '.npz', \n","                      cropped_embeddings)\n","  np.savez_compressed(train_params['classification_save_path'] + 'ROI-embeddings-' + data + '.npz', \n","                      ROI_embeddings)\n","  \n","#ResNet script inputs\n","def get_ResNet_inputs2(df, data, train_params):\n","  #get data\n","  df_sub = df[df['DATA'] == data]\n","  #get embeddings\n","  cropped_embeddings = list(df_sub.apply(read_embeddings2, axis = 1, args =('cropped-embeddings',)))\n","  ROI_embeddings = list(df_sub.apply(read_embeddings2, axis = 1, args =('ROI-embeddings',)))\n","  #save the information\n","  np.savez_compressed(train_params['classification_save_path'] + 'cropped-embeddings-' + data + '.npz', \n","                      cropped_embeddings)\n","  np.savez_compressed(train_params['classification_save_path'] + 'ROI-embeddings-' + data + '.npz', \n","                      ROI_embeddings)"],"metadata":{"id":"2CuQSSR9d6KA","executionInfo":{"status":"ok","timestamp":1662912256034,"user_tz":420,"elapsed":169,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#load in the file with the high-res segmentations\n","df = pd.read_pickle('/content/gdrive/MyDrive/AAA_Project/Masters-Thesis/AAA-DICOM/BB-AAA-UNet-results/high-res-segmentation-2022-09-03-22-06/df_results.pkl')"],"metadata":{"id":"dNtf9fXvBIXV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save the classification data to a folder\n","\n","#save folder\n","save_folder = '/content/gdrive/MyDrive/AAA_Project/Masters-Thesis/AAA-DICOM/BB-AAA-UNet-results/'\n","#specify a save location\n","folder = datetime.now().strftime(\"classifier-%Y-%m-%d-%H-%M\") + '/'\n","#the save path for everything\n","save_path = save_folder + folder\n","if os.path.isdir(save_path) == False:\n","  os.mkdir(save_path)"],"metadata":{"id":"AdsiM4ItCOn_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load in the first model\n","train_params = None\n","model = None\n","train_params, model = load_pretrained_model('/content/gdrive/MyDrive/AAA_Project/Masters-Thesis/AAA-DICOM/BB-AAA-UNet-results/localizer-2022-09-03-19-33/', device)\n","train_params['classification_save_path'] = save_path"],"metadata":{"id":"JYEoOl6LAN2r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","#need to pass in the inference CTAs\n","df['infer-ROI-CTA-64'] = df.apply(localized_CTA_inference, axis = 1, args = (train_params, model, device))\n","#save the train params info\n","save_params(train_params, train_params['save_path'] + 'train_params.json')\n","#save the pkl file as well\n","#classification folder\n","df.to_pickle(train_params['classification_save_path'] + 'df_results.pkl')"],"metadata":{"id":"_GCrdWH5BBjv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load in the second model\n","train_params = None\n","model = None\n","train_params, model = load_pretrained_model('/content/gdrive/MyDrive/AAA_Project/Masters-Thesis/AAA-DICOM/BB-AAA-UNet-results/high-res-segmentation-2022-09-03-22-06/', device)"],"metadata":{"id":"h7keKCF0eLsE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save the second model in the correct format (temporoary)\n","torch.save(model.state_dict(), '/content/model.pth')"],"metadata":{"id":"FULh91Qkqsr-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define the model\n","model = None\n","model_params = train_params['model_params'] #high res\n","new_model = UNet_Bottleneck(model_params['n_channels'], model_params['base_filter_num'], model_params['num_blocks'], model_params['num_classes'], \n","                            model_params['mode'], model_params['dropout'], model_params['dropout_rate'], model_params['use_pooling'])"],"metadata":{"id":"nzSBLn-kdwrd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#now load in the pretrained model for embedding extraction\n","new_model.load_state_dict(torch.load('/content/model.pth'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vq9QxaT_rTNY","executionInfo":{"status":"ok","timestamp":1662852424940,"user_tz":420,"elapsed":12829,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}},"outputId":"2505b9af-d216-414d-e479-3116c356a1db"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["%%time\n","#update the df with the embeddings\n","df['cropped-embeddings'], df['ROI-embeddings'] = zip(*df.apply(generate_embeddings, axis = 1, args = (device, train_params, new_model)))\n","#save the df in the classification_path\n","df.to_pickle(train_params['classification_save_path'] + 'df_results.pkl')"],"metadata":{"id":"F4pY8A54y6eM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#read in the train params and df\n","train_params = load_params('/content/gdrive/MyDrive/AAA_Project/Masters-Thesis/AAA-DICOM/BB-AAA-UNet-results/classifier-2022-09-10-22-12/train_params.json')\n","df = pd.read_pickle('/content/gdrive/MyDrive/AAA_Project/Masters-Thesis/AAA-DICOM/BB-AAA-UNet-results/classifier-2022-09-10-22-12/df_results.pkl')"],"metadata":{"id":"hP0y6JHayYUC","executionInfo":{"status":"ok","timestamp":1662912269558,"user_tz":420,"elapsed":2081,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["%%time\n","#convert the embeddings into a usable format (since the ResNet model requires local training)\n","get_ResNet_inputs2(df, 'TRAIN', train_params)\n","get_ResNet_inputs2(df, 'TEST', train_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q2zT-we_00ZC","executionInfo":{"status":"ok","timestamp":1662904001653,"user_tz":420,"elapsed":40698,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}},"outputId":"68a31ab4-922a-4986-8275-047c01245264"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:719: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  val = np.asanyarray(val)\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 5.47 s, sys: 727 ms, total: 6.2 s\n","Wall time: 40.5 s\n"]}]},{"cell_type":"code","source":["#save the train params info\n","save_params(train_params, train_params['classification_save_path'] + 'train_params.json')"],"metadata":{"id":"om3bWC156wPL","executionInfo":{"status":"ok","timestamp":1662904010969,"user_tz":420,"elapsed":152,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#read in the comressed numpy files (example)\n","arr = np.load('/content/gdrive/MyDrive/AAA_Project/Masters-Thesis/AAA-DICOM/BB-AAA-UNet-results/classifier-2022-09-10-22-12/cropped-embeddings-TEST.npz', allow_pickle = True)['arr_0']\n","print(arr.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JanvyG2g5vK6","executionInfo":{"status":"ok","timestamp":1662904078967,"user_tz":420,"elapsed":370,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}},"outputId":"fe1cdc33-9683-4383-9b03-ccaf19ffbcd8"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["(30, 3)\n","(30, 3)\n"]}]},{"cell_type":"code","source":["#ResNet script inputs\n","def get_ResNet_inputs3(df, data, train_params):\n","  #get data\n","  df_sub = df[df['DATA'] == data]\n","  #get embeddings\n","  cropped_embeddings = list(df_sub.apply(read_embeddings2, axis = 1, args =('crop-CTA-64',)))\n","  ROI_embeddings = list(df_sub.apply(read_embeddings2, axis = 1, args =('infer-ROI-CTA-64',)))\n","  #save the information\n","  np.savez_compressed(train_params['classification_save_path'] + 'crop-CTA-64-' + data + '.npz', \n","                      cropped_embeddings)\n","  np.savez_compressed(train_params['classification_save_path'] + 'infer-ROI-CTA-64-' + data + '.npz', \n","                      ROI_embeddings)"],"metadata":{"id":"6FutDjEETOXh","executionInfo":{"status":"ok","timestamp":1662912450670,"user_tz":420,"elapsed":156,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["#also generate data for the cropped images\n","df.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qe59kUCMSULk","executionInfo":{"status":"ok","timestamp":1662912277632,"user_tz":420,"elapsed":174,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}},"outputId":"d102eaff-d06e-485c-9fd2-376796c7e31f"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Original CTA', 'Original SegGT', 'Status', 'Hospital', 'Patient',\n","       'CTA', 'SegGT', 'AAA-ILT-Calc', 'AAA', 'CTA-64', 'AAA-ILT-Calc-64',\n","       'crop-CTA-64', 'crop-AAA-64', 'crop-CTA', 'AAA-64', 'DATA',\n","       'localized-predictions', 'high-res-predictions', 'DSC', 'FP', 'FN',\n","       'JC', 'infer-ROI-CTA-64', 'cropped-embeddings', 'ROI-embeddings'],\n","      dtype='object')"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["get_ResNet_inputs3(df, 'TRAIN', train_params)\n","get_ResNet_inputs3(df, 'TEST', train_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hz_e8lOnTozc","executionInfo":{"status":"ok","timestamp":1662912569440,"user_tz":420,"elapsed":77074,"user":{"displayName":"Anish Salvi","userId":"07789164058419329949"}},"outputId":"2707f97c-2353-4083-8c36-632525a810fe"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:719: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  val = np.asanyarray(val)\n"]}]},{"cell_type":"code","source":["#read in the comressed numpy files (example)\n","arr = np.load('/content/gdrive/MyDrive/AAA_Project/Masters-Thesis/AAA-DICOM/BB-AAA-UNet-results/classifier-2022-09-10-22-12/cropped-embeddings-TEST.npz', allow_pickle = True)['arr_0']\n","print(arr.shape)"],"metadata":{"id":"cVDYhs_8WgwG"},"execution_count":null,"outputs":[]}]}