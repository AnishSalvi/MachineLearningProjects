{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuh99H6VWfe4"
      },
      "outputs": [],
      "source": [
        "#installations\n",
        "#!pip install --quiet SimpleITK\n",
        "#Anaconda Powershell\n",
        "#:L\n",
        "#conda activate data_processing\n",
        "#jupyter serverextension enable -- py jupyter_http_over_ws\n",
        "#jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8892 --NotebookApp.port_retries=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "import sys\n",
        "#this path must be specified\n",
        "sys.path.append('AnishSalvi/ImageRx/PAD-Net/vRAD_Dataset_Inputs/Transformer_Explainability')\n",
        "from modules.layers_ours import *\n",
        "from baselines.ViT.helpers import load_pretrained\n",
        "from baselines.ViT.weight_init import trunc_normal_\n",
        "from baselines.ViT.layer_helpers import to_2tuple\n",
        "from baselines.ViT.ViT_explanation_generator import LRP\n",
        "#import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import json\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#check if the gpu machine is available\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  gpu = torch.cuda.get_device_name(0)\n",
        "  print('Device: ', gpu)\n",
        "else:\n",
        "  device = 'cpu'\n",
        "  gpu = None\n",
        "  print('Device', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPbXCbnpWxzc",
        "outputId": "f61cce87-ccb5-4886-ec81-72e8c4c1a9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  NVIDIA GeForce RTX 3060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#functions\n",
        "\n",
        "def compute_rollout_attention(all_layer_matrices, start_layer=0):\n",
        "    # adding residual consideration\n",
        "    num_tokens = all_layer_matrices[0].shape[1]\n",
        "    batch_size = all_layer_matrices[0].shape[0]\n",
        "    eye = torch.eye(num_tokens).expand(batch_size, num_tokens, num_tokens).to(all_layer_matrices[0].device)\n",
        "    all_layer_matrices = [all_layer_matrices[i] + eye for i in range(len(all_layer_matrices))]\n",
        "    # all_layer_matrices = [all_layer_matrices[i] / all_layer_matrices[i].sum(dim=-1, keepdim=True)\n",
        "    #                       for i in range(len(all_layer_matrices))]\n",
        "    joint_attention = all_layer_matrices[start_layer]\n",
        "    for i in range(start_layer+1, len(all_layer_matrices)):\n",
        "        joint_attention = all_layer_matrices[i].bmm(joint_attention)\n",
        "    return joint_attention\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = Linear(in_features, hidden_features)\n",
        "        self.act = GELU()\n",
        "        self.fc2 = Linear(hidden_features, out_features)\n",
        "        self.drop = Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "    def relprop(self, cam, **kwargs):\n",
        "        cam = self.drop.relprop(cam, **kwargs)\n",
        "        cam = self.fc2.relprop(cam, **kwargs)\n",
        "        cam = self.act.relprop(cam, **kwargs)\n",
        "        cam = self.fc1.relprop(cam, **kwargs)\n",
        "        return cam\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False,attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        # A = Q*K^T\n",
        "        self.matmul1 = einsum('bhid,bhjd->bhij')\n",
        "        # attn = A*V\n",
        "        self.matmul2 = einsum('bhij,bhjd->bhid')\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = Dropout(attn_drop)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(proj_drop)\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "\n",
        "        self.attn_cam = None\n",
        "        self.attn = None\n",
        "        self.v = None\n",
        "        self.v_cam = None\n",
        "        self.attn_gradients = None\n",
        "\n",
        "    def get_attn(self):\n",
        "        return self.attn\n",
        "\n",
        "    def save_attn(self, attn):\n",
        "        self.attn = attn\n",
        "\n",
        "    def save_attn_cam(self, cam):\n",
        "        self.attn_cam = cam\n",
        "\n",
        "    def get_attn_cam(self):\n",
        "        return self.attn_cam\n",
        "\n",
        "    def get_v(self):\n",
        "        return self.v\n",
        "\n",
        "    def save_v(self, v):\n",
        "        self.v = v\n",
        "\n",
        "    def save_v_cam(self, cam):\n",
        "        self.v_cam = cam\n",
        "\n",
        "    def get_v_cam(self):\n",
        "        return self.v_cam\n",
        "\n",
        "    def save_attn_gradients(self, attn_gradients):\n",
        "        self.attn_gradients = attn_gradients\n",
        "\n",
        "    def get_attn_gradients(self):\n",
        "        return self.attn_gradients\n",
        "\n",
        "    #this may cause issues for 3d? or does it act upoon the patches\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.num_heads\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
        "\n",
        "        self.save_v(v)\n",
        "\n",
        "        dots = self.matmul1([q, k]) * self.scale\n",
        "\n",
        "        attn = self.softmax(dots)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        self.save_attn(attn)\n",
        "        attn.register_hook(self.save_attn_gradients)\n",
        "\n",
        "        out = self.matmul2([attn, v])\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "    def relprop(self, cam, **kwargs):\n",
        "        cam = self.proj_drop.relprop(cam, **kwargs)\n",
        "        cam = self.proj.relprop(cam, **kwargs)\n",
        "        cam = rearrange(cam, 'b n (h d) -> b h n d', h=self.num_heads)\n",
        "\n",
        "        # attn = A*V\n",
        "        (cam1, cam_v)= self.matmul2.relprop(cam, **kwargs)\n",
        "        cam1 /= 2\n",
        "        cam_v /= 2\n",
        "\n",
        "        self.save_v_cam(cam_v)\n",
        "        self.save_attn_cam(cam1)\n",
        "\n",
        "        cam1 = self.attn_drop.relprop(cam1, **kwargs)\n",
        "        cam1 = self.softmax.relprop(cam1, **kwargs)\n",
        "\n",
        "        # A = Q*K^T\n",
        "        (cam_q, cam_k) = self.matmul1.relprop(cam1, **kwargs)\n",
        "        cam_q /= 2\n",
        "        cam_k /= 2\n",
        "\n",
        "        cam_qkv = rearrange([cam_q, cam_k, cam_v], 'qkv b h n d -> b n (qkv h d)', qkv=3, h=self.num_heads)\n",
        "\n",
        "        return self.qkv.relprop(cam_qkv, **kwargs)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = LayerNorm(dim, eps=1e-6)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.norm2 = LayerNorm(dim, eps=1e-6)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n",
        "\n",
        "        self.add1 = Add()\n",
        "        self.add2 = Add()\n",
        "        self.clone1 = Clone()\n",
        "        self.clone2 = Clone()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1, x2 = self.clone1(x, 2)\n",
        "        x = self.add1([x1, self.attn(self.norm1(x2))])\n",
        "        x1, x2 = self.clone2(x, 2)\n",
        "        x = self.add2([x1, self.mlp(self.norm2(x2))])\n",
        "        return x\n",
        "\n",
        "    def relprop(self, cam, **kwargs):\n",
        "        (cam1, cam2) = self.add2.relprop(cam, **kwargs)\n",
        "        cam2 = self.mlp.relprop(cam2, **kwargs)\n",
        "        cam2 = self.norm2.relprop(cam2, **kwargs)\n",
        "        cam = self.clone2.relprop((cam1, cam2), **kwargs)\n",
        "\n",
        "        (cam1, cam2) = self.add1.relprop(cam, **kwargs)\n",
        "        cam2 = self.attn.relprop(cam2, **kwargs)\n",
        "        cam2 = self.norm1.relprop(cam2, **kwargs)\n",
        "        cam = self.clone1.relprop((cam1, cam2), **kwargs)\n",
        "        return cam\n",
        "\n",
        "#3D patch embedder\n",
        "class PatchEmbed3D(nn.Module):\n",
        "  \"\"\" Image to Patch Embedding\n",
        "  \"\"\"\n",
        "  def __init__(self, img_size = (200, 100, 150), patch_size = (20, 10, 15), in_chans = 1, embed_dim = 768):\n",
        "    super().__init__()\n",
        "    img_size = to_2tuple(img_size)\n",
        "    patch_size = to_2tuple(patch_size)\n",
        "    num_patches = (img_size[2] // patch_size[2]) * (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.num_patches = num_patches\n",
        "    self.proj = torch.nn.Conv3d(in_chans, embed_dim, kernel_size = patch_size, stride = patch_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, H, W, D = x.shape\n",
        "    # FIXME look at relaxing size constraints\n",
        "    assert H == self.img_size[0] and W == self.img_size[1] and D == self.img_size[2], \\\n",
        "      f\"Input image size ({H}*{W}*{D}) doesn't match model ({self.img_size[0]} * {self.img_size[1]} * {self.img_size[2]}).\"\n",
        "    x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "    return x\n",
        "\n",
        "  def relprop(self, cam, **kwargs):\n",
        "    cam = cam.transpose(1,2) #why transpose?\n",
        "    cam = cam.reshape(cam.shape[0], cam.shape[1], cam.reshape[2], #is the cam in 3d as well?\n",
        "                      (self.img_size[0] // self.patch_size[0]),\n",
        "                      (self.img_size[1] // self.patch_size[1]),\n",
        "                      (self.img_size[2] // self.patch_size[2])\n",
        "                      )\n",
        "    return self.proj.relprop(cam, **kwargs)\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=(200, 100, 150), patch_size=(20, 10, 15), in_chans=1, num_classes=2, embed_dim=768, depth=12,\n",
        "                 num_heads=12, mlp_ratio=4., qkv_bias=False, mlp_head=False, drop_rate=0., attn_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        self.patch_embed = PatchEmbed3D(\n",
        "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "        self.norm = LayerNorm(embed_dim)\n",
        "        if mlp_head:\n",
        "            # paper diagram suggests 'MLP head', but results in 4M extra parameters vs paper\n",
        "            self.head = Mlp(embed_dim, int(embed_dim * mlp_ratio), num_classes)\n",
        "        else:\n",
        "            # with a single Linear layer as head, the param count within rounding of paper\n",
        "            self.head = Linear(embed_dim, num_classes)\n",
        "\n",
        "        # FIXME not quite sure what the proper weight init is supposed to be,\n",
        "        # normal / trunc normal w/ std == .02 similar to other Bert like transformers\n",
        "        trunc_normal_(self.pos_embed, std=.02)  # embeddings same as weights?\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        self.pool = IndexSelect()\n",
        "        self.add = Add()\n",
        "\n",
        "        self.inp_grad = None\n",
        "\n",
        "    def save_inp_grad(self,grad):\n",
        "        self.inp_grad = grad\n",
        "\n",
        "    def get_inp_grad(self):\n",
        "        return self.inp_grad\n",
        "\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @property\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token'}\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.add([x, self.pos_embed])\n",
        "\n",
        "        x.register_hook(self.save_inp_grad)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.pool(x, dim=1, indices=torch.tensor(0, device=x.device))\n",
        "        x = x.squeeze(1)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "    def relprop(self, cam=None,method=\"transformer_attribution\", is_ablation=False, start_layer=0, **kwargs):\n",
        "        # print(kwargs)\n",
        "        # print(\"conservation 1\", cam.sum())\n",
        "        cam = self.head.relprop(cam, **kwargs)\n",
        "        cam = cam.unsqueeze(1)\n",
        "        cam = self.pool.relprop(cam, **kwargs)\n",
        "        cam = self.norm.relprop(cam, **kwargs)\n",
        "        for blk in reversed(self.blocks):\n",
        "            cam = blk.relprop(cam, **kwargs)\n",
        "\n",
        "        # print(\"conservation 2\", cam.sum())\n",
        "        # print(\"min\", cam.min())\n",
        "\n",
        "        if method == \"full\":\n",
        "            (cam, _) = self.add.relprop(cam, **kwargs)\n",
        "            cam = cam[:, 1:]\n",
        "            cam = self.patch_embed.relprop(cam, **kwargs)\n",
        "            # sum on channels\n",
        "            cam = cam.sum(dim=1)\n",
        "            return cam\n",
        "\n",
        "        elif method == \"rollout\":\n",
        "            # cam rollout\n",
        "            attn_cams = []\n",
        "            for blk in self.blocks:\n",
        "                attn_heads = blk.attn.get_attn_cam().clamp(min=0)\n",
        "                avg_heads = (attn_heads.sum(dim=1) / attn_heads.shape[1]).detach()\n",
        "                attn_cams.append(avg_heads)\n",
        "            cam = compute_rollout_attention(attn_cams, start_layer=start_layer)\n",
        "            cam = cam[:, 0, 1:]\n",
        "            return cam\n",
        "\n",
        "        # our method, method name grad is legacy\n",
        "        elif method == \"transformer_attribution\" or method == \"grad\":\n",
        "            cams = []\n",
        "            for blk in self.blocks:\n",
        "                grad = blk.attn.get_attn_gradients()\n",
        "                cam = blk.attn.get_attn_cam()\n",
        "                cam = cam[0].reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "                grad = grad[0].reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "                cam = grad * cam\n",
        "                cam = cam.clamp(min=0).mean(dim=0)\n",
        "                cams.append(cam.unsqueeze(0))\n",
        "            rollout = compute_rollout_attention(cams, start_layer=start_layer)\n",
        "            cam = rollout[:, 0, 1:]\n",
        "            return cam\n",
        "\n",
        "        elif method == \"last_layer\":\n",
        "            cam = self.blocks[-1].attn.get_attn_cam()\n",
        "            cam = cam[0].reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "            if is_ablation:\n",
        "                grad = self.blocks[-1].attn.get_attn_gradients()\n",
        "                grad = grad[0].reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "                cam = grad * cam\n",
        "            cam = cam.clamp(min=0).mean(dim=0)\n",
        "            cam = cam[0, 1:]\n",
        "            return cam\n",
        "\n",
        "        elif method == \"last_layer_attn\":\n",
        "            cam = self.blocks[-1].attn.get_attn()\n",
        "            cam = cam[0].reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "            cam = cam.clamp(min=0).mean(dim=0)\n",
        "            cam = cam[0, 1:]\n",
        "            return cam\n",
        "\n",
        "        elif method == \"second_layer\":\n",
        "            cam = self.blocks[1].attn.get_attn_cam()\n",
        "            cam = cam[0].reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "            if is_ablation:\n",
        "                grad = self.blocks[1].attn.get_attn_gradients()\n",
        "                grad = grad[0].reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "                cam = grad * cam\n",
        "            cam = cam.clamp(min=0).mean(dim=0)\n",
        "            cam = cam[0, 1:]\n",
        "            return cam\n",
        "\n",
        "\n",
        "def _conv_filter(state_dict, patch_size=30):\n",
        "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
        "    out_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if 'patch_embed.proj.weight' in k:\n",
        "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
        "        out_dict[k] = v\n",
        "    return out_dict\n",
        "\n",
        "#save json file\n",
        "def save_params(hyper_params, save_path):\n",
        "  json_string = json.dumps(hyper_params)\n",
        "  with open(save_path, 'w') as outfile:\n",
        "    outfile.write(json_string)\n",
        "\n",
        "#load the train params back in\n",
        "def load_params(fpath):\n",
        "  # Opening JSON file\n",
        "  with open(fpath) as json_file:\n",
        "    data = json.load(json_file)\n",
        "  return data\n",
        "\n",
        "#get the model\n",
        "def get_model(model_config):\n",
        "  #model\n",
        "  model = VisionTransformer(\n",
        "      img_size = model_config['img_size'],\n",
        "      patch_size = model_config['patch_size'],\n",
        "      in_chans = model_config['in_chans'],\n",
        "      num_classes = model_config['num_classes'],\n",
        "      embed_dim = model_config['embed_dim'],\n",
        "      depth = model_config['depth'],\n",
        "      num_heads = model_config['num_heads'],\n",
        "      mlp_ratio = model_config['mlp_ratio'],\n",
        "      qkv_bias = model_config['qkv_bias'],\n",
        "      mlp_head = model_config['mlp_head'],\n",
        "      drop_rate = model_config['drop_rate'],\n",
        "      attn_drop_rate = model_config['attn_drop_rate']\n",
        "  )\n",
        "  #return\n",
        "  return model\n",
        "\n",
        "#load pretrained model\n",
        "def load_pretrained_model(save_folder, device):\n",
        "  #load\n",
        "  model_config = load_params(save_folder + 'model_config.json')\n",
        "  #weights\n",
        "  if model_config['save_weights_only']:\n",
        "    model = get_model(model_config)\n",
        "    #important to do prior\n",
        "    model.eval()\n",
        "    #load weights\n",
        "    model.load_state_dict(torch.load(model_config['save_folder'] + 'model_weights.pth', map_location = device))\n",
        "  else:\n",
        "    model = torch.load(model_config['save_folder'] + 'model.pth', map_location = device)\n",
        "  #send\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  #return\n",
        "  return model, model_config\n",
        "\n",
        "\n",
        "#Dataset\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, df, col_image):\n",
        "      #params\n",
        "      self.df = df\n",
        "      self.col_image = col_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      #row\n",
        "      row = self.df.iloc[idx]\n",
        "\n",
        "      #read image\n",
        "      image = torch.Tensor(np.expand_dims(sitk.GetArrayFromImage(sitk.ReadImage(row[self.col_image])), axis = 0))\n",
        "\n",
        "      #return\n",
        "      return image\n",
        "\n",
        "#split\n",
        "def split_given_size(a, size):\n",
        "  return np.split(a, np.arange(size, len(a), size))\n",
        "\n",
        "#determine dataset splitting\n",
        "def get_split(row, model_config):\n",
        "  if row['KFold'] in model_config['val_kfolds']:\n",
        "    dataset = 'VAL'\n",
        "  else:\n",
        "    dataset = 'TRAIN'\n",
        "  #return\n",
        "  return dataset\n",
        "\n",
        "#function determine classification performance\n",
        "def get_performance(df, model_config):\n",
        "  #labels2class\n",
        "  encoding = remove_repeat_sort(dict(zip(df['Annotation'],\n",
        "                                           df[model_config['col_label']])))\n",
        "  #store info\n",
        "  model_config['encoding'] = encoding\n",
        "\n",
        "  #iterate\n",
        "  for split in set(df['Split']):\n",
        "    #split data\n",
        "    df_data = df[df['Split'] == split]\n",
        "    #report\n",
        "    config = sklearn.metrics.classification_report(df_data[model_config['col_label']].to_numpy().astype(float),\n",
        "                                                   df_data['Class_Prediction'].to_numpy(), target_names = list(encoding.keys()),\n",
        "                                                   output_dict = True)\n",
        "\n",
        "    #save\n",
        "    model_config[split + '_performance'] = config\n",
        "    #plot confusion matrix\n",
        "    plot_confusion_matrix(df_data[model_config['col_label']].to_numpy().astype(float),\n",
        "                          df_data['Class_Prediction'].to_numpy(),\n",
        "                          split, list(encoding.keys()), model_config)\n",
        "\n",
        "  #save the config\n",
        "  save_params(model_config, model_config['save_folder'] + 'model_config.json')\n",
        "\n",
        "#remove repeat and sort\n",
        "def remove_repeat_sort(input_raw):\n",
        "  #remove repeat\n",
        "  result = {}\n",
        "  for key,value in input_raw.items():\n",
        "    if value not in result.values():\n",
        "        result[key] = value\n",
        "  #sort\n",
        "  keys = list(result.keys())\n",
        "  values = list(result.values())\n",
        "  sorted_value_index = np.argsort(values)\n",
        "  sorted_dict = {keys[i]: values[i] for i in sorted_value_index}\n",
        "  #return\n",
        "  return sorted_dict\n",
        "\n",
        "#plot confusion matrix\n",
        "def plot_confusion_matrix(labels, predictions, title, ticks, model_config):\n",
        "  #cmap\n",
        "  cm = confusion_matrix(labels, predictions)\n",
        "  #sub\n",
        "  ax = plt.subplot()\n",
        "  sns.set(font_scale=2.0) # Adjust to fit\n",
        "  sns.heatmap(cm, annot=True, ax=ax, cmap=\"binary\", fmt=\"g\");\n",
        "  # Labels, title and ticks\n",
        "  label_font = {'size':'20'}  # Adjust to fit\n",
        "  ax.set_xlabel('Predictions', fontdict=label_font);\n",
        "  ax.set_ylabel('Class Labels', fontdict=label_font);\n",
        "\n",
        "  title_font = {'size':'20'}  # Adjust to fit\n",
        "  ax.set_title(title, fontdict=title_font);\n",
        "\n",
        "  ax.tick_params(axis='both', which='major', labelsize=8)  # Adjust to fit\n",
        "  if ticks is not None:\n",
        "    ax.xaxis.set_ticklabels(ticks);\n",
        "    ax.yaxis.set_ticklabels(ticks);\n",
        "\n",
        "  #save\n",
        "  plt.savefig(model_config['save_folder'] + title + '.png', bbox_inches = \"tight\")\n",
        "\n",
        "  #plt.show()\n",
        "  plt.close()\n",
        "\n",
        "#analyuze model config\n",
        "def analyze(model_config):\n",
        "  ls_metrics = []\n",
        "  df_col = []\n",
        "  #append\n",
        "  ls_metrics.append(model_config['save_folder'])\n",
        "  df_col.append('save_folder')\n",
        "  for tag in ['TRAIN', 'VAL']:\n",
        "    for class_label in (list(model_config['encoding'].keys())+['weighted avg', 'macro avg', 'accuracy']):\n",
        "      metrics = model_config[tag +'_performance'][class_label]\n",
        "      #just for accuracy\n",
        "      if class_label == 'accuracy':\n",
        "        ls_metrics.append(metrics)\n",
        "        df_col.append(tag +  ' ' + class_label)\n",
        "      else:\n",
        "        #all else\n",
        "        for metric_type in list(metrics.keys()):\n",
        "          ls_metrics.append(metrics[metric_type])\n",
        "          df_col.append(tag +  ' ' + class_label + ' ' + metric_type)\n",
        "  return ls_metrics, df_col"
      ],
      "metadata": {
        "id": "YnGADVktWzX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#params\n",
        "sweep_folder = 'AnishSalvi/ImageRx/PAD-Net/ViT_classification/results/3D-ViT-sweep-2023-03-05-20-46-10/'\n",
        "model_tag = '3D-ViT'"
      ],
      "metadata": {
        "id": "5uisa6XOW02H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#main\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  #load\n",
        "  sweep_config = load_params(sweep_folder + 'sweep_config.json')\n",
        "  #df\n",
        "  df = pd.read_pickle(sweep_config['parameters']['data_path']['value'])\n",
        "  #get the data\n",
        "  dset_infer = CustomImageDataset(df, sweep_config['parameters']['col_image']['value'])\n",
        "\n",
        "  #get model paths\n",
        "  model_dir = sorted(glob.glob(sweep_folder + model_tag + '*'))\n",
        "\n",
        "  #iterate\n",
        "  for model_path in model_dir:\n",
        "    #infer\n",
        "    save_dfs = []\n",
        "    #clear\n",
        "    model, model_config = (None, None)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "    #load params\n",
        "    model, model_config = load_pretrained_model(model_path + '/', device)\n",
        "    #load\n",
        "    infer_loader = DataLoader(dset_infer, batch_size = model_config['batch_size'])\n",
        "    #split the df in similar fashion\n",
        "    ls_df = split_given_size(df, model_config['batch_size'])\n",
        "    #iterate\n",
        "    for i, x in enumerate(infer_loader):\n",
        "      #get the small\n",
        "      df_small = ls_df[i]\n",
        "      #pass through pretrained model\n",
        "      gpu_prediction = model(x.to(device))\n",
        "      #obtain the individual class predictions\n",
        "      pred_prob = torch.softmax(gpu_prediction, dim = 1)\n",
        "      #obtain the class prediction by majority\n",
        "      pred_class = torch.argmax(pred_prob, dim = 1)\n",
        "      #class model prediction\n",
        "      df_small['Class_Prediction'] = pred_class.cpu().detach().numpy()\n",
        "      #individual class probabilities\n",
        "      for j in range(model_config['num_classes']):\n",
        "        pred_tag = 'Class_' + str(j) + '_Prob'\n",
        "        df_small[pred_tag] = pred_prob.cpu().detach().numpy()[:, j]\n",
        "      #append\n",
        "      save_dfs.append(df_small)\n",
        "      #reset\n",
        "      x, gpu_prediction, pred_prob, pred_class = (None, None, None, None)\n",
        "      if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    #concat\n",
        "    df_pred = pd.concat(save_dfs)\n",
        "    #label if in training or validation\n",
        "    df_pred['Split'] = df_pred.apply(get_split, axis = 1, args = (model_config,))\n",
        "    #can perform analysis here later\n",
        "    get_performance(df_pred, model_config)\n",
        "    #save the results\n",
        "    df_pred.to_pickle(model_config['save_folder'] + 'df_results.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "365TWDwjW2QZ",
        "outputId": "24f4cf50-e5e9-49d4-f56b-5bc8b13b8256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: total: 3min 37s\n",
            "Wall time: 1min 52s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#specify the most relevant columns to keep during analysis\n",
        "cols = ['save_folder',\n",
        "        'VAL Control/Non-PAD precision', 'VAL Control/Non-PAD recall', 'VAL Control/Non-PAD f1-score',\n",
        "        'VAL PAD, <50% stenosis precision', 'VAL PAD, <50% stenosis recall','VAL PAD, <50% stenosis f1-score',\n",
        "        'VAL PAD, >50% stenosis precision', 'VAL PAD, >50% stenosis recall', 'VAL PAD, >50% stenosis f1-score',\n",
        "\n",
        "        'VAL weighted avg precision', 'VAL weighted avg recall', 'VAL weighted avg f1-score', 'VAL weighted avg support', 'VAL accuracy'\n",
        "\n",
        " ]\n",
        "#sweep folder\n",
        "#model tag"
      ],
      "metadata": {
        "id": "zDdKBE_oVo78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#load\n",
        "sweep_config = load_params(sweep_folder + 'sweep_config.json')\n",
        "#get model paths\n",
        "model_dir = sorted(glob.glob(sweep_folder + model_tag + '*'))\n",
        "\n",
        "#performance\n",
        "performance = []\n",
        "#iterate\n",
        "for model_path in model_dir:\n",
        "  model_config = load_params(model_path + '/' + 'model_config.json')\n",
        "  #analyze\n",
        "  model_metrics, df_cols = analyze(model_config)\n",
        "  #append\n",
        "  performance.append(model_metrics)\n",
        "#init\n",
        "df_sweep = pd.DataFrame(performance, columns = df_cols)\n",
        "#sort\n",
        "df_sweep = df_sweep.sort_values(by = ['VAL weighted avg f1-score'], ascending = False, ignore_index = True)\n",
        "#save\n",
        "df_sweep.to_excel(sweep_config['parameters']['save_folder']['value'] + 'df_results.xlsx')\n",
        "#visual\n",
        "display(df_sweep[cols].round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "clEiwMQNIGmO",
        "outputId": "6b25c3a6-c8f6-4301-fbfd-127b84b6b213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                         save_folder  \\\n",
              "0  AnishSalvi/ImageRx/PAD-Net/ViT_classification/...   \n",
              "\n",
              "   TRAIN Control/Non-PAD precision  TRAIN Control/Non-PAD recall  \\\n",
              "0                             0.73                          0.67   \n",
              "\n",
              "   TRAIN Control/Non-PAD f1-score  TRAIN PAD, <50% stenosis precision  \\\n",
              "0                             0.7                                0.35   \n",
              "\n",
              "   TRAIN PAD, <50% stenosis recall  TRAIN PAD, <50% stenosis f1-score  \\\n",
              "0                             0.92                               0.51   \n",
              "\n",
              "   TRAIN PAD, >50% stenosis precision  TRAIN PAD, >50% stenosis recall  \\\n",
              "0                                0.88                             0.55   \n",
              "\n",
              "   TRAIN PAD, >50% stenosis f1-score  VAL weighted avg precision  \\\n",
              "0                               0.67                        0.43   \n",
              "\n",
              "   VAL weighted avg recall  VAL weighted avg f1-score  \\\n",
              "0                     0.26                        0.3   \n",
              "\n",
              "   VAL weighted avg support  VAL accuracy  \n",
              "0                        58          0.26  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>save_folder</th>\n",
              "      <th>TRAIN Control/Non-PAD precision</th>\n",
              "      <th>TRAIN Control/Non-PAD recall</th>\n",
              "      <th>TRAIN Control/Non-PAD f1-score</th>\n",
              "      <th>TRAIN PAD, &lt;50% stenosis precision</th>\n",
              "      <th>TRAIN PAD, &lt;50% stenosis recall</th>\n",
              "      <th>TRAIN PAD, &lt;50% stenosis f1-score</th>\n",
              "      <th>TRAIN PAD, &gt;50% stenosis precision</th>\n",
              "      <th>TRAIN PAD, &gt;50% stenosis recall</th>\n",
              "      <th>TRAIN PAD, &gt;50% stenosis f1-score</th>\n",
              "      <th>VAL weighted avg precision</th>\n",
              "      <th>VAL weighted avg recall</th>\n",
              "      <th>VAL weighted avg f1-score</th>\n",
              "      <th>VAL weighted avg support</th>\n",
              "      <th>VAL accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AnishSalvi/ImageRx/PAD-Net/ViT_classification/...</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.3</td>\n",
              "      <td>58</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: total: 281 ms\n",
            "Wall time: 525 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove wandb stuff saved locally\n",
        "import shutil\n",
        "shutil.rmtree('wandb')"
      ],
      "metadata": {
        "id": "Yro1BB9NkhtI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}