{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ff8d7d63e0f43818208fdb9c483a589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6123d6a37644e0d8c696921e959437b",
              "IPY_MODEL_ce98d1bfc3fc4e3fb816466bfa1cf943"
            ],
            "layout": "IPY_MODEL_efe286c8b308403bb28841a1843b6b4c"
          }
        },
        "a6123d6a37644e0d8c696921e959437b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c2b7f5d390e462495f458ed3be4592a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c3d205e3d6b34bc5bd379358feb6ac00",
            "value": "0.001 MB of 0.012 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "ce98d1bfc3fc4e3fb816466bfa1cf943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0227c18117c46419a47ec10703e1a47",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9d0e4899d9a437ea1c084b5ab0ee5c4",
            "value": 0.09601701153185573
          }
        },
        "efe286c8b308403bb28841a1843b6b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c2b7f5d390e462495f458ed3be4592a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3d205e3d6b34bc5bd379358feb6ac00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0227c18117c46419a47ec10703e1a47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9d0e4899d9a437ea1c084b5ab0ee5c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvW2nSZvgMrM"
      },
      "outputs": [],
      "source": [
        "#installations\n",
        "#!pip install --quiet SimpleITK\n",
        "#Anaconda Powershell\n",
        "#:L\n",
        "#conda activate data_processing\n",
        "#jupyter serverextension enable -- py jupyter_http_over_ws\n",
        "#jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8892 --NotebookApp.port_retries=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "import sys\n",
        "#this path must be specified\n",
        "sys.path.append('AnishSalvi/ImageRx/PAD-Net/vRAD_Dataset_Inputs/Transformer_Explainability')\n",
        "from modules.layers_ours import *\n",
        "from baselines.ViT.helpers import load_pretrained\n",
        "from baselines.ViT.weight_init import trunc_normal_\n",
        "from baselines.ViT.layer_helpers import to_2tuple\n",
        "from baselines.ViT.ViT_explanation_generator import LRP\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "#import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import torchvision\n",
        "#from livelossplot import PlotLosses\n",
        "#import scipy\n",
        "#import sklearn\n",
        "#from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "#from sklearn.utils import shuffle\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "#import seaborn as sns\n",
        "#from ctviewer import CTViewer\n",
        "#from sklearn.metrics import RocCurveDisplay\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import json\n",
        "#check if the gpu machine is available\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  gpu = torch.cuda.get_device_name(0)\n",
        "  print('Device: ', gpu)\n",
        "else:\n",
        "  device = 'cpu'\n",
        "  gpu = None\n",
        "  print('Device', device)\n",
        "\n",
        "#wanddb\n",
        "key = \"23f8a4a10d7a355a4469d7d7b6b696b8536cba35\"\n",
        "#Weights and Bias\n",
        "if key:\n",
        "  wandb.login(key=key) #API Key is in your wandb account, under settings (wandb.ai/settings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnQ_JokVgUcI",
        "outputId": "940c5461-c61d-499b-ae7c-7c399194241d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  NVIDIA GeForce RTX 3060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manish_s\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\MeDCaVE_3/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#functions\n",
        "\n",
        "#functions\n",
        "def compute_rollout_attention(all_layer_matrices, start_layer=0):\n",
        "    # adding residual consideration\n",
        "    num_tokens = all_layer_matrices[0].shape[1]\n",
        "    batch_size = all_layer_matrices[0].shape[0]\n",
        "    eye = torch.eye(num_tokens).expand(batch_size, num_tokens, num_tokens).to(all_layer_matrices[0].device)\n",
        "    all_layer_matrices = [all_layer_matrices[i] + eye for i in range(len(all_layer_matrices))]\n",
        "    # all_layer_matrices = [all_layer_matrices[i] / all_layer_matrices[i].sum(dim=-1, keepdim=True)\n",
        "    #                       for i in range(len(all_layer_matrices))]\n",
        "    joint_attention = all_layer_matrices[start_layer]\n",
        "    for i in range(start_layer+1, len(all_layer_matrices)):\n",
        "        joint_attention = all_layer_matrices[i].bmm(joint_attention)\n",
        "    return joint_attention\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = Linear(in_features, hidden_features)\n",
        "        self.act = GELU()\n",
        "        self.fc2 = Linear(hidden_features, out_features)\n",
        "        self.drop = Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "    def relprop(self, cam, **kwargs):\n",
        "        cam = self.drop.relprop(cam, **kwargs)\n",
        "        cam = self.fc2.relprop(cam, **kwargs)\n",
        "        cam = self.act.relprop(cam, **kwargs)\n",
        "        cam = self.fc1.relprop(cam, **kwargs)\n",
        "        return cam\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False,attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        # A = Q*K^T\n",
        "        self.matmul1 = einsum('bhid,bhjd->bhij')\n",
        "        # attn = A*V\n",
        "        self.matmul2 = einsum('bhij,bhjd->bhid')\n",
        "\n",
        "        self.qkv = Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = Dropout(attn_drop)\n",
        "        self.proj = Linear(dim, dim)\n",
        "        self.proj_drop = Dropout(proj_drop)\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "\n",
        "        self.attn_cam = None\n",
        "        self.attn = None\n",
        "        self.v = None\n",
        "        self.v_cam = None\n",
        "        self.attn_gradients = None\n",
        "\n",
        "    def get_attn(self):\n",
        "        return self.attn\n",
        "\n",
        "    def save_attn(self, attn):\n",
        "        self.attn = attn\n",
        "\n",
        "    def save_attn_cam(self, cam):\n",
        "        self.attn_cam = cam\n",
        "\n",
        "    def get_attn_cam(self):\n",
        "        return self.attn_cam\n",
        "\n",
        "    def get_v(self):\n",
        "        return self.v\n",
        "\n",
        "    def save_v(self, v):\n",
        "        self.v = v\n",
        "\n",
        "    def save_v_cam(self, cam):\n",
        "        self.v_cam = cam\n",
        "\n",
        "    def get_v_cam(self):\n",
        "        return self.v_cam\n",
        "\n",
        "    def save_attn_gradients(self, attn_gradients):\n",
        "        self.attn_gradients = attn_gradients\n",
        "\n",
        "    def get_attn_gradients(self):\n",
        "        return self.attn_gradients\n",
        "\n",
        "    #this may cause issues for 3d? or does it act upoon the patches\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.num_heads\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
        "\n",
        "        self.save_v(v)\n",
        "\n",
        "        dots = self.matmul1([q, k]) * self.scale\n",
        "\n",
        "        attn = self.softmax(dots)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        self.save_attn(attn)\n",
        "        attn.register_hook(self.save_attn_gradients)\n",
        "\n",
        "        out = self.matmul2([attn, v])\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "    def relprop(self, cam, **kwargs):\n",
        "        cam = self.proj_drop.relprop(cam, **kwargs)\n",
        "        cam = self.proj.relprop(cam, **kwargs)\n",
        "        cam = rearrange(cam, 'b n (h d) -> b h n d', h=self.num_heads)\n",
        "\n",
        "        # attn = A*V\n",
        "        (cam1, cam_v)= self.matmul2.relprop(cam, **kwargs)\n",
        "        cam1 /= 2\n",
        "        cam_v /= 2\n",
        "\n",
        "        self.save_v_cam(cam_v)\n",
        "        self.save_attn_cam(cam1)\n",
        "\n",
        "        cam1 = self.attn_drop.relprop(cam1, **kwargs)\n",
        "        cam1 = self.softmax.relprop(cam1, **kwargs)\n",
        "\n",
        "        # A = Q*K^T\n",
        "        (cam_q, cam_k) = self.matmul1.relprop(cam1, **kwargs)\n",
        "        cam_q /= 2\n",
        "        cam_k /= 2\n",
        "\n",
        "        cam_qkv = rearrange([cam_q, cam_k, cam_v], 'qkv b h n d -> b n (qkv h d)', qkv=3, h=self.num_heads)\n",
        "\n",
        "        return self.qkv.relprop(cam_qkv, **kwargs)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = LayerNorm(dim, eps=1e-6)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.norm2 = LayerNorm(dim, eps=1e-6)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n",
        "\n",
        "        self.add1 = Add()\n",
        "        self.add2 = Add()\n",
        "        self.clone1 = Clone()\n",
        "        self.clone2 = Clone()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1, x2 = self.clone1(x, 2)\n",
        "        x = self.add1([x1, self.attn(self.norm1(x2))])\n",
        "        x1, x2 = self.clone2(x, 2)\n",
        "        x = self.add2([x1, self.mlp(self.norm2(x2))])\n",
        "        return x\n",
        "\n",
        "    def relprop(self, cam, **kwargs):\n",
        "        (cam1, cam2) = self.add2.relprop(cam, **kwargs)\n",
        "        cam2 = self.mlp.relprop(cam2, **kwargs)\n",
        "        cam2 = self.norm2.relprop(cam2, **kwargs)\n",
        "        cam = self.clone2.relprop((cam1, cam2), **kwargs)\n",
        "\n",
        "        (cam1, cam2) = self.add1.relprop(cam, **kwargs)\n",
        "        cam2 = self.attn.relprop(cam2, **kwargs)\n",
        "        cam2 = self.norm1.relprop(cam2, **kwargs)\n",
        "        cam = self.clone1.relprop((cam1, cam2), **kwargs)\n",
        "        return cam\n",
        "\n",
        "#3D patch embedder\n",
        "class PatchEmbed3D(nn.Module):\n",
        "  \"\"\" Image to Patch Embedding\n",
        "  \"\"\"\n",
        "  def __init__(self, img_size = (200, 100, 150), patch_size = (20, 10, 15), in_chans = 1, embed_dim = 768):\n",
        "    super().__init__()\n",
        "    img_size = to_2tuple(img_size)\n",
        "    patch_size = to_2tuple(patch_size)\n",
        "    num_patches = (img_size[2] // patch_size[2]) * (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.num_patches = num_patches\n",
        "    self.proj = torch.nn.Conv3d(in_chans, embed_dim, kernel_size = patch_size, stride = patch_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, H, W, D = x.shape\n",
        "    # FIXME look at relaxing size constraints\n",
        "    assert H == self.img_size[0] and W == self.img_size[1] and D == self.img_size[2], \\\n",
        "      f\"Input image size ({H}*{W}*{D}) doesn't match model ({self.img_size[0]} * {self.img_size[1]} * {self.img_size[2]}).\"\n",
        "    x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "    return x\n",
        "\n",
        "  def relprop(self, cam, **kwargs):\n",
        "    cam = cam.transpose(1,2) #why transpose?\n",
        "    cam = cam.reshape(cam.shape[0], cam.shape[1], cam.reshape[2], #is the cam in 3d as well?\n",
        "                      (self.img_size[0] // self.patch_size[0]),\n",
        "                      (self.img_size[1] // self.patch_size[1]),\n",
        "                      (self.img_size[2] // self.patch_size[2])\n",
        "                      )\n",
        "    return self.proj.relprop(cam, **kwargs)\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=(200, 100, 150), patch_size=(20, 10, 15), in_chans=1, num_classes=2, embed_dim=768, depth=12,\n",
        "                 num_heads=12, mlp_ratio=4., qkv_bias=False, mlp_head=False, drop_rate=0., attn_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        self.patch_embed = PatchEmbed3D(\n",
        "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "        self.norm = LayerNorm(embed_dim)\n",
        "        if mlp_head:\n",
        "            # paper diagram suggests 'MLP head', but results in 4M extra parameters vs paper\n",
        "            self.head = Mlp(embed_dim, int(embed_dim * mlp_ratio), num_classes)\n",
        "        else:\n",
        "            # with a single Linear layer as head, the param count within rounding of paper\n",
        "            self.head = Linear(embed_dim, num_classes)\n",
        "\n",
        "        # FIXME not quite sure what the proper weight init is supposed to be,\n",
        "        # normal / trunc normal w/ std == .02 similar to other Bert like transformers\n",
        "        trunc_normal_(self.pos_embed, std=.02)  # embeddings same as weights?\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        self.pool = IndexSelect()\n",
        "        self.add = Add()\n",
        "\n",
        "        self.inp_grad = None\n",
        "\n",
        "    def save_inp_grad(self,grad):\n",
        "        self.inp_grad = grad\n",
        "\n",
        "    def get_inp_grad(self):\n",
        "        return self.inp_grad\n",
        "\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @property\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token'}\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.add([x, self.pos_embed])\n",
        "\n",
        "        x.register_hook(self.save_inp_grad)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.pool(x, dim=1, indices=torch.tensor(0, device=x.device))\n",
        "        x = x.squeeze(1)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "    def relprop(self, cam=None,method=\"transformer_attribution\", is_ablation=False, start_layer=0, **kwargs):\n",
        "        # print(kwargs)\n",
        "        # print(\"conservation 1\", cam.sum())\n",
        "        cam = self.head.relprop(cam, **kwargs)\n",
        "        cam = cam.unsqueeze(1)\n",
        "        cam = self.pool.relprop(cam, **kwargs)\n",
        "        cam = self.norm.relprop(cam, **kwargs)\n",
        "        for blk in reversed(self.blocks):\n",
        "            cam = blk.relprop(cam, **kwargs)\n",
        "\n",
        "        # print(\"conservation 2\", cam.sum())\n",
        "        # print(\"min\", cam.min())\n",
        "\n",
        "        if method == \"full\":\n",
        "            (cam, _) = self.add.relprop(cam, **kwargs)\n",
        "            cam = cam[:, 1:]\n",
        "            cam = self.patch_embed.relprop(cam, **kwargs)\n",
        "            # sum on channels\n",
        "            cam = cam.sum(dim=1)\n",
        "            return cam\n",
        "\n",
        "        elif method == \"rollout\":\n",
        "            # cam rollout\n",
        "            attn_cams = []\n",
        "            for blk in self.blocks:\n",
        "                attn_heads = blk.attn.get_attn_cam().clamp(min=0)\n",
        "                avg_heads = (attn_heads.sum(dim=1) / attn_heads.shape[1]).detach()\n",
        "                attn_cams.append(avg_heads)\n",
        "            cam = compute_rollout_attention(attn_cams, start_layer=start_layer)\n",
        "            cam = cam[:, 0, 1:]\n",
        "            return cam\n",
        "\n",
        "        # our method, method name grad is legacy\n",
        "        elif method == \"transformer_attribution\" or method == \"grad\":\n",
        "            cams = []\n",
        "            for blk in self.blocks:\n",
        "                grad = blk.attn.get_attn_gradients()\n",
        "                cam = blk.attn.get_attn_cam()\n",
        "                cam = cam[0].reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "                grad = grad[0].reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "                cam = grad * cam\n",
        "                cam = cam.clamp(min=0).mean(dim=0)\n",
        "                cams.append(cam.unsqueeze(0))\n",
        "            rollout = compute_rollout_attention(cams, start_layer=start_layer)\n",
        "            cam = rollout[:, 0, 1:]\n",
        "            return cam\n",
        "\n",
        "        elif method == \"last_layer\":\n",
        "            cam = self.blocks[-1].attn.get_attn_cam()\n",
        "            cam = cam[0].reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "            if is_ablation:\n",
        "                grad = self.blocks[-1].attn.get_attn_gradients()\n",
        "                grad = grad[0].reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "                cam = grad * cam\n",
        "            cam = cam.clamp(min=0).mean(dim=0)\n",
        "            cam = cam[0, 1:]\n",
        "            return cam\n",
        "\n",
        "        elif method == \"last_layer_attn\":\n",
        "            cam = self.blocks[-1].attn.get_attn()\n",
        "            cam = cam[0].reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "            cam = cam.clamp(min=0).mean(dim=0)\n",
        "            cam = cam[0, 1:]\n",
        "            return cam\n",
        "\n",
        "        elif method == \"second_layer\":\n",
        "            cam = self.blocks[1].attn.get_attn_cam()\n",
        "            cam = cam[0].reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "            if is_ablation:\n",
        "                grad = self.blocks[1].attn.get_attn_gradients()\n",
        "                grad = grad[0].reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "                cam = grad * cam\n",
        "            cam = cam.clamp(min=0).mean(dim=0)\n",
        "            cam = cam[0, 1:]\n",
        "            return cam\n",
        "\n",
        "\n",
        "def _conv_filter(state_dict, patch_size=30):\n",
        "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
        "    out_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if 'patch_embed.proj.weight' in k:\n",
        "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
        "        out_dict[k] = v\n",
        "    return out_dict\n",
        "\n",
        "#Dataset\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, df, col_image, col_label, aug = False, shuffle = False):\n",
        "      #params\n",
        "      self.df = df\n",
        "      if shuffle:\n",
        "        self.df = self.df.sample(frac = 1, random_state = 42,).reset_index()\n",
        "      self.aug = aug\n",
        "      self.col_image = col_image\n",
        "      self.col_label = col_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      #row\n",
        "      row = self.df.iloc[idx]\n",
        "\n",
        "      #read image\n",
        "      image = torch.Tensor(np.expand_dims(sitk.GetArrayFromImage(sitk.ReadImage(row[self.col_image])), axis = 0))\n",
        "      label = row[self.col_label]\n",
        "\n",
        "      #if aug\n",
        "      if self.aug:\n",
        "        #augmentation (increase this given size of the data)\n",
        "        if random.random() > 0.5: #0.7 0.8\n",
        "          #horizontal\n",
        "          image = torchvision.transforms.functional.hflip(image)\n",
        "        if random.random() > 0.5:\n",
        "          #vertical\n",
        "          image = torchvision.transforms.functional.vflip(image)\n",
        "        #if random.random() > 0.5:\n",
        "          #rotate\n",
        "          image = torchvision.transforms.functional.rotate(image,\n",
        "                                                           random.choice([30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330]))\n",
        "        #noise\n",
        "        #if random.random() > 0.5:\n",
        "          #noise = torch.normal(random.uniform(-0.9, 0.9), random.uniform(0.01, 0.1), image.shape)\n",
        "          #image = image + noise\n",
        "\n",
        "        #alternate roations doesn't work due to shape\n",
        "        #if random.random() > 0.5:\n",
        "        #  ls = random.sample([1,2,3], 2)\n",
        "        #  k = random.randint(1, 3)\n",
        "        #  image = torch.rot90(image, k, ls)\n",
        "\n",
        "        #affine or perspective may be more aggressive? tranlate and scale --> have to make sure are valid inputs!\n",
        "        #hopefully increase scale invariance!\n",
        "        #if random.random() > 0.5:\n",
        "          #careful not to cut off\n",
        "          #x = random.choice([5, 10, 15, -5, -10, -15])\n",
        "          #y = random.choice([5, 10, 15, -5, -10, -15])\n",
        "          #higher indicates more zoom in (AAA usually in center; may decide to remove if model not fitting correctly)\n",
        "          #may also remove the perspective aspect of the problem (gauging if bigger or smaller!)\n",
        "          #scaling = random.choice([0.9, 1.1, 1.5, 1.9])\n",
        "          #image = torchvision.transforms.functional.affine(image, angle = 0, translate = (x, y), scale = scaling, shear = 0, fill = -1,\n",
        "          #interpolation = torchvision.transforms.InterpolationMode.BILINEAR)\n",
        "      #return\n",
        "      return image, label\n",
        "\n",
        "#save json file\n",
        "def save_params(hyper_params, save_path):\n",
        "  json_string = json.dumps(hyper_params)\n",
        "  with open(save_path, 'w') as outfile:\n",
        "    outfile.write(json_string)\n",
        "\n",
        "#load the train params back in\n",
        "def load_params(fpath):\n",
        "  # Opening JSON file\n",
        "  with open(fpath) as json_file:\n",
        "    data = json.load(json_file)\n",
        "  return data\n",
        "\n",
        "#get the optimizer\n",
        "def get_optimizer(model_config, model):\n",
        "  #AdamW\n",
        "  if model_config['optimizer'] == 'AdamW':\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr = model_config['init_lr'], weight_decay = model_config['weight_decay'])\n",
        "  #Adam\n",
        "  if model_config['optimizer'] == 'Adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = model_config['init_lr'], weight_decay = model_config['weight_decay'])\n",
        "  #NAdam\n",
        "  if model_config['optimizer'] == 'NAdam':\n",
        "    optimizer = torch.optim.NAdam(model.parameters(), lr = model_config['init_lr'], weight_decay = model_config['weight_decay'])\n",
        "  #RAdam\n",
        "  if model_config['optimizer'] == 'RAdam':\n",
        "    optimizer = torch.optim.RAdam(model.parameters(), lr = model_config['init_lr'], weight_decay = model_config['weight_decay'])\n",
        "  #return\n",
        "  return optimizer\n",
        "\n",
        "#get the model\n",
        "def get_model(model_config):\n",
        "  #model\n",
        "  model = VisionTransformer(\n",
        "      img_size = model_config['img_size'],\n",
        "      patch_size = model_config['patch_size'],\n",
        "      in_chans = model_config['in_chans'],\n",
        "      num_classes = model_config['num_classes'],\n",
        "      embed_dim = model_config['embed_dim'],\n",
        "      depth = model_config['depth'],\n",
        "      num_heads = model_config['num_heads'],\n",
        "      mlp_ratio = model_config['mlp_ratio'],\n",
        "      qkv_bias = model_config['qkv_bias'],\n",
        "      mlp_head = model_config['mlp_head'],\n",
        "      drop_rate = model_config['drop_rate'],\n",
        "      attn_drop_rate = model_config['attn_drop_rate']\n",
        "  )\n",
        "  #return\n",
        "  return model\n",
        "\n",
        "#model saving policy\n",
        "def save_model(model_config, model):\n",
        "  #save model use weights instead\n",
        "  model.eval()\n",
        "  #depending on choice\n",
        "  if model_config['save_weights_only']:\n",
        "    torch.save(model.state_dict(), model_config['save_folder'] + 'model_weights.pth')\n",
        "  else:\n",
        "    torch.save(model, model_config['save_folder'] + 'model.pth')\n",
        "  #save info\n",
        "  save_params(model_config, model_config['save_folder'] + 'model_config.json')\n",
        "\n",
        "#update previously saved config only\n",
        "def update_config_stopearly(save_path):\n",
        "  #load\n",
        "  model_config = load_params(save_path + 'model_config.json')\n",
        "  #update\n",
        "  model_config['early_stopping']['stopped_early'] = True\n",
        "  #save\n",
        "  save_params(model_config, save_path + 'model_config.json')\n",
        "\n",
        "#new saving policy\n",
        "def new_saving_policy(early_stop, best_model, model_config, model, epoch):\n",
        "  #if there is an early stop\n",
        "  if early_stop:\n",
        "    #exit training\n",
        "    exit_training = True\n",
        "    #has the model been already saved?\n",
        "    if model_config['early_stopping']['model_criteria']:\n",
        "      #save just the config with update\n",
        "      update_config_stopearly(model_config['save_folder'])\n",
        "    #if not already saved\n",
        "    else:\n",
        "      #update\n",
        "      model_config['early_stopping']['stopped_early'] = True\n",
        "      #save the model and config\n",
        "      save_model(model_config, model)\n",
        "  #if there is not early stop\n",
        "  else:\n",
        "    #exit\n",
        "    exit_training = False\n",
        "    #need to log that we did not exit training early\n",
        "    model_config['early_stopping']['stopped_early'] = False\n",
        "    #you want to save the model every n_epochs often\n",
        "    if model_config['save_best_model'] == False:\n",
        "      #check if epocch is divisible and nonzero\n",
        "      if (epoch % model_config['save_after_n_epochs'] == 0) and (epoch != 0):\n",
        "        #indicate the model was saved\n",
        "        model_config['early_stopping']['model_criteria'] = True\n",
        "        #save\n",
        "        save_model(model_config, model)\n",
        "    else:\n",
        "      #you want to save the best model\n",
        "      if model_config['epochs_trained'] >= model_config['save_after_n_epochs']:\n",
        "        #check if current model is the best model\n",
        "        if best_model:\n",
        "          #log that it is the best model\n",
        "          model_config['early_stopping']['best_model'] = True\n",
        "          #indicate the model was saved\n",
        "          model_config['early_stopping']['model_criteria'] = True\n",
        "          #then save\n",
        "          save_model(model_config, model)\n",
        "        #if current model is not the best model but want to save for the initital run\n",
        "        if (best_model == False) and (model_config['epochs_trained'] == model_config['save_after_n_epochs']):\n",
        "          #inidicate the model was saved\n",
        "          model_config['early_stopping']['model_criteria'] = True\n",
        "          #save the model and config\n",
        "          save_model(model_config, model)\n",
        "\n",
        "  #return\n",
        "  return model_config, exit_training\n",
        "\n",
        "#class earlystopping\n",
        "class EarlyStopping:\n",
        "  #early stop if validation does not improve for given patience\n",
        "  def __init__(self, model_config, verbose = True, trace_func = print):\n",
        "    #set up\n",
        "    self.patience = model_config['early_stopping']['patience']\n",
        "    self.delta = model_config['early_stopping']['delta']\n",
        "    self.verbose = verbose\n",
        "    self.trace_func = trace_func\n",
        "    self.counter = 0\n",
        "    self.best_score = None\n",
        "    self.best_model = False\n",
        "    self.early_stop = False\n",
        "\n",
        "  #call\n",
        "  def __call__(self, val_loss):\n",
        "    #neg val loss\n",
        "    score = -val_loss\n",
        "    #init condition\n",
        "    if self.best_score is None:\n",
        "      self.best_score = score\n",
        "    #count number of times model failed to meet the condition\n",
        "    elif score < self.best_score + self.delta:\n",
        "      self.counter += 1\n",
        "      self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "      self.best_model = False\n",
        "      #identify when early stopping is required\n",
        "      if self.counter >= self.patience:\n",
        "        self.early_stop = True\n",
        "    #if the model shows best score\n",
        "    else:\n",
        "      #get the score and counter\n",
        "      self.best_score = score\n",
        "      self.counter = 0\n",
        "      self.best_model = True\n",
        "    #return the interl\n",
        "    return self.early_stop, self.best_model\n",
        "\n",
        "#get the loss fn\n",
        "def get_loss(model_config, device, df = None):\n",
        "  #criterion if manually weighted\n",
        "  if (model_config['loss'] == 'CE') and (model_config['weighted'] == True):\n",
        "    #get\n",
        "    criterion = nn.CrossEntropyLoss(weight = torch.Tensor(model_config['weights']).to(device))\n",
        "  #criterion if not weighted\n",
        "  if (model_config['loss'] == 'CE') and (model_config['weighted'] == False):\n",
        "    #get\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "  #criterion if using automatic weighting?\n",
        "  if (model_config['loss'] == 'CE') and (model_config['weighted'] == 'auto'):\n",
        "    #get\n",
        "    criterion = nn.CrossEntropyLoss(weight = torch.Tensor(auto_weights(model_config, df)).to(device))\n",
        "  #return\n",
        "  return criterion\n",
        "\n",
        "#get scheduler\n",
        "def get_scheduler(model_config, optimizer):\n",
        "  #plateau\n",
        "  if model_config['scheduler']['description'] == 'plateau':\n",
        "    #get the scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = model_config['scheduler']['mode'],\n",
        "                                                           factor = model_config['scheduler']['factor'],\n",
        "                                                           patience = model_config['scheduler']['patience'],\n",
        "                                                           threshold = model_config['scheduler']['threshold'],\n",
        "                                                           threshold_mode = model_config['scheduler']['threshold_mode'],\n",
        "                                                           cooldown = model_config['scheduler']['cooldown'],\n",
        "                                                           min_lr = model_config['scheduler']['min_lr'],\n",
        "                                                           eps = model_config['scheduler']['eps'],\n",
        "                                                           verbose = True)\n",
        "  #return\n",
        "  return scheduler\n",
        "\n",
        "#get sampler\n",
        "def get_sampler(df, model_config):\n",
        "  labels_unique, counts = np.unique(df[model_config['col_label']], return_counts = True)\n",
        "  class_weights = [sum(counts) / c for c in counts]\n",
        "  example_weights = [class_weights[e] for e in df[model_config['col_label']]]\n",
        "  sampler = torch.utils.data.WeightedRandomSampler(example_weights, len(df['Annotation_Label']))\n",
        "  return sampler\n",
        "\n",
        "#get loss weights automatically\n",
        "def auto_weights(model_config, df):\n",
        "  #get class weights\n",
        "  y = df[model_config['col_label']].to_numpy().astype(np.int8)\n",
        "  #get the class weights\n",
        "  class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
        "  #convert to tensor\n",
        "  class_weights = torch.tensor(class_weights, dtype = torch.float)\n",
        "  #return (may have to send to gpu)\n",
        "  return class_weights"
      ],
      "metadata": {
        "id": "ERtnwnR0gXwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters in wandb format\n",
        "sweep_config = {\n",
        "    #name decided later (sweep name)\n",
        "    'name': None,\n",
        "    #sweep method\n",
        "    'method': 'grid',\n",
        "    #metric\n",
        "    'metric': {\n",
        "        'name': 'val_acc',\n",
        "        'goal': 'maximize',\n",
        "    },\n",
        "    #values which may be altered wandb wants all components\n",
        "    'parameters': {\n",
        "        #description\n",
        "        'description': {'value': '3D ViT for classification of vRAD data for 2 folds'},\n",
        "        #project\n",
        "        'project':{'value': '3D-PAD-ViT-Sweep'},\n",
        "        'model': {'value': None}, #placeholder for actual name\n",
        "        #documentation\n",
        "        'data_path': {'value': 'AnishSalvi/ImageRx/PAD-Net/ViT_classification/dataset/dataset-2023-03-04-01-37-09.pkl'},\n",
        "        #path to save the results\n",
        "        'save_folder': {'value': 'AnishSalvi/ImageRx/PAD-Net/ViT_classification/results/'},\n",
        "        'col_image': {'value': 'Norm-256-256-256'},\n",
        "        'col_label': {'value': 'Annotation_Label'},\n",
        "        'device': {'value': device},\n",
        "        'val_kfolds': {'value': [4, 5]},\n",
        "        #model params\n",
        "        'img_size': {'values': [(256, 256, 256)]},\n",
        "        'patch_size': {'values': [(64, 64, 64)]},\n",
        "        'in_chans': {'values': [1]},\n",
        "        'num_classes': {'values': [3]},\n",
        "        'embed_dim': {'values': [512]},\n",
        "        'depth': {'values': [8]},\n",
        "        'num_heads': {'values': [8]},\n",
        "        'mlp_ratio': {'values': [8.]},\n",
        "        'qkv_bias': {'values': [False]},\n",
        "        'mlp_head': {'values': [True]},\n",
        "        'drop_rate': {'values': [10e-2]},\n",
        "        'attn_drop_rate': {'values': [10e-2]},\n",
        "        #training params\n",
        "        'aug': {'values': [True]},\n",
        "        'batch_size': {'values': [4]},\n",
        "        'init_lr': {'values': [1e-6]},\n",
        "        'epochs': {'values': [100]},\n",
        "        'epochs_trained': {'value': 0}, #this is updated in the script!\n",
        "        'save_after_n_epochs': {'value': 5}, #depends on if you want to save the best model\n",
        "        'weight_decay': {'values': [1e-4]},\n",
        "        'optimizer': {'values': ['AdamW']},\n",
        "        'scheduler': {'values': [\n",
        "            {'description': 'plateau',\n",
        "             'mode': 'min',\n",
        "             'factor': 5e-1,\n",
        "             'patience': 10,\n",
        "             'threshold': 1e-3,\n",
        "             'threshold_mode': 'rel',\n",
        "             'cooldown': 0,\n",
        "             'min_lr': 0,\n",
        "             'eps': 1}]},\n",
        "        'loss': {'values': ['CE']},\n",
        "        'weighted': {'values': ['auto']},\n",
        "        'weights': {'values': [(0.3, 0.5, 0.2)]}, #by default // depends on weighted\n",
        "        'sampler': {'values':[False]},\n",
        "        #saving\n",
        "        'save_weights_only': {'value': True},\n",
        "        'save_best_model': {'value': True},\n",
        "        #early stopping\n",
        "        'early_stopping': {'value':\n",
        "            {'patience': 10,\n",
        "            'delta': 1e-4,\n",
        "            'stopped_early': None,\n",
        "            'best_model': None,\n",
        "            'model_criteria': False}\n",
        "        },\n",
        "        #log the model loss and acc\n",
        "        'log_train_loss': {'value': None},\n",
        "        'log_train_acc': {'value': None},\n",
        "        'log_val_loss': {'value': None},\n",
        "        'log_val_acc': {'value': None},\n",
        "        #updating performance in WandB\n",
        "        'train_loss': {'value': None},\n",
        "        'train_acc': {'value': None},\n",
        "        'val_loss': {'value': None},\n",
        "        'val_acc': {'value': None}\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "YnQRR0Dx_LC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main script\n",
        "def main(config = None):\n",
        "  #clear workspace when finished with a single model run\n",
        "  model, x, y_true, y_pred, loss = (None, None, None, None, None)\n",
        "  dset_train, train_loader, dset_val, val_loader = (None, None, None, None)\n",
        "  criterion, optimizer, scheduler, early_stopper = (None, None, None, None)\n",
        "  #reset\n",
        "  if device == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  #name the model\n",
        "  model_name = datetime.now().strftime('3D-ViT-classification-%Y-%m-%d-%H-%M-%S')\n",
        "\n",
        "  #init a new wandb run (config = sweep_config)\n",
        "  with wandb.init(config = config, name = model_name):\n",
        "    #set up the config (WandB, locked)\n",
        "    config = wandb.config\n",
        "    #dict (not locked)\n",
        "    model_config = dict(config)\n",
        "    #name the model\n",
        "    model_config['model'] = model_name\n",
        "    #save location\n",
        "    model_config['save_folder'] = model_config['save_folder'] +  model_config['model'] + '/'\n",
        "    #create the model folder\n",
        "    if os.path.isdir(model_config['save_folder']) == False:\n",
        "      os.mkdir(model_config['save_folder'])\n",
        "\n",
        "    #training data(not in valkfolds!)\n",
        "    df_train = df[~df['KFold'].isin(model_config['val_kfolds'])]\n",
        "\n",
        "    #get the training data (remove the folds corresponding to validation)\n",
        "    dset_train = CustomImageDataset(df_train,\n",
        "                                    col_image = model_config['col_image'], col_label = model_config['col_label'],\n",
        "                                    aug = model_config['aug'], shuffle = False)\n",
        "\n",
        "    #if train sampler\n",
        "    if model_config['sampler']:\n",
        "      train_sampler = get_sampler(df_train, model_config)\n",
        "    else:\n",
        "      train_sampler = None\n",
        "\n",
        "    #train loader\n",
        "    train_loader = DataLoader(dset_train, sampler = train_sampler, batch_size = model_config['batch_size'])\n",
        "\n",
        "    #val data (in valkfolds)\n",
        "    df_val = df[df['KFold'].isin(model_config['val_kfolds'])]\n",
        "\n",
        "    #get the validation data\n",
        "    dset_val = CustomImageDataset(df_val,\n",
        "                                  col_image = model_config['col_image'], col_label = model_config['col_label'],\n",
        "                                  aug = False, shuffle = False)\n",
        "\n",
        "    #if val sampler\n",
        "    if model_config['sampler']:\n",
        "      val_sampler = get_sampler(df_val, model_config)\n",
        "    else:\n",
        "      val_sampler = None\n",
        "\n",
        "    #val loader\n",
        "    val_loader = DataLoader(dset_val, sampler = val_sampler, batch_size = model_config['batch_size'])\n",
        "\n",
        "    #init the model\n",
        "    model = get_model(model_config)\n",
        "    #send\n",
        "    model.to(device)\n",
        "    #criterion (can get loss based on training data)\n",
        "    criterion = get_loss(model_config, device, df_train)\n",
        "    #optimizer\n",
        "    optimizer = get_optimizer(model_config, model)\n",
        "    #scheduler\n",
        "    scheduler = get_scheduler(model_config, optimizer)\n",
        "    #early stopping (save time during the sweep)\n",
        "    early_stopper = EarlyStopping(model_config)\n",
        "\n",
        "    #track in Jupter Notebook\n",
        "    #liveloss = PlotLosses()\n",
        "    #logs\n",
        "    #logs = {}\n",
        "\n",
        "    #track for later\n",
        "    log_train_loss = []\n",
        "    log_train_acc = []\n",
        "    log_val_loss = []\n",
        "    log_val_acc = []\n",
        "\n",
        "\n",
        "    #iterate through the entire dataset\n",
        "    #+1 for shifting (python starts at 0)\n",
        "    for epoch in range(model_config['epochs_trained'] + 1, model_config['epochs'] + 1):\n",
        "      #determine train losses\n",
        "      train_epoch_loss = 0\n",
        "      #set for training\n",
        "      model.train()\n",
        "      #iterate through the training data\n",
        "      for i, (x, y_true) in enumerate(train_loader):\n",
        "        #zero optimizer\n",
        "        optimizer.zero_grad()\n",
        "        #send to device\n",
        "        #x = x.to(device)\n",
        "        #y_true = y_true.to(device)\n",
        "        #predict\n",
        "        y_pred = model(x.to(device))\n",
        "        #determine loss (should already be averaged)\n",
        "        loss = criterion(y_pred, y_true.to(device))\n",
        "        #backward\n",
        "        loss.backward()\n",
        "        #step\n",
        "        optimizer.step()\n",
        "        #track the loss\n",
        "        train_epoch_loss = train_epoch_loss + loss.item()\n",
        "        #reset\n",
        "        x, y_true, y_pred, loss = (None, None, None, None)\n",
        "        if device == 'cuda':\n",
        "          torch.cuda.empty_cache()\n",
        "      #calculate train loss\n",
        "      train_loss = train_epoch_loss / len(train_loader)\n",
        "      #calculate train acc\n",
        "      train_acc = 1 - train_loss\n",
        "\n",
        "      #determine validation losses\n",
        "      val_epoch_loss = 0\n",
        "      #specify eval\n",
        "      model.eval()\n",
        "      #set\n",
        "      #with torch.no_grad():\n",
        "      for i, (x, y_true) in enumerate(val_loader):\n",
        "        #send to device\n",
        "        #x = x.to(device)\n",
        "        #y_true = y_true.to(device)\n",
        "        #predict\n",
        "        y_pred = model(x.to(device))\n",
        "        #determine loss\n",
        "        loss = criterion(y_pred, y_true.to(device))\n",
        "        #track the loss (shoudld already be averaged)\n",
        "        val_epoch_loss = val_epoch_loss + loss.item()\n",
        "        #reset\n",
        "        x, y_true, y_pred, loss = (None, None, None, None)\n",
        "        if device == 'cuda':\n",
        "          torch.cuda.empty_cache()\n",
        "      #calulate val loss\n",
        "      val_loss = val_epoch_loss / len(val_loader)\n",
        "      #calulate val acc\n",
        "      val_acc = 1 - val_loss\n",
        "\n",
        "      #scheduler\n",
        "      scheduler.step(train_loss)\n",
        "\n",
        "      #record for training\n",
        "      log_train_loss.append(train_loss)\n",
        "      log_train_acc.append(train_acc)\n",
        "      #record for validation\n",
        "      log_val_loss.append(val_loss)\n",
        "      log_val_acc.append(val_acc)\n",
        "\n",
        "      #wont log lossess or acc after early stopping or save best model\n",
        "\n",
        "      #log the most recent info\n",
        "      model_config['train_loss'] = train_loss\n",
        "      model_config['train_acc'] = train_acc\n",
        "      model_config['val_loss'] = val_loss\n",
        "      model_config['val_acc'] = val_acc\n",
        "\n",
        "      #log all the info\n",
        "      model_config['log_train_loss'] = log_train_loss\n",
        "      model_config['log_train_acc'] = log_train_acc\n",
        "      model_config['log_val_loss'] = log_val_loss\n",
        "      model_config['log_val_acc'] = log_val_acc\n",
        "\n",
        "      #keep track of each epoch\n",
        "      model_config['epochs_trained'] = epoch\n",
        "\n",
        "      #print\n",
        "      print('Epoch {0} of {1}: Train Loss {2:.2g} & Acc {3:.2g} v Val Loss {4:.2g} and Acc {5:.2g}'.format(epoch, model_config['epochs'],\n",
        "                                                                                                           train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "      #wandb\n",
        "      wandb.log(model_config)\n",
        "\n",
        "\n",
        "      #determine if early stopping is required by validation loss\n",
        "      early_stop, best_model = early_stopper(val_loss)\n",
        "      #saving policy and determine if training should be exited based on early stop and best model\n",
        "      model_config, exit_training = new_saving_policy(early_stop, best_model, model_config, model, epoch)\n",
        "\n",
        "      #specify the logs\n",
        "      #prefix = ''\n",
        "      #logs['Loss'] = train_loss\n",
        "      #logs['Acc'] = train_acc\n",
        "      #logs\n",
        "      #prefix = 'val_'\n",
        "      #logs[prefix + 'Loss'] = val_loss\n",
        "      #logs[prefix + 'Acc'] = val_acc\n",
        "\n",
        "      #living loss\n",
        "      #liveloss.update(logs)\n",
        "      #send\n",
        "      #liveloss.send()\n",
        "\n",
        "      #exit training early\n",
        "      if exit_training:\n",
        "        print('Early Stop: Exit Training')\n",
        "        break\n",
        "\n",
        "    #clear workspace when finished with a single model run\n",
        "    model, x, y_true, y_pred, loss = (None, None, None, None, None)\n",
        "    dset_train, train_loader, dset_val, val_loader = (None, None, None, None)\n",
        "    criterion, optimizer, scheduler, early_stopper = (None, None, None, None)\n",
        "    #reset\n",
        "    if device == 'cuda':\n",
        "      torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hw2ap7Xti5Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main script\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  #read the pickle file\n",
        "  df = pd.read_pickle(sweep_config['parameters']['data_path']['value'])\n",
        "\n",
        "  #specify the sweep save location\n",
        "  sweep_config['name'] = datetime.now().strftime('3D-ViT-sweep-%Y-%m-%d-%H-%M-%S')\n",
        "  #set\n",
        "  sweep_config['parameters']['save_folder']['value'] = sweep_config['parameters']['save_folder']['value'] + sweep_config['name'] + '/'\n",
        "  #create the sweep folder\n",
        "  if os.path.isdir(sweep_config['parameters']['save_folder']['value']) == False:\n",
        "    os.mkdir(sweep_config['parameters']['save_folder']['value'])\n",
        "  #save the sweep config in the sweep folder\n",
        "  save_params(sweep_config, sweep_config['parameters']['save_folder']['value'] + 'sweep_config.json')\n",
        "  #now run the main script\n",
        "\n",
        "  #select the project folder\n",
        "  sweep_id = wandb.sweep(sweep_config, project = sweep_config['parameters']['project']['value'])\n",
        "  #execute the search\n",
        "  wandb.agent(sweep_id, main)\n",
        "  #finish\n",
        "  wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4ff8d7d63e0f43818208fdb9c483a589",
            "a6123d6a37644e0d8c696921e959437b",
            "ce98d1bfc3fc4e3fb816466bfa1cf943",
            "efe286c8b308403bb28841a1843b6b4c",
            "7c2b7f5d390e462495f458ed3be4592a",
            "c3d205e3d6b34bc5bd379358feb6ac00",
            "a0227c18117c46419a47ec10703e1a47",
            "e9d0e4899d9a437ea1c084b5ab0ee5c4"
          ]
        },
        "id": "S-L84KNIc0nA",
        "outputId": "11d43499-6351-4e38-ef0f-92ecb1f268a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: pokwf2wx\n",
            "Sweep URL: https://wandb.ai/anish_s/3D-PAD-ViT-Sweep/sweeps/pokwf2wx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h01n4yxk with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattn_drop_rate: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taug: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcol_image: Norm-256-256-256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcol_label: Annotation_Label\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: AnishSalvi/ImageRx/PAD-Net/ViT_classification/dataset/dataset-2023-03-04-01-37-09.pkl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdepth: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdescription: 3D ViT for classification of vRAD data for 2 folds\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdevice: cuda\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_rate: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping: {'best_model': None, 'delta': 0.0001, 'model_criteria': False, 'patience': 10, 'stopped_early': None}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dim: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs_trained: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: [256, 256, 256]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_chans: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_lr: 1e-06\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlog_train_acc: None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlog_train_loss: None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlog_val_acc: None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlog_val_loss: None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: CE\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmlp_head: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmlp_ratio: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_classes: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_heads: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: AdamW\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tpatch_size: [64, 64, 64]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject: 3D-PAD-ViT-Sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tqkv_bias: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsampler: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_after_n_epochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_best_model: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_folder: AnishSalvi/ImageRx/PAD-Net/ViT_classification/results/3D-ViT-sweep-2023-03-12-15-10-36/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsave_weights_only: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: {'cooldown': 0, 'description': 'plateau', 'eps': 1, 'factor': 0.5, 'min_lr': 0, 'mode': 'min', 'patience': 10, 'threshold': 0.001, 'threshold_mode': 'rel'}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_acc: None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_loss: None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_acc: None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_kfolds: [4, 5]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_loss: None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweighted: auto\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweights: [0.3, 0.5, 0.2]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.13.11 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>L:\\wandb\\run-20230312_151041-h01n4yxk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/anish_s/3D-PAD-ViT-Sweep/runs/h01n4yxk' target=\"_blank\">3D-ViT-classification-2023-03-12-15-10-38</a></strong> to <a href='https://wandb.ai/anish_s/3D-PAD-ViT-Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/anish_s/3D-PAD-ViT-Sweep/sweeps/pokwf2wx' target=\"_blank\">https://wandb.ai/anish_s/3D-PAD-ViT-Sweep/sweeps/pokwf2wx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/anish_s/3D-PAD-ViT-Sweep' target=\"_blank\">https://wandb.ai/anish_s/3D-PAD-ViT-Sweep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/anish_s/3D-PAD-ViT-Sweep/sweeps/pokwf2wx' target=\"_blank\">https://wandb.ai/anish_s/3D-PAD-ViT-Sweep/sweeps/pokwf2wx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/anish_s/3D-PAD-ViT-Sweep/runs/h01n4yxk' target=\"_blank\">https://wandb.ai/anish_s/3D-PAD-ViT-Sweep/runs/h01n4yxk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 100: Train Loss 1.2 & Acc -0.16 v Val Loss 1.2 and Acc -0.2\n",
            "Epoch 2 of 100: Train Loss 1.1 & Acc -0.053 v Val Loss 1.2 and Acc -0.22\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch 3 of 100: Train Loss 1 & Acc 0.0015 v Val Loss 1.2 and Acc -0.19\n",
            "Epoch 4 of 100: Train Loss 1 & Acc -0.04 v Val Loss 1.3 and Acc -0.26\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch 5 of 100: Train Loss 1 & Acc -0.024 v Val Loss 1.2 and Acc -0.21\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch 6 of 100: Train Loss 1 & Acc -0.012 v Val Loss 1.2 and Acc -0.23\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch 7 of 100: Train Loss 1 & Acc -0.022 v Val Loss 1.2 and Acc -0.25\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch 8 of 100: Train Loss 0.95 & Acc 0.049 v Val Loss 1.3 and Acc -0.27\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch 9 of 100: Train Loss 0.99 & Acc 0.0096 v Val Loss 1.3 and Acc -0.26\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch 10 of 100: Train Loss 0.97 & Acc 0.03 v Val Loss 1.2 and Acc -0.24\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch 11 of 100: Train Loss 0.92 & Acc 0.083 v Val Loss 1.2 and Acc -0.19\n",
            "Epoch 12 of 100: Train Loss 0.86 & Acc 0.14 v Val Loss 1.3 and Acc -0.28\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch 13 of 100: Train Loss 0.95 & Acc 0.045 v Val Loss 1.3 and Acc -0.28\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch 14 of 100: Train Loss 0.9 & Acc 0.1 v Val Loss 1.3 and Acc -0.29\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch 15 of 100: Train Loss 0.95 & Acc 0.048 v Val Loss 1.4 and Acc -0.37\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch 16 of 100: Train Loss 0.88 & Acc 0.12 v Val Loss 1.4 and Acc -0.44\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch 17 of 100: Train Loss 0.88 & Acc 0.12 v Val Loss 1.3 and Acc -0.32\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch 18 of 100: Train Loss 0.86 & Acc 0.14 v Val Loss 1.4 and Acc -0.39\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch 19 of 100: Train Loss 0.89 & Acc 0.11 v Val Loss 1.4 and Acc -0.37\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch 20 of 100: Train Loss 0.82 & Acc 0.18 v Val Loss 1.5 and Acc -0.45\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch 21 of 100: Train Loss 0.84 & Acc 0.16 v Val Loss 1.4 and Acc -0.38\n",
            "EarlyStopping counter: 10 out of 10\n",
            "Early Stop: Exit Training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ff8d7d63e0f43818208fdb9c483a589"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>attn_drop_rate</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>aug</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>batch_size</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>depth</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>drop_rate</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>embed_dim</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epochs</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epochs_trained</td><td>â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>in_chans</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>init_lr</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>mlp_head</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>mlp_ratio</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>num_classes</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>num_heads</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>qkv_bias</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>sampler</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>save_after_n_epochs</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>save_best_model</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>save_weights_only</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_acc</td><td>â–â–ƒâ–„â–ƒâ–„â–„â–„â–…â–„â–…â–†â–‡â–…â–†â–…â–‡â–‡â–‡â–†â–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–†â–…â–†â–…â–…â–…â–„â–…â–„â–ƒâ–‚â–„â–ƒâ–„â–‚â–‚â–‚â–ƒâ–â–</td></tr><tr><td>val_acc</td><td>â–ˆâ–‡â–ˆâ–†â–‡â–‡â–†â–†â–†â–‡â–ˆâ–†â–…â–…â–ƒâ–â–…â–ƒâ–ƒâ–â–ƒ</td></tr><tr><td>val_loss</td><td>â–â–‚â–â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–â–ƒâ–„â–„â–†â–ˆâ–„â–†â–†â–ˆâ–†</td></tr><tr><td>weight_decay</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>attn_drop_rate</td><td>0.1</td></tr><tr><td>aug</td><td>True</td></tr><tr><td>batch_size</td><td>4</td></tr><tr><td>col_image</td><td>Norm-256-256-256</td></tr><tr><td>col_label</td><td>Annotation_Label</td></tr><tr><td>data_path</td><td>AnishSalvi/ImageRx/P...</td></tr><tr><td>depth</td><td>8</td></tr><tr><td>description</td><td>3D ViT for classific...</td></tr><tr><td>device</td><td>cuda</td></tr><tr><td>drop_rate</td><td>0.1</td></tr><tr><td>embed_dim</td><td>512</td></tr><tr><td>epochs</td><td>100</td></tr><tr><td>epochs_trained</td><td>21</td></tr><tr><td>in_chans</td><td>1</td></tr><tr><td>init_lr</td><td>0.0</td></tr><tr><td>loss</td><td>CE</td></tr><tr><td>mlp_head</td><td>True</td></tr><tr><td>mlp_ratio</td><td>8</td></tr><tr><td>model</td><td>3D-ViT-classificatio...</td></tr><tr><td>num_classes</td><td>3</td></tr><tr><td>num_heads</td><td>8</td></tr><tr><td>optimizer</td><td>AdamW</td></tr><tr><td>project</td><td>3D-PAD-ViT-Sweep</td></tr><tr><td>qkv_bias</td><td>False</td></tr><tr><td>sampler</td><td>False</td></tr><tr><td>save_after_n_epochs</td><td>5</td></tr><tr><td>save_best_model</td><td>True</td></tr><tr><td>save_folder</td><td>AnishSalvi/ImageRx/P...</td></tr><tr><td>save_weights_only</td><td>True</td></tr><tr><td>train_acc</td><td>0.16027</td></tr><tr><td>train_loss</td><td>0.83973</td></tr><tr><td>val_acc</td><td>-0.38461</td></tr><tr><td>val_loss</td><td>1.38461</td></tr><tr><td>weight_decay</td><td>0.0001</td></tr><tr><td>weighted</td><td>auto</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">3D-ViT-classification-2023-03-12-15-10-38</strong> at: <a href='https://wandb.ai/anish_s/3D-PAD-ViT-Sweep/runs/h01n4yxk' target=\"_blank\">https://wandb.ai/anish_s/3D-PAD-ViT-Sweep/runs/h01n4yxk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>.wandb\\run-20230312_151041-h01n4yxk\\logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wandb files\n",
        "import shutil\n",
        "shutil.rmtree('wandb')\n",
        "#https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL-Ujcps--Gg",
        "outputId": "6a770ce9-bcbc-4cfd-fc4a-dc2a5b35aaf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PermissionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#wandb files\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwandb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\shutil.py:750\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\shutil.py:620\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    618\u001b[0m             os\u001b[38;5;241m.\u001b[39munlink(fullname)\n\u001b[0;32m    619\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m             \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    622\u001b[0m     os\u001b[38;5;241m.\u001b[39mrmdir(path)\n",
            "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\shutil.py:618\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 618\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    620\u001b[0m         onerror(os\u001b[38;5;241m.\u001b[39munlink, fullname, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
            "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'wandb\\\\debug-cli.MeDCaVE_3.log'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FI36X6x4cXWj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}